<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Zindex&#39;s blog</title>
  
  <subtitle>全干工程师</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-06-05T08:02:10.409Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Zindex</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>k3s 安装小记</title>
    <link href="http://yoursite.com/2019/06/04/k3s-setup/"/>
    <id>http://yoursite.com/2019/06/04/k3s-setup/</id>
    <published>2019-06-04T06:22:24.000Z</published>
    <updated>2019-06-05T08:02:10.409Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://k3s.io/" target="_blank" rel="noopener">k3s</a> 刚出来的时候，我刚好看到这个项目，然后了解到这是一个轻量级的 k8s 发行版。之前刚好遇到在阿里云<a href="https://promotion.aliyun.com/ntms/act/campus2018.html" target="_blank" rel="noopener">学生机</a>（1C2G）上安装 k8s 后内存占用太多的问题，因此就决定尝试。最后的效果超出了预期，k3s 可以帮助我们在低配置机器上运行 k8s 集群，缓解了 k8s 对于资源占用的压力，降低了服务器的成本。</p><a id="more"></a><h3 id="k3s-简介"><a href="#k3s-简介" class="headerlink" title="k3s 简介"></a>k3s 简介</h3><p>k3s 是 <a href="https://rancher.com/" target="_blank" rel="noopener">Rancher</a> 推出的轻量级 k8s。k3s 本身包含了 k8s 的源码，所以本质上和 k8s 没有区别。但为了降低资源占用，k3s 和 k8s 还是有一些区别的，主要是：</p><ul><li>使用了相比 Docker 更轻量的 <a href="https://containerd.io/" target="_blank" rel="noopener">containerd</a> 作为容器运行时（Docker 并不是唯一的容器选择）</li><li>去掉了 k8s 的 Legacy, alpha, non-default features</li><li>用 sqlite3 作为默认的存储，而不是 etcd</li><li>其他的一些优化，最终 k3s 只是一个 binary 文件，非常易于部署</li></ul><p>所以 k3s 适用于边缘计算，IoT 等资源紧张的场景。同时 k3s 也是非常容易部署的，官网上提供了<a href="https://raw.githubusercontent.com/rancher/k3s/master/install.sh" target="_blank" rel="noopener">一键部署的脚本</a>。</p><h3 id="安装环境"><a href="#安装环境" class="headerlink" title="安装环境"></a>安装环境</h3><p> 本文的安装环境：</p><ul><li>阿里云 1C2G 机器若干，运行 CentOS 7.6 64位</li><li>k3s <a href="https://github.com/rancher/k3s/releases/tag/v0.5.0" target="_blank" rel="noopener">v0.5.0</a></li></ul><h3 id="安装脚本"><a href="#安装脚本" class="headerlink" title="安装脚本"></a>安装脚本</h3><p><code>https://get.k3s.io</code> 这是 k3s 的安装脚本。我们直接运行这个脚本就可以安装 k3s。因为我们需要在 k3s 运行之前做一些事情，所以运行脚本的时候我们选择只安装，不启动 k3s</p><pre><code>// 下载脚本curl -sfL https://get.k3s.io &gt; install.sh// 运行脚本INSTALL_K3S_SKIP_START=true ./install.sh</code></pre><p>如果速度太慢，可以把 binary 手动下载然后传到国内的对象存储，然后去脚本里面把 binary 地址改成国内的地址。</p><h3 id="镜像获取-amp-amp-启动-k3s"><a href="#镜像获取-amp-amp-启动-k3s" class="headerlink" title="镜像获取 &amp;&amp; 启动 k3s"></a>镜像获取 &amp;&amp; 启动 k3s</h3><p>安装玩之后我们还要做一个事情，就是把之后 k3s 要用到的一些镜像下载到本地。因为 k3s 用的 pause 镜像地址是 gcr.io 的，所以国内是访问不了的。</p><blockquote><p>k3s 提供了 air-gap support 这个特性来支持镜像的本地预加载。这个特性本身是为了无法访问外网的环境准备的。国内的环境其实也是等于是没法访问外网，所以刚好可以用这个特性解决问题。一开始 k3s 刚出来的时候没有这个特性，所以只能重新编译 k3s binary 然后改掉硬编码的镜像地址了。</p></blockquote><p>我们去 k3s 的 release 里面获取 <code>k3s-airgap-images-$ARCH.tar</code> $ARCH 是我们服务器的 CPU 架构。想办法把这个文件传到服务器上。</p><p>然后运行：</p><pre><code>sudo mkdir -p /var/lib/rancher/k3s/agent/images/sudo cp ./k3s-airgap-images-$ARCH.tar /var/lib/rancher/k3s/agent/images/</code></pre><p>最后运行：</p><pre><code>systemctl start k3s</code></pre><p>然后 k3s 就启动了。</p><p>可以运行 <code>kubectl</code> 和 <code>netstat -nplt</code> 查看 k3s 是否正常启动。值得注意的是要看看 <code>ipconfig</code> 里是否出现了 flannel 的网络设备。之后的操作就和 k8s 一样了，用 <code>kubectl</code> 命令进行操作。</p><p>这个时候我们启动的是一个 k3s server（master节点），当然这个节点本身也是一个 worker Node。我们如果需要更多的节点，就需要手动加入更多。</p><h3 id="加入节点"><a href="#加入节点" class="headerlink" title="加入节点"></a>加入节点</h3><p>和 master 节点安装一样，首先获取安装脚本，然后运行：</p><pre><code>K3S_TOKEN=xxx K3S_URL=https://server-url:6443 INSTALL_K3S_SKIP_START=true ./install.sh </code></pre><blockquote><p>token 是从 master 节点的 <code>/var/lib/rancher/k3s/server/node-token</code> 文件里获取的。</p></blockquote><p>就可以安装节点了，然后按上一节里的复制镜像到本地目录。</p><p>最后启动节点：</p><pre><code>systemctl start k3s-agent</code></pre><h3 id="一点黑魔法"><a href="#一点黑魔法" class="headerlink" title="一点黑魔法"></a>一点黑魔法</h3><p>如果我们用的是多台学生机，就肯定会遇到一个问题，这些机器不在同一个局域网里面，用内网 IP 是没法相互访问的（因为一个账号只能有一台机器，专有网络是无法跨账号的）。</p><p>如果机器之前内网 IP 不通，那 k3s 就算搭建起来了，节点之间的通信也是有问题的。</p><p>因此我们需要一点黑魔法，这个魔法就是阿里云<a href="https://help.aliyun.com/product/59006.html" target="_blank" rel="noopener">云企业网</a>。云企业网可以将多个 VPC 连接起来，可以跨账号，跨区域。这就是我们想要的东西！阿里云真香~</p><p>把我们的所有节点都加入云企业网之后，所有的节点之间就是内网互通的了。这之后再安装 k3s，整个集群就可以正常的工作了。</p><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p>k3s 默认安装了 Traefik，如果不想用这个，可以在启动的时候加入参数：</p><pre><code>INSTALL_K3S_SKIP_START=true INSTALL_K3S_EXEC=&quot;--no-deploy traefik&quot; ./install.sh</code></pre><p>官方的安装脚本写的很好，如果需要一些定制化的安装需求，多看看安装脚本，往往可以解决问题。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://k3s.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;k3s&lt;/a&gt; 刚出来的时候，我刚好看到这个项目，然后了解到这是一个轻量级的 k8s 发行版。之前刚好遇到在阿里云&lt;a href=&quot;https://promotion.aliyun.com/ntms/act/campus2018.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;学生机&lt;/a&gt;（1C2G）上安装 k8s 后内存占用太多的问题，因此就决定尝试。最后的效果超出了预期，k3s 可以帮助我们在低配置机器上运行 k8s 集群，缓解了 k8s 对于资源占用的压力，降低了服务器的成本。&lt;/p&gt;
    
    </summary>
    
      <category term="k3s" scheme="http://yoursite.com/categories/k3s/"/>
    
      <category term="k8s" scheme="http://yoursite.com/categories/k3s/k8s/"/>
    
      <category term="cloud" scheme="http://yoursite.com/categories/k3s/k8s/cloud/"/>
    
    
      <category term="k3s" scheme="http://yoursite.com/tags/k3s/"/>
    
      <category term="k8s" scheme="http://yoursite.com/tags/k8s/"/>
    
      <category term="cloud" scheme="http://yoursite.com/tags/cloud/"/>
    
  </entry>
  
  <entry>
    <title>木犀的第二代后端架构</title>
    <link href="http://yoursite.com/2019/03/27/muxi-arch-2019/"/>
    <id>http://yoursite.com/2019/03/27/muxi-arch-2019/</id>
    <published>2019-03-27T09:58:31.000Z</published>
    <updated>2019-03-27T12:46:17.953Z</updated>
    
    <content type="html"><![CDATA[<p>2019 是木犀的第五年。前五年，木犀的后端发展经过了一个从无到有的过程，从最初的单机 Flask + 自己部署的数据库到如今基于 K8s 的分布式架构 + 云数据库，从 Python 到 Golang，我们逐渐确立了我们的第二代架构。</p><p>如今，已经没有必要去细究这个过程究竟是如何逐步发生的。现在要做的，就是确立我们现在的新架构，并且在今年把我们的主力应用都切换到新架构上，为下一个五年的发展打好基础。</p><p>新架构，简单的说，就是 Cloud Native，拥抱了当前非常流行的容器云，Golang 等云原生相关的技术。其实这套架构没什么特殊的，用到的都是热门技术，但这依然是我们经过了过去几年间不断的摸索而总结出来的。后面我们会在这套技术上深入挖掘，争取知其然，知其所以然。</p><a id="more"></a><p>首先一张图总结：</p><p><img src="http://ossworkbench.muxixyz.com/1553690740.4136577.Screen_Shot_2019-03-27_at_8.43.55_PM.png" alt=""></p><h3 id="应用技术栈"><a href="#应用技术栈" class="headerlink" title="应用技术栈"></a>应用技术栈</h3><blockquote><p>动态语言一时爽</p></blockquote><p>我们需要一门强类型，静态编译型，高性能，高并发，稳定，健壮的语言，来作为我们的主力开发语言。Golang 就是一样的一门语言，而且我们重点关注的云计算领域，Golang 是统治级的存在。所以我们选了 Golang 作为今后我们应用开发的主力语言。</p><p>Java 也是备选之一，今后可以调研 Java 的运行时开销，来确定是不是使用 Go + Java 作为我们的技术栈。目前来看 Golang 做 Web 开发还是比较快的，Java 的开发速度也是一个不确定因素。Java 的好处就是就业市场上需求广，生态繁荣，Web 开发上社区有多年的积累，大数据领域 Java 是绝对的主流。</p><p>在服务内部通信上我们使用 <a href="https://grpc.io/" target="_blank" rel="noopener">grpc</a>。grpc 对 Golang 的技术栈比较友好，以后也有跨语言通信的能力。服务发现和服务治理之类，K8s 其实已经做好了，所以我们不用做特别的工作。</p><p>在开发流程规范上，我们针对 Go Web 项目有一个<a href="https://github.com/muxih4ck/Go-Web-Application-Guideline" target="_blank" rel="noopener">规范</a>。</p><h3 id="DB"><a href="#DB" class="headerlink" title="DB"></a>DB</h3><p>主力是阿里云 RDS 上的 MySQL，单独的云端数据库让我们免除了运维的麻烦，但 SQL 查询性能，建索引等等还是需要我们自己把关。</p><p>另外自己搭了一个 Mongo，因为阿里云上的 Mongo 太贵了。Mongo 要做好自动数据备份工作。</p><h3 id="容器调度"><a href="#容器调度" class="headerlink" title="容器调度"></a>容器调度</h3><p>Kubernetes 在木犀已经投入生产接近两年了，我们在 Kubernetes 上有了一定的使用过经验，踩了一些坑。对于原理我们还需要深入研究。</p><blockquote><p>对 K8s 抱有疑问的同学可以看这篇文章 <a href="https://www.yuque.com/huarou/gd4szw/remeed" target="_blank" rel="noopener">Kubernetes 是下一代操作系统 | 面向 Kubernetes 编程</a> </p></blockquote><p>下一步要做的就是开发一个部署平台，可以自动化的构建镜像和部署应用。但这个事情需要对 k8s 掌握的比较好，因此进展比较慢。</p><p>今年出现的 <a href="https://k3s.io/" target="_blank" rel="noopener">k3s</a> 也会被我们投入使用，k3s 适合在嵌入式环境和边缘计算等场景使用，是 k8s 的裁剪版，大大降低了运行时的内存占用。我们会用大量的廉价机器（学生机）组成集群，作为我们的测试集群，以及运行一些分布式任务。</p><h3 id="错误监控"><a href="#错误监控" class="headerlink" title="错误监控"></a>错误监控</h3><p>用户经常会报错误，当然大部分时候用户没有办法反馈给你，出错了就默默的卸载了应用。我们需要一种方式来监控错误，以便提升发现问题，提升用户体验。</p><p>想看错误，就要看应用该的日志。使用 K8s 部署之后，日志需要登录到集群去看。而且我们无法统计错误出现的次数和种类，除非把做好日志的分析工作。</p><p>有没有一种服务可以简单方便的提供错误的监控呢？<a href="https://sentry.io/" target="_blank" rel="noopener">Sentry</a> 就是我们想要的答案。</p><p>错误监控的效果其实可以通过对日志进行分析处理而达到。但日志处理需要不菲的机器配置支持，因此现阶段可以自建，配置要求普通的 Sentry 是我们的最佳选择。</p><h3 id="日志"><a href="#日志" class="headerlink" title="日志"></a>日志</h3><p>日志处理上，ELK stack 是不错的选择。但目前来看，日志分析还不是刚需，因此在第二代架构中，这是一个可选的部分。</p><h3 id="App-监控"><a href="#App-监控" class="headerlink" title="App 监控"></a>App 监控</h3><p>我们使用 Influxdb 搭建简单的服务，App 把日志上报到 Influx，后续可以做用户留存，错误监控等等作用。</p><h3 id="中间件"><a href="#中间件" class="headerlink" title="中间件"></a>中间件</h3><p>消息队列目前用 Rabbitmq，还需要采坑。缓存一直用的 Redis。</p><h3 id="未来"><a href="#未来" class="headerlink" title="未来"></a>未来</h3><p>广度上，对于 Service Mesh/Serverless/Spark 等等这些新老技术，我们没有涉足过的，我们在今后都要去调研和尝试。新技术让我们的应用开发边的更加稳定和简单。</p><p>深度上，对于目前我们使用的中间件和 DB，我们要作为我们的主要研究方向，深入的了解和学习。最终只有成为领域专家，才能在这个技术圈子有立足之地。才能形成我们团队的技术壁垒。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;2019 是木犀的第五年。前五年，木犀的后端发展经过了一个从无到有的过程，从最初的单机 Flask + 自己部署的数据库到如今基于 K8s 的分布式架构 + 云数据库，从 Python 到 Golang，我们逐渐确立了我们的第二代架构。&lt;/p&gt;
&lt;p&gt;如今，已经没有必要去细究这个过程究竟是如何逐步发生的。现在要做的，就是确立我们现在的新架构，并且在今年把我们的主力应用都切换到新架构上，为下一个五年的发展打好基础。&lt;/p&gt;
&lt;p&gt;新架构，简单的说，就是 Cloud Native，拥抱了当前非常流行的容器云，Golang 等云原生相关的技术。其实这套架构没什么特殊的，用到的都是热门技术，但这依然是我们经过了过去几年间不断的摸索而总结出来的。后面我们会在这套技术上深入挖掘，争取知其然，知其所以然。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>后端工作方向介绍</title>
    <link href="http://yoursite.com/2018/09/14/backend-job-road/"/>
    <id>http://yoursite.com/2018/09/14/backend-job-road/</id>
    <published>2018-09-14T03:12:52.000Z</published>
    <updated>2018-09-24T05:37:31.426Z</updated>
    
    <content type="html"><![CDATA[<p>这篇博客是写给团队后端方向的同学看的。主要是介绍我在工作一年之后对于后端工作方向的一些新的认知（虽然职位是前端，但在这个大环境下面，对后端的工作内容还是有一些了解的）。让大家在学校里可以对外面工业界的需求有一个大致的认识，在自己的兴趣和就业的导向之间找到一个平衡。</p><a id="more"></a><h3 id="互联网公司的技术部门组织架构"><a href="#互联网公司的技术部门组织架构" class="headerlink" title="互联网公司的技术部门组织架构"></a>互联网公司的技术部门组织架构</h3><h4 id="从单体到分布式：后端工程师的分工"><a href="#从单体到分布式：后端工程师的分工" class="headerlink" title="从单体到分布式：后端工程师的分工"></a>从单体到分布式：后端工程师的分工</h4><p>要理解一个（大型）互联网公司的组织架构，首先要理解一个产品的后端服务从 0 到 1 的发展过程。《大型网站技术架构》这本书对于这个过程有详细的解释。具体的看<a href="https://blog.csdn.net/chaofanwei/article/details/26865169" target="_blank" rel="noopener">这篇博客</a>。</p><p>大致来说就是这样的过程：</p><pre><code>单机系统：Java/PHP/Node/Python + Linux + MySql + Nginx。 ==&gt;流量增大，数据库读写出现瓶颈，数据库分库分表  ==&gt;流量继续增大，单机部署无法应对高并发，于是集群部署，横向拓展 ==&gt;为了提高性能，抽出单独的缓存服务 ==&gt;分布式数据库/分布式存储 ==&gt;微服务架构 ==&gt;最终进化为适应于特定业务场景的复杂分布式架构</code></pre><p>在小型的公司，一般就只有一个后端开发团队，里面没有非常明确的角色分配。服务，数据库，缓存，部署，都要自己来搞。当公司变大的时候，就会出现一些分工。</p><p>开发工程师主要负责开发服务，运维和 DBA 负责服务的部署，数据库的安全和效率。架构师负责设计整个应用的架构。</p><h4 id="中间件-数据库：后端服务的基石"><a href="#中间件-数据库：后端服务的基石" class="headerlink" title="中间件/数据库：后端服务的基石"></a>中间件/数据库：后端服务的基石</h4><p>上一节说到互联网公司的后端架构最终演化为一个分布式的系统。那一个分布式的系统运行，需要许多中间件的支持。比如消息队列，分布式缓存，分布式数据库，微服务框架，容器编排调度等等。这些被其他服务调用的，负责通信/存储的服务，就是所谓的中间件。中间件可以说是连接两个服务的一个中介。</p><p>公司越来越大，就需要有人专门去维护这些公共组件，让开发工程师可以专注于业务逻辑的开发。比如维护自己的消息队列，数据库，缓存，RPC框架等等。在大公司中，开源的方案并不能满足业务需求，所以需要对开源的方案进行改进和创新。</p><p>上一节中的分工里没有提到的，就是中间件研发的工程师。这些工程师在一个大型的互联网公司中是非常重要的。</p><h4 id="数据平台：公司决策的引擎"><a href="#数据平台：公司决策的引擎" class="headerlink" title="数据平台：公司决策的引擎"></a>数据平台：公司决策的引擎</h4><p>科技公司在运行中会产生各种各样的数据，比如用户行为数据，订单数据，访问记录数据。还有服务器的运行状态等等数据。这些数据都是一种资产，是公司来了解业务情况的一个窗口。</p><p>大数据这个方向其实已经非常成熟了，它主要包括数据的搜集，清洗，加工，存储，计算，分析。在这个链路上，需要很多工程师的努力。我们需要工程师来进行数据的清洗和加工，需要工程师来搭建数据仓库，数据加工平台，数据分析平台。需要工程师来优化数据实时和离线计算的引擎。需要工程师来写各个终端的数据上报服务。</p><p>所以大型的互联网公司都会有一个数据部门，让数据产生价值。</p><h4 id="运维-安全：保证服务的高可用和安全性"><a href="#运维-安全：保证服务的高可用和安全性" class="headerlink" title="运维/安全：保证服务的高可用和安全性"></a>运维/安全：保证服务的高可用和安全性</h4><p>传统意义上的运维负责公司服务器的管理，生产环境的稳定性，公司机房的设计和日常维护。大型公司中，还会涉及异地多活等等高可用方案。</p><p>运维这个角色现在也叫运维开发（DevOps）或者 SRE（网站稳定性工程师）。通过脚本和工具进行自动化的运维。比如开发要使用数据库，运维就搭建一个多副本的数据库集群，并且让数据库可以自动备份，还写了一个平台来访问和管理数据库。又或者有一个基础软件，需要安装在 500 台机器上。运维就开发了一个软件，可以让大家一键部署。</p><p>安全则是一个很大的领域，主要负责产品的信息安全。公司里一般都会有安全部。</p><h4 id="一个大型科技公司的技术部门组织架构：从技术部到中台"><a href="#一个大型科技公司的技术部门组织架构：从技术部到中台" class="headerlink" title="一个大型科技公司的技术部门组织架构：从技术部到中台"></a>一个大型科技公司的技术部门组织架构：从技术部到中台</h4><p>小型公司一般只有一个技术部，里面分前端后台等角色。大公司一般是按业务来划分部门。比如电商业务，金融业务，零售业务等等。所谓的业务最终会落地到一个实际的产品中，比如淘宝。一个业务部门可能会同时运营多款产品。</p><p>这里需要引入一个概念，叫中台。和中台相对的就是前台。前台部门就是业务部门，直接开发面向消费者的产品的。中台部门就是负责给前台部门输送弹药的。负责支持前台部门。这里的支持不是帮前台部门写业务代码，而是提供一些基础的服务。比如前文里说到的中间件和数据库，还有数据平台。</p><p>我们在写产品的时候，会有一个抽象公共代码的过程。把多个模块中公共的部分抽出来。那在公司的运营中，我们也可以把公共的职能抽出来，变为中台。</p><p>每个业务部门自己去维护自己的中间件或者数据库，或者搭建数据平台，会造成很大的资源浪费。对部门之前的合作也不利。一个科技公司，统一自己内部的技术栈和基础设施，有几点好处：</p><ul><li>方便内部的技术交流和合作</li><li>营造统一的技术氛围，利于公司内部的认同感</li><li>方便对外输出技术</li><li>降低各部门开发的成本</li></ul><p>其实中台战略的好处是显而易见的，让专业的人做专业的事，前台负责业务上攻城略地，中台部门负责中台能力的搭建，支持前台业务。</p><p>按上述的模式设计的一个虚构的公司 foobar 的技术部门组织架构是这样的：</p><p><img src="http://wx3.sinaimg.cn/large/64c45edcly1fvkgm81vvfj219q0p8di9.jpg" alt=""></p><blockquote><p>需要留意的是，本文里介绍的组织架构只是一个高度理想化的模型。现实中的大型科技公司的组织架构有很多种类。有的公司没有统一的中台部门，中台部门的职能被分散到各个业务部门中。只不过小前台+大中台是本文所推崇的一种组织架构。</p></blockquote><!--木犀 -> 后端 + 平台技术大中台 小前台前台是什么 中台是什么 CTO 主管 下面 中间件 数据库 数据平台 运维 大安全 产品，前台技术中台（数据中台，基础平台，运维中台，安全中台）--><h3 id="职位概览"><a href="#职位概览" class="headerlink" title="职位概览"></a>职位概览</h3><h4 id="研发（Java-C"><a href="#研发（Java-C" class="headerlink" title="研发（Java/C++)"></a>研发（Java/C++)</h4><p>支持各种系统（面向消费者的，或者内部系统）的研发。基础平台，人工智能，数据平台也需要有平台来展现，所以也需要研发工程师。</p><p>简单的说就是写接口的。</p><h4 id="数据研发"><a href="#数据研发" class="headerlink" title="数据研发"></a>数据研发</h4><p>企业的数据中台，从数据的采集，加工，计算，存储，分析，都需要熟悉大数据相关技术（数据仓库/实时计算/离线计算/数据清洗）的工程师来参与。</p><h4 id="基础平台研发（Java-C-Go"><a href="#基础平台研发（Java-C-Go" class="headerlink" title="基础平台研发（Java/C++/Go)"></a>基础平台研发（Java/C++/Go)</h4><p>数据库开发，网络（CDN，机房网络），分布式存储，云计算（虚拟化，操作系统），云计算网络（SDN），中间件（消息队列，缓存，分布式服务框架）</p><h4 id="运维（DevOps-DBA）"><a href="#运维（DevOps-DBA）" class="headerlink" title="运维（DevOps, DBA）"></a>运维（DevOps, DBA）</h4><p>运维开发，负责产品，数据库，中间件的运维，开发相应的软件来做运维自动化。也会做一些监控系统来监控服务的运行情况。</p><h4 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h4><p>算法其实不属于后端，是单独的一个类别，里面有视觉/语音/推荐等等各种小方向的职位。</p><h3 id="应用-vs-底层"><a href="#应用-vs-底层" class="headerlink" title="应用 vs 底层"></a>应用 vs 底层</h3><p>这么多后端方向的职位，我们可以大致分为<strong>应用</strong>和<strong>底层</strong>两个部分。</p><p>应用就是指业务代码，也就是所谓的写 API。</p><p>底层就是指中台下面的几个方向，比如中间件/数据平台等等的研发。当然底层里面也有数据库开发/Linux 内核开发等等偏<strong>系统级软件</strong>的开发。也有<strong>平台软件/中间件</strong>的研发，比如 K8s 这样的容器调度系统，或者是一个消息队列。这样的软件<strong>更靠近应用层</strong>，对底层操作系统的依赖没那么大。</p><p>今后去工作的时候也会遇到应用 vs 底层的选择。能直接投相关的职位当然是最好，大公司一般都会分清楚招的职位具体是做什么。有的公司可能是先以应用研发的名义统一招人，后面才会有具体的方向。先进入这个行业工作，然后选择自己的方向，也是一个不错的道路。</p><h3 id="计算机基础要学到什么样的程度？"><a href="#计算机基础要学到什么样的程度？" class="headerlink" title="计算机基础要学到什么样的程度？"></a>计算机基础要学到什么样的程度？</h3><p>国内的教学比较水。我们对一门课的要求是按国外 CS 强校的标准来定的。国外课程的特点是这样的：</p><ul><li>教学内容紧凑，只讲核心内容，并且最后会涉及目前时代的新技术，作为拓展</li><li>鼓励阅读 Paper 了解技术的原始发表文件，培养学术习惯</li><li>作业和 Lab 相辅相成，课后需要花很多时间</li><li>作业和 Lab 有老师的 Office Hour（答疑时间）和助教指导，还有课程论坛。有的会由助教开专门的 Lab 复盘课</li><li>日常出勤占比例很低，普遍会提供课程录像供后面查阅</li><li>Lab 设计合理，一般都是环环相扣，系统类的课，一般最终会要求写一个完整的系统，这个系统是这门课的内容的精髓和总结</li></ul><p>后端比较关键的课程是这些（除了数据结构和算法，以及编程思想和设计模式一类的课程）：</p><ul><li>计算机网络 </li><li>数据库系统</li><li>分布式系统</li><li>操作系统</li></ul><p>这样的课，我们的要求就是，针对每一门课，找到一门国外的课，跟着学完内容（PPT 和要求读的课本相关章节看过，理解，如果有课程视频要看视频），<strong>做完 Lab</strong>。作业可以根据自己兴趣来做。重点还是在 Lab。</p><p>这些 Lab 一般都是要你实现一些底层的东西，比如数据库系统，会让你实现 Buffer Pool 和 B+ Tree。计算机网络让你实现一个完整的 Web Server，还有 TCP 协议等等。分布式系统一般要求实现简单的分布式 KV 存储，还有 Raft 等分布式一致性协议。这些东西都是一门学科最精髓的，而你手动实现过了，那你对这个东西的理解自然远远超过普通的刷题看书的学习方式。</p><blockquote><p>国外 CS 强校招人要求很高，比如 CMU，会要求你在高中就有专业方面的经验和极大的兴趣。所以大家如果做 Lab 做的不爽是很正常的。但在我们整个团队的努力下，我们可以把这些 Lab 一个个做出来。留下攻略给后人。这样我们团队就掌握了这些技术。（国外 CS 名校毕业生比较抢手也是因为他们受到的教育的确不一般）</p></blockquote><h3 id="什么样的技术栈比较好找工作？"><a href="#什么样的技术栈比较好找工作？" class="headerlink" title="什么样的技术栈比较好找工作？"></a>什么样的技术栈比较好找工作？</h3><p>应用研发的需求自然要大于底层研发，也只有比较大的企业才会有专职的底层研发。所以说如果技术栈和当前公司流行的技术栈匹配的话，就比较好找工作。现在比较流行的后端开发语言就是 <strong>Java 和 C++</strong> 了。电商公司一般都是 Java。腾讯和百度是用的 C++ 比较多。微博用的是 Java 和 PHP。用 PHP 的大公司也有，但在公司里不是主流，所以 PHP 就可以排除了。同样 Python 在大公司里主要还是做一些辅助的工作，比如要开发一个运维系统，就可以用 Python 来写，这样比较快。</p><p>一些创业公司也喜欢用 Python 来快速搭建产品原型。比如豆瓣/知乎/字节跳动。但后期如果业务复杂了，一般会用 Java 重构。</p><p>Go 语言在大公司用的也挺多，主要是做云计算方向。但很少有大公司直接招 Go 工程师，所以首先精通 Java/C++ 的一种，然后还懂 Go，是比较好的一种选择。</p><p>底层研发用的语言就比较受限制了。中间件一般是 Java/C++，系统软件一般是 C/C++。云计算相关的底层系统，Go 用的会比较多。其他语言在底层开发里是很少见到的。</p><h3 id="团队后端的几个方向"><a href="#团队后端的几个方向" class="headerlink" title="团队后端的几个方向"></a>团队后端的几个方向</h3><p>应用研发由整个后端组的同学来承担（客户端那边也会参与一些中间层服务的编写，简单的全栈工程）</p><p>深入的专业方向大概有以下几个：</p><ul><li>Data 组：数据库/缓存中间件/大数据存储/大数据计算/日志存储和分析/分布式存储原理</li><li>Cloud 组： 云计算/容器编排/分布式系统/消息队列中间件/微服务框架/RPC框架</li><li>DevOps 组： 自动化运维/高可用/DBA（规划）</li></ul><p>产出：</p><ul><li>团队相关方向业务的支持（比如日志系统，比如容器调度）</li><li>相关方向课程的 Lab 题解和指南</li><li>相关方向开源项目的原理</li><li>产出一个我们团队在这方向的小项目（简化版），可以用来学习原理，就像大学都有自己为了教操作系统课程而开发的玩具操作系统一样</li><li>相关方向新技术的调研和应用</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇博客是写给团队后端方向的同学看的。主要是介绍我在工作一年之后对于后端工作方向的一些新的认知（虽然职位是前端，但在这个大环境下面，对后端的工作内容还是有一些了解的）。让大家在学校里可以对外面工业界的需求有一个大致的认识，在自己的兴趣和就业的导向之间找到一个平衡。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Webpack 4 配置最佳实践</title>
    <link href="http://yoursite.com/2018/06/19/webpack-4-config-best-practices/"/>
    <id>http://yoursite.com/2018/06/19/webpack-4-config-best-practices/</id>
    <published>2018-06-19T11:12:49.000Z</published>
    <updated>2018-06-19T11:14:44.274Z</updated>
    
    <content type="html"><![CDATA[<p>Webpack 4 发布已经有一段时间了。Webpack 的版本号已经来到了 4.12.x。但因为 Webpack 官方还没有完成迁移指南，在文档层面上还有所欠缺，大部分人对升级 Webpack 还是一头雾水。</p><p>不过 Webpack 的开发团队已经写了一些零散的文章，官网上也有了新版配置的文档。社区中一些开发者也已经成功试水，升级到了 Webpack 4，并且总结成了博客。所以我也终于去了解了 Webpack 4 的具体情况。以下就是我对迁移到 Webpack 4 的一些经验。</p><p>本文的重点在：</p><ul><li>Webpack 4 在配置上带来了哪些便利？要迁移需要修改配置文件的哪些内容？</li><li>之前的 Webpack 配置最佳实践在 Webpack 4 这个版本，还适用吗？</li></ul><a id="more"></a><h3 id="Webpack-4-之前的-Webpack-最佳实践"><a href="#Webpack-4-之前的-Webpack-最佳实践" class="headerlink" title="Webpack 4 之前的 Webpack 最佳实践"></a>Webpack 4 之前的 Webpack 最佳实践</h3><p>这里以 Vue 官方的 Webpack 模板 <a href="https://github.com/vuejs-templates/webpack" target="_blank" rel="noopener">vuejs-templates/webpack</a> 为例，说说 Webpack 4 之前，社区里比较成熟的 Webpack 配置文件是怎样组织的。</p><h4 id="区分开发和生产环境"><a href="#区分开发和生产环境" class="headerlink" title="区分开发和生产环境"></a>区分开发和生产环境</h4><p>大致的目录结构是这样的：</p><pre><code>+ build+ config+ src</code></pre><p>在 build 目录下有四个 webpack 的配置。分别是：</p><ul><li>webpack.base.conf.js</li><li>webpack.dev.conf.js</li><li>webpack.prod.conf.js</li><li>webpack.test.conf.js</li></ul><p>这分别对应开发、生产和测试环境的配置。其中 webpack.base.conf.js 是一些公共的配置项。我们使用 <a href="https://github.com/survivejs/webpack-merge" target="_blank" rel="noopener">webpack-merge</a> 把这些公共配置项和环境特定的配置项 merge 起来，成为一个完整的配置项。比如 webpack.dev.conf.js 中：</p><pre><code class="javascript">&#39;use strict&#39;const merge = require(&#39;webpack-merge&#39;)const baseWebpackConfig = require(&#39;./webpack.base.conf&#39;)const devWebpackConfig = merge(baseWebpackConfig, {   ...})</code></pre><p>这三个环境不仅有一部分配置不同，更关键的是，每个配置中用 <code>webpack.DefinePlugin</code> 向代码注入了 <code>NODE\_ENV</code> 这个环境变量。</p><p>这个变量在不同环境下有不同的值，比如 dev 环境下就是 development。这些环境变量的值是在 config 文件夹下的配置文件中定义的。Webpack 首先从配置文件中读取这个值，然后注入。比如这样：</p><p><em>build/webpack.dev.js</em></p><pre><code class="javascript">plugins: [  new webpack.DefinePlugin({    &#39;process.env&#39;: require(&#39;../config/dev.env.js&#39;)  }),]</code></pre><p><em>config/dev.env.js</em></p><pre><code class="javascript">module.exports ={  NODE_ENV: &#39;&quot;development&quot;&#39;}</code></pre><p>至于不同环境下环境变量具体的值，比如开发环境是 development，生产环境是 production，其实是大家约定俗成的。</p><p>框架、库的作者，或者是我们的业务代码里，都会有一些根据环境做判断，执行不同逻辑的代码，比如这样：</p><pre><code class="javascript">if (process.env.NODE_ENV !== &#39;production&#39;) {  console.warn(&quot;error!&quot;)}</code></pre><p>这些代码会在代码压缩的时候被预执行一次，然后如果条件表达式的值是 true，那这个 true 分支里的内容就被移除了。这是一种编译时的死代码优化。这种区分不同的环境，并给环境变量设置不同的值的实践，让我们开启了编译时按环境对代码进行针对性优化的可能。</p><h4 id="Code-Splitting-amp-amp-Long-term-caching"><a href="#Code-Splitting-amp-amp-Long-term-caching" class="headerlink" title="Code Splitting &amp;&amp; Long-term caching"></a>Code Splitting &amp;&amp; Long-term caching</h4><p>Code Splitting 一般需要做这些事情：</p><ul><li>为 Vendor 单独打包（Vendor 指第三方的库或者公共的基础组件，因为 Vendor 的变化比较少，单独打包利于缓存）</li><li>为 Manifest （Webpack 的 Runtime 代码）单独打包</li><li>为不同入口的公共业务代码打包（同理，也是为了缓存和加载速度）</li><li>为异步加载的代码打一个公共的包</li></ul><p>Code Splitting 一般是通过配置 CommonsChunkPlugin 来完成的。一个典型的配置如下，分别为 vendor、manifest 和 vendor-async 配置了 CommonsChunkPlugin。</p><pre><code class="javascript">    new webpack.optimize.CommonsChunkPlugin({      name: &#39;vendor&#39;,      minChunks (module) {        return (          module.resource &amp;&amp;          /\.js$/.test(module.resource) &amp;&amp;          module.resource.indexOf(            path.join(__dirname, &#39;../node_modules&#39;)          ) === 0        )      }    }),    new webpack.optimize.CommonsChunkPlugin({      name: &#39;manifest&#39;,      minChunks: Infinity    }),    new webpack.optimize.CommonsChunkPlugin({      name: &#39;app&#39;,      async: &#39;vendor-async&#39;,      children: true,      minChunks: 3    }),</code></pre><p>CommonsChunkPlugin 的特点就是配置比较难懂，大家的配置往往是复制过来的，这些代码基本上成了模板代码（boilerplate）。如果 Code Splitting 的要求简单倒好，如果有比较特殊的要求，比如把不同入口的 vendor 打不同的包，那就很难配置了。总的来说配置 Code Splitting 是一个比较痛苦的事情。</p><p>而 Long-term caching 策略是这样的：给静态文件一个很长的缓存过期时间，比如一年。然后在给文件名里加上一个 hash，每次构建时，当文件内容改变时，文件名中的 hash 也会改变。浏览器在根据文件名作为文件的标识，所以当 hash 改变时，浏览器就会重新加载这个文件。</p><p>Webpack 的 Output 选项中可以配置文件名的 hash，比如这样：</p><pre><code class="javascript">output: {  path: config.build.assetsRoot,  filename: utils.assetsPath(&#39;js/[name].[chunkhash].js&#39;),  chunkFilename: utils.assetsPath(&#39;js/[id].[chunkhash].js&#39;)},</code></pre><h3 id="Webpack-4-下的最佳实践"><a href="#Webpack-4-下的最佳实践" class="headerlink" title="Webpack 4 下的最佳实践"></a>Webpack 4 下的最佳实践</h3><h4 id="Webpack-4-的变与不变"><a href="#Webpack-4-的变与不变" class="headerlink" title="Webpack 4 的变与不变"></a>Webpack 4 的变与不变</h4><p>Webpack 4 这个版本的 API 有一些 breaking change，但不代表说这个版本就发生了翻天覆地的变化。其实变化的点只有几个。而且只要你仔细了解了这些变化，你一定会拍手叫好。</p><p>迁移到 Webpack 4 也只需要检查一下 <a href="https://dev.to/flexdinesh/upgrade-to-webpack-4---5bc5" target="_blank" rel="noopener">checklist</a>，看看这些点是否都覆盖到了，就可以了。</p><h4 id="开发和生产环境的区分"><a href="#开发和生产环境的区分" class="headerlink" title="开发和生产环境的区分"></a>开发和生产环境的区分</h4><p>Webpack 4 引入了 <a href="https://webpack.js.org/concepts/mode/" target="_blank" rel="noopener">mode</a> 这个选项。这个选项的值可以是 development 或者 production。</p><p>设置了 mode 之后会把 <code>process.env.NODE\_ENV</code> 也设置为 development 或者 production。然后在 production 模式下，会默认开启 UglifyJsPlugin 等等一堆插件。</p><p>Webpack 4 支持零配置使用，可以从命令行指定 entry 的位置，如果不指定，就是 <code>src/index.js</code>。mode 参数也可以从命令行参数传入。这样一些常用的生产环境打包优化都可以直接启用。</p><p>我们需要注意，Webpack 4 的零配置是有限度的，如果要加上自己想加的插件，或者要加多个 entry，还是需要一个配置文件。</p><p>虽然如此，Webpack 4 在各个方面都做了努力，努力让零配置可以做的事情更多。这种内置优化的方式使得我们在项目起步的时候，可以把主要精力放在业务开发上，等后期业务变复杂之后，才需要关注配置文件的编写。</p><p>在 Webpack 4 推出 mode 这个选项之前，如果想要为不同的开发环境打造不同的构建选项，我们只能通过建立多个 Webpack 配置且分别设置不同的环境变量值这种方式。这也是社区里的最佳实践。</p><p>Webpack 4 推出的 mode 选项，其实是一种<strong>对社区中最佳实践的吸收</strong>。这种思路我是很赞同的。开源项目来自于社区，在社区中成长，从社区中吸收养分，然后回报社区，这是一个良性循环。最近我在很多前端项目中都看到了类似的趋势。接下来要讲的其他几个 Webpack 4 的特性也是和社区的反馈离不开的。</p><p>那么上文中介绍的使用多个 Webpack 配置，以及手动环境变量注入的方式，是否在 Webpack 4 下就不适用了呢？其实不然。<strong>在Webpack 4 下，对于一个正经的项目，我们依然需要多个不同的配置文件</strong>。如果我们对为测试环境的打包做一些特殊处理，我们还需要在那个配置文件里用 <code>webpack.DefinePlugin</code> 手动注入 <code>NODE\_ENV</code> 的值（比如 test）。</p><blockquote><p>Webpack 4 下如果需要一个 test 环境，那 test 环境的 mode 也是 development。因为 mode 只有开发和生产两种，测试环境应该是属于开发阶段。</p></blockquote><h4 id="第三方库-build-的选择"><a href="#第三方库-build-的选择" class="headerlink" title="第三方库 build 的选择"></a>第三方库 build 的选择</h4><p>在 Webpack 3 时代，我们需要在生产环境的的 Webpack 配置里给第三方库设置 alias，把这个库的路径设置为 production build 文件的路径。以此来引入生产版本的依赖。</p><p>比如这样：</p><pre><code class="javascript">resolve: {  extensions: [&quot;.js&quot;, &quot;.vue&quot;, &quot;.json&quot;],  alias: {    vue$: &quot;vue/dist/vue.runtime.min.js&quot;  }},</code></pre><p>在 Webpack 4 引入了 mode 之后，对于部分依赖，我们可以不用配置 alias，比如 React。React 的入口文件是这样的：</p><pre><code class="javascript">&#39;use strict&#39;;if (process.env.NODE_ENV === &#39;production&#39;) {  module.exports = require(&#39;./cjs/react.production.min.js&#39;);} else {  module.exports = require(&#39;./cjs/react.development.js&#39;);}</code></pre><p>这样就实现了 0 配置自动选择生产 build。</p><p>但大部分的第三库并没有做这个入口的环境判断。所以这种情况下我们还是需要手动配置 alias。</p><h4 id="Code-Splitting"><a href="#Code-Splitting" class="headerlink" title="Code Splitting"></a>Code Splitting</h4><p>Webpack 4 下还有一个大改动，就是废弃了 CommonsChunkPlugin，引入了 <code>optimization.splitChunks</code> 这个选项。</p><p><code>optimization.splitChunks</code> 默认是不用设置的。如果 mode 是 production，那 Webpack 4 就会开启 Code Splitting。</p><blockquote><p>默认 Webpack 4 只会对按需加载的代码做分割。如果我们需要配置初始加载的代码也加入到代码分割中，可以设置 <code>splitChunks.chunks</code> 为 <code>&#39;all&#39;</code>。</p></blockquote><p>Webpack 4 的 Code Splitting 最大的特点就是配置简单（0配置起步），和<strong>基于内置规则自动拆分</strong>。内置的代码切分的规则是这样的：</p><ul><li>新 bundle 被两个及以上模块引用，或者来自 node_modules</li><li>新 bundle 大于 30kb （压缩之前）</li><li>异步加载并发加载的 bundle 数不能大于 5 个</li><li>初始加载的 bundle 数不能大于 3 个</li></ul><p>简单的说，Webpack 会把代码中的公共模块自动抽出来，变成一个包，前提是这个包大于 30kb，不然 Webpack 是不会抽出公共代码的，因为增加一次请求的成本是不能忽视的。</p><p>具体的业务场景下，具体的拆分逻辑，可以看 <a href="https://webpack.js.org/plugins/split-chunks-plugin/" target="_blank" rel="noopener">SplitChunksPlugin 的文档</a>以及 <a href="https://medium.com/webpack/webpack-4-code-splitting-chunk-graph-and-the-splitchunks-optimization-be739a861366" target="_blank" rel="noopener">webpack 4: Code Splitting, chunk graph and the splitChunks optimization</a> 这篇博客。这两篇文章基本罗列了所有可能出现的情况。</p><p>如果是普通的应用，Webpack 4 内置的规则就足够了。</p><p>如果是特殊的需求，Webpack 4 的 <code>optimization.splitChunks</code> API也可以满足。</p><p>splitChunks 有一个参数叫 cacheGroups，这个参数类似之前的 CommonChunks 实例。cacheGroups 里每个对象就是一个用户定义的 chunk。</p><p>之前我们讲到，Webpack 4 内置有一套代码分割的规则，那用户也可以自定义 cacheGroups，也就是自定义 chunk。那一个 module 应该被抽到哪个 chunk 呢？这是由 cacheGroups 的抽取范围控制的。每个 cacheGroups 都可以定义自己抽取模块的范围，也就是哪些文件中的公共代码会抽取到自己这个 chunk 中。不同的 cacheGroups 之间的模块范围如果有交集，我们可以用 priority 属性控制优先级。Webpack 4 默认的抽取的优先级是最低的，所以模块会优先被抽取到用户的自定义 chunk 中。</p><blockquote></blockquote><p>splitChunksPlugin 提供了两种控制 chunk 抽取模块范围的方式。一种是 test 属性。这个属性可以传入字符串、正则或者函数，所有的 module 都会去匹配 test 传入的条件，如果条件符合，就被纳入这个 chunk 的备选模块范围。如果我们传入的条件是字符串或者正则，那匹配的流程是这样的：首先匹配 module 的路径，然后匹配 module 之前所在 chunk 的 name。</p><p>比如我们想把所有 node_modules 中引入的模块打包成一个模块：</p><pre><code class="javascript">  vendors1: {    test: /[\\/]node_modules[\\/]/,    name: &#39;vendor&#39;,    chunks: &#39;all&#39;,  }</code></pre><p>因为从 node_modules 中加载的依赖路径中都带有 node_modules，所以这个正则会匹配所有从 node_modules 中加载的依赖。</p><p>test 属性可以以 module 为单位控制 chunk 的抽取范围，是一种细粒度比较小的方式。splitChunksPlugin 的第二种控制抽取模块范围的方式就是 chunks 属性。chunks 可以是字符串，比如 <code>&#39;all&#39;|&#39;async&#39;|&#39;initial&#39;</code>，分别代表了全部 chunk，按需加载的 chunk 以及初始加载的 chunk。chunks 也可以是一个函数，在这个函数里我们可以拿到 <code>chunk.name</code>。这给了我们通过入口来分割代码的能力。这是一种细粒度比较大的方式，以 chunk 为单位。</p><p>举个例子，比如我们有 a, b, c 三个入口。我们希望 a，b 的公共代码单独打包为 common。也就是说 c 的代码不参与公共代码的分割。</p><p>我们可以定义一个 cacheGroups，然后设置 chunks 属性为一个函数，这个函数负责过滤这个 cacheGroups 包含的 chunk 是哪些。示例代码如下：</p><pre><code>  optimization: {    splitChunks: {      cacheGroups: {        common: {          chunks(chunk) {            return chunk.name !== &#39;c&#39;;          },          name: &#39;common&#39;,          minChunks: 2,        },      },    },  },</code></pre><p>上面配置的意思就是：我们想把 a，b 入口中的公共代码单独打包为一个名为 common 的 chunk。使用 <code>chunk.name</code>，我们可以轻松的完成这个需求。</p><p>在上面的情况中，我们知道 chunks 属性可以用来按入口切分几组公共代码。现在我们来看一个稍微复杂一些的情况：对不同分组入口中引入的 node_modules 中的依赖进行分组。</p><p>比如我们有 a, b, c, d 四个入口。我们希望 a，b 的依赖打包为 vendor1，c, d 的依赖打包为 vendor2。</p><p>这个需求要求我们对入口和模块都做过滤，所以我们需要使用 test 属性这个细粒度比较小的方式。我们的思路就是，写两个 cacheGroup，一个 cacheGroup 的判断条件是：如果 module 在 a 或者 b chunk 被引入，并且 module 的路径包含 <code>node\_modules</code>，那这个 module 就应该被打包到 vendors1 中。 vendors2 同理。</p><pre><code class="javascript">  vendors1: {    test: module =&gt; {      for (const chunk of module.chunksIterable) {            if (chunk.name &amp;&amp; /(a|b)/.test(chunk.name)) {                if (module.nameForCondition() &amp;&amp; /[\\/]node_modules[\\/]/.test(module.nameForCondition())) {                 return true;             }            }       }      return false;    },    minChunks: 2,    name: &#39;vendors1&#39;,    chunks: &#39;all&#39;,  },  vendors2: {    test: module =&gt; {      for (const chunk of module.chunksIterable) {            if (chunk.name &amp;&amp; /(c|d)/.test(chunk.name)) {                if (module.nameForCondition() &amp;&amp; /[\\/]node_modules[\\/]/.test(module.nameForCondition())) {                 return true;             }            }       }      return false;    },    minChunks: 2,    name: &#39;vendors2&#39;,    chunks: &#39;all&#39;,  },};</code></pre><h4 id="Long-term-caching"><a href="#Long-term-caching" class="headerlink" title="Long-term caching"></a>Long-term caching</h4><p>Long-term caching 这里，基本的操作和 Webpack 3 是一样的。不过 Webpack 3 的 Long-term caching 在操作的时候，有个小问题，这个问题是关于 chunk 内容和 hash 变化不一致的：</p><p><strong>在公共代码 Vendor 内容不变的情况下，添加 entry，或者 external 依赖，或者异步模块的时候，Vendor 的 hash 会改变</strong>。</p><p>之前 Webpack 官方的专栏里面有一篇文章讲这个问题：<a href="https://medium.com/webpack/predictable-long-term-caching-with-webpack-d3eee1d3fa31" target="_blank" rel="noopener">Predictable long term caching with Webpack</a>。给出了一个解决方案。</p><p>这个方案的核心就是，Webpack 内部维护了一个自增的 id，每个 chunk 都有一个 id。所以当增加 entry 或者其他类型 chunk 的时候，id 就会变化，导致内容没有变化的 chunk 的 id 也发生了变化。</p><p>对此我们的应对方案是，使用 <code>webpack.NamedChunksPlugin</code> 把 chunk id 变为一个字符串标识符，这个字符包一般就是模块的相对路径。这样模块的 chunk id 就可以稳定下来。</p><p><img src="https://cdn.yuque.com/lark/0/2018/png/97613/1527958892306-f1799341-b89d-42fb-ab63-863d6365a874.png" alt="Screen Shot 2018-06-03 at 12.59.28 AM.png | left"></p><p><em>这里的 vendors1 就是 chunk id</em></p><blockquote><p><a href="https://webpack.js.org/plugins/hashed-module-ids-plugin/" target="_blank" rel="noopener">HashedModuleIdsPlugin</a> 的作用和 NamedChunksPlugin 是一样的，只不过 HashedModuleIdsPlugin 把根据模块相对路径生成的 hash 作为 chunk id，这样 chunk id 会更短。因此在生产中更推荐用 HashedModuleIdsPlugin。</p></blockquote><p>这篇文章说还讲到，<code>webpack.NamedChunksPlugin</code> 只能对普通的 Webpack 模块起作用，异步模块，external 模块是不会起作用的。</p><blockquote><p>异步模块可以在 import 的时候加上 chunkName 的注释，比如这样：import(/<em> webpackChunkName: “lodash” </em>/ ‘lodash’).then() 这样就有 Name 了</p></blockquote><p>所以我们需要再使用一个插件：<a href="https://github.com/timse/name-all-modules-plugin" target="_blank" rel="noopener">name-all-modules-plugin</a></p><blockquote><p>这个插件中用到一些老的 API，Webpack 4 会发出警告，这个 <a href="https://github.com/timse/name-all-modules-plugin/pull/2" target="_blank" rel="noopener">pr</a> 有新的版本，不过作者不一定会 merge。我们使用的时候可以直接 copy 这个插件的代码到我们的 Webpack 配置里面。</p></blockquote><p>做了这些工作之后，我们的 Vendor 的 ChunkId 就再也不会发生不该发生的变化了。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>Webpack 4 的改变主要是对社区中最佳实践的吸收。Webpack 4 通过新的 API 大大提升了 Code Splitting 的体验。但 Long-term caching 中 Vendor hash 的问题还是没有解决，需要手动配置。本文主要介绍的就是 Webpack 配置最佳实践在 Webpack 3.x 和 4.x 背景下的异同。希望对读者的 Webpack 4 项目的配置文件组织有所帮助。</p><p>另外，推荐 <a href="https://survivejs.com/webpack/" target="_blank" rel="noopener"><em>SURVIVEJS - WEBPACK</em></a> 这个在线教程。这个教程总结了 Webpack 在实际开发中的实践，并且把材料更新到了最新的 Webpack 4。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Webpack 4 发布已经有一段时间了。Webpack 的版本号已经来到了 4.12.x。但因为 Webpack 官方还没有完成迁移指南，在文档层面上还有所欠缺，大部分人对升级 Webpack 还是一头雾水。&lt;/p&gt;
&lt;p&gt;不过 Webpack 的开发团队已经写了一些零散的文章，官网上也有了新版配置的文档。社区中一些开发者也已经成功试水，升级到了 Webpack 4，并且总结成了博客。所以我也终于去了解了 Webpack 4 的具体情况。以下就是我对迁移到 Webpack 4 的一些经验。&lt;/p&gt;
&lt;p&gt;本文的重点在：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Webpack 4 在配置上带来了哪些便利？要迁移需要修改配置文件的哪些内容？&lt;/li&gt;
&lt;li&gt;之前的 Webpack 配置最佳实践在 Webpack 4 这个版本，还适用吗？&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Webpack" scheme="http://yoursite.com/categories/Webpack/"/>
    
    
      <category term="Webpack" scheme="http://yoursite.com/tags/Webpack/"/>
    
  </entry>
  
  <entry>
    <title>Iris + Casbin 权限控制实战</title>
    <link href="http://yoursite.com/2018/05/14/casbin-iris/"/>
    <id>http://yoursite.com/2018/05/14/casbin-iris/</id>
    <published>2018-05-14T13:17:27.000Z</published>
    <updated>2019-06-05T07:55:14.535Z</updated>
    
    <content type="html"><![CDATA[<p>在木犀的 PaaS 云平台的设计中，需要有一个细粒度比较小的权限控制系统。不同用户对不同的资源，拥有不同的权限。土办法已经不管用了，我们需要更系统，更规范的权限控制系统。本文讲的就是如何将权限控制库 <a href="https://github.com/casbin/casbin" target="_blank" rel="noopener">Casbin</a> 接入 Iris Web 框架。</p><a id="more"></a><h3 id="Iris-中间件机制简介"><a href="#Iris-中间件机制简介" class="headerlink" title="Iris 中间件机制简介"></a>Iris 中间件机制简介</h3><p>Iris 这个框架是基于中间件的，和 Nodejs 的 Koa 和 Express 很像。所谓中间件机制，就是一个请求到达之后，会生成一个上下文信息，里面包含了这个请求的一些信息。然后我们依次调用中间件函数，把上下文对象作为参数传入。需要注意是中间件函数的调用是嵌套的，在中间件函数中我们可以调用 <code>ctx.Next()</code> 方法，进入下一个中间件函数。当最后一个中间件函数返回时，之前调用过的中间件会依次返回。这个数据流被形象的称为“洋葱模型”。</p><p><img src="https://raw.githubusercontent.com/zxc0328/for-picgo/master/64c45edcgy1frlkp8fedzj20da0c3t91.jpg" alt=""></p><p>一个典型的中间件是这样的：</p><pre><code>func middleware(ctx iris.Context) {   // get info from context    requestPath := ctx.Path()   // set info with context    ctx.Values().Set(&quot;info&quot;, shareInformation)    // call next middleware    ctx.Next()}</code></pre><p>我们可以在中间件中读取 ctx 结构，根据上面附带的信息，我们可以做一些针对性的事情。</p><p>一个常用的中间件场景就是访问控制。我们可以根据 ctx 上带的用户信息，来查看用户的权限，如果用户没有要访问的资源的权限，我们就拒绝这次访问。比如这样：</p><pre><code>func auth(ctx iris.Context) {   // check if user has permission   if !c.Check(ctx.Request()) {        ctx.StatusCode(http.StatusForbidden) // Status Forbiden        ctx.StopExecution()         return    }    ctx.Next()}</code></pre><p>在本文的权限控制的场景下，中间件的作用就是在请求验证失败时，提前返回 403 状态码。</p><h3 id="Casbin-简介"><a href="#Casbin-简介" class="headerlink" title="Casbin 简介"></a>Casbin 简介</h3><p><a href="https://github.com/casbin/casbin" target="_blank" rel="noopener">Casbin</a> 是由北大的一位博士生主导开发的一个基于 Go 语言的权限控制库。支持 ACL，RBAC，ABAC 等常用的访问控制模型。</p><p>Casbin 的核心是<strong>一套基于 PERM metamodel (Policy, Effect, Request, Matchers) 的 DSL</strong>。Casbin 从用这种 DSL 定义的配置文件中读取访问控制模型，作为后续权限验证的基础。</p><p>一个典型的配置文件如下：</p><pre><code># Request definition[request_definition]r = sub, obj, act# Policy definition[policy_definition]p = sub, obj, act# Policy effect[policy_effect]e = some(where (p.eft == allow))# Matchers[matchers]m = r.sub == p.sub &amp;&amp; r.obj == p.obj &amp;&amp; r.act == p.act</code></pre><p>可以看到这个配置文件主要定义了 Request 和 Policy 的组成结构。Policy effect 和 Matchers 则灵活的多，可以包含一些自定义的表达式。</p><p>比如我们要加入一个名叫 root 的超级管理员，就可以这样写：</p><pre><code>[matchers]m = r.sub == p.sub &amp;&amp; r.obj == p.obj &amp;&amp; r.act == p.act || r.sub == &quot;root&quot;</code></pre><p>又比如我们可以用正则匹配来判断权限是否 match：</p><pre><code>[matchers]m = r.sub == p.sub &amp;&amp; keyMatch(r.obj, p.obj) &amp;&amp; regexMatch(r.act, p.act)</code></pre><p>Casbin 文档中有<a href="https://github.com/casbin/casbin#examples" target="_blank" rel="noopener">一节</a>展示了 Casbin 所支持的权限控制模型的示例配置。我们可以根据那些例子来打造我们自己的权限控制模型。</p><p>有了权限控制模型，我们还需要权限规则。权限规则是若干行数据的集合。每行数据就是一条规则。权限规则的具体格式因权限模型中的定义而异，最简单的 ACL 模型的规则是这样的：</p><pre><code>p, alice, data1, readp, bob, data2, write</code></pre><p>意思就是 alice 可以读 data1，bob 可以写 data2。</p><h3 id="为-Casbin-适配-Iris-中间件"><a href="#为-Casbin-适配-Iris-中间件" class="headerlink" title="为 Casbin 适配 Iris 中间件"></a>为 Casbin 适配 Iris 中间件</h3><p>既然实现权限控制的最佳位置是中间件，我们就需要为 Casbin 写一个 Iris 中间件。社区里的 <a href="https://github.com/iris-contrib/middleware/tree/master/casbin" target="_blank" rel="noopener">Casbin-iris 插件</a>就是一个不错的例子，我们可以以这个插件为基础进行开发。</p><p>首先我们来看看这个中间件是如何使用的，这个插件有 Warpper 和 Middleware 两种用法。差别在于 Warpper 方法会在所有路由被调用。而 Middleware 让我们可以控制哪些路由启用权限控制。我们选择 Middleware 方式做示例。打开 <a href="https://github.com/iris-contrib/middleware/blob/master/casbin/_examples/middleware/main.go" target="_blank" rel="noopener"><code>casbin/_examples/middleware/main.go</code></a>，其中核心的几行代码是这样的：</p><pre><code>var Enforcer = casbin.NewEnforcer(&quot;casbinmodel.conf&quot;, &quot;casbinpolicy.csv&quot;)func newApp() *iris.Application {    casbinMiddleware := casbinMiddleware.New(Enforcer)    app := iris.New()    app.Use(casbinMiddleware.ServeHTTP)    app.Get(&quot;/&quot;, hi)    app.Get(&quot;/dataset1/{p:path}&quot;, hi) // p, alice, /dataset1/*, GET    app.Post(&quot;/dataset1/resource1&quot;, hi)    app.Get(&quot;/dataset2/resource2&quot;, hi)    app.Post(&quot;/dataset2/folder1/{p:path}&quot;, hi)    app.Any(&quot;/dataset2/resource1&quot;, hi)    return app}</code></pre><p>首先通过 NewEnforcer 方法初始化一个 Casbin Enforcer。NewEnforcer 方法接收两个参数，一个是访问控制模型文件的路径，一个是权限规则文件的路径。</p><p>然后调用 casbinMiddleware.New 方法，传入 Casbin Enforcer，进行一些初始化工作。最后调用 <code>app.Use(casbinMiddleware.ServeHTTP)</code>，应用中间件。</p><p>看起来还挺简单的。但这里存在一个问题，这个中间件是如何拿到鉴权需要的用户信息的呢？这个过程对于开发者是不透明的。我们查看<a href="https://github.com/iris-contrib/middleware/blob/master/casbin/casbin.go" target="_blank" rel="noopener">源码</a>，发现里面有这样的代码：</p><pre><code>// Username gets the username from the basicauth.func Username(r *http.Request) string {    username, _, _ := r.BasicAuth()    return username}</code></pre><p>原来这个中间件假设请求通过 HTTP Basic Auth 方式进行认证。然后从请求的 headers 中获取认证信息。</p><p>但在实际生产中，我们认证用户身份的方式有很多种，最常见的就是通过 session 得知用户的身份，或者通过 token 这样的凭证来确定用户的身份。这个中间件如果要使用到生产中去，需要进行一些改动。</p><p>以下就是修改过的中间件，为了测试，我在 Username 函数中直接返回了用户名，后续使用时可以在这个函数里进行用户身份的获取。</p><pre><code>package casbinimport (    &quot;net/http&quot;    &quot;github.com/kataras/iris/context&quot;    &quot;github.com/casbin/casbin&quot;)func New(e *casbin.Enforcer) *Casbin {    return &amp;Casbin{enforcer: e}}func (c *Casbin) Wrapper() func(w http.ResponseWriter, r *http.Request, router http.HandlerFunc) {    return func(w http.ResponseWriter, r *http.Request, router http.HandlerFunc) {        if !c.Check(r) {            w.WriteHeader(http.StatusForbidden)            w.Write([]byte(&quot;403 Forbidden&quot;))            return        }        router(w, r)    }}func (c *Casbin) ServeHTTP(ctx context.Context) {    if !c.Check(ctx.Request()) {        ctx.StatusCode(http.StatusForbidden) // Status Forbiden        ctx.StopExecution()        return    }    ctx.Next()}type Casbin struct {    enforcer *casbin.Enforcer}// Check checks the username, request&#39;s method and path and// returns true if permission grandted otherwise false.func (c *Casbin) Check(r *http.Request) bool {    username := Username(r)    method := r.Method    path := r.URL.Path    return c.enforcer.Enforce(username, path, method)}// Username gets the username from the basicauth.func Username(r *http.Request) string {   // TODO: get username form db using userId in session or token    return &quot;alice&quot;}</code></pre><p>大家可以用<a href="https://github.com/iris-contrib/middleware/tree/master/casbin/_examples/middleware" target="_blank" rel="noopener">这里</a>的代码、<a href="https://github.com/iris-contrib/middleware/blob/master/casbin/_examples/middleware/casbinmodel.conf" target="_blank" rel="noopener">模型</a>和<a href="https://github.com/iris-contrib/middleware/blob/master/casbin/_examples/middleware/casbinpolicy.csv" target="_blank" rel="noopener">规则文件</a>进行测试（将中间件替换为上面的版本）。如果我们 GET /dataset2/resource2 这个路径，就会返回 403。这说明中间件正常工作了。因为 alice 是没有 /dataset2/resource2 这个资源的 GET 权限的。</p><h3 id="选择合适的访问控制模型"><a href="#选择合适的访问控制模型" class="headerlink" title="选择合适的访问控制模型"></a>选择合适的访问控制模型</h3><p>我选择了 RBAC with domains/tenants 这个模型作为木犀云平台的访问控制模型。PaaS 平台中有服务、应用等多种资源，所以需要按领域模型区分。用户中可以存在超级管理员等角色，所以需要角色。需要注意的是 Casbin 的 RBAC 中的角色其实是一种分组。比如我们可以定义一个叫 admin 的用户，这个用户对所有的资源都有权限规则存在，然后我们可以把其他用户和这个用户分为一组，那这些用户也都有了 admin 用户的权限。</p><pre><code>[request_definition]r = sub, dom, obj, act[policy_definition]p = sub, dom, obj, act[role_definition]g = _, _, _[policy_effect]e = some(where (p.eft == allow))[matchers]m = g(r.sub, p.sub, r.dom) &amp;&amp; r.dom == p.dom &amp;&amp; r.obj == p.obj &amp;&amp; r.act == p.act</code></pre><p>这个模型定义中的 g 就是指 group。</p><p>示例规则如下：</p><pre><code>p, admin, domain1, data1, readp, admin, domain1, data1, writep, admin, domain2, data2, readp, admin, domain2, data2, writeg, alice, admin, domain1g, bob, admin, domain2</code></pre><p>如上所示，alice 和 bob 分别是 domian1 和 domain2 的管理员。</p><h3 id="Casbin-Policy-持久化"><a href="#Casbin-Policy-持久化" class="headerlink" title="Casbin Policy 持久化"></a>Casbin Policy 持久化</h3><p>Casbin 的 policy 可以保存在一个 csv 文件中。也可以被持久化到数据库中。</p><p>所谓的“持久化到数据库中”的意思，就是在数据库中创建一个表，把行数据都存放到数据库中。</p><p>对于一个 Web 应用，我们想要的当然是后者。所以我们还需要将 Casbin 和数据库连接起来。</p><p>Casbin 支持 gorm 和 xorm 等等常见的 orm。我们以 gorm 为例，Canbin 的 gorm 适配库是 <a href="https://github.com/casbin/gorm-adapter" target="_blank" rel="noopener">gorm-adapter</a>。</p><p>接入 gorm 并不是很复杂的事情，其实就是把 NewEnforcer 中的第二个 policy file 参数换成 gorm-adapter 的一个实例就可以。</p><pre><code>// 自动创建一个数据库，叫 casbin// 如果需要制定数据库名，可以这样 a := gormadapter.NewAdapter(&quot;mysql&quot;, &quot;mysql_username:mysql_password@tcp(127.0.0.1:3306)/abc&quot;, true)var a = gormadapter.NewAdapter(&quot;mysql&quot;, &quot;root:muxi@tcp(127.0.0.1:3306)/&quot;) var Enforcer = casbin.NewEnforcer(&quot;casbinmodel.conf&quot;, a)</code></pre><p>然后我们可以调用 API 对规则进行添加和删除等等操作：</p><pre><code>Enforcer.LoadPolicy()Enforcer.AddPolicy(&quot;admin&quot;, &quot;app&quot;, &quot;/app/1&quot;, &quot;GET&quot;)Enforcer.AddGroupingPolicy(&quot;alice&quot;, &quot;admin&quot;, &quot;app&quot;)</code></pre><blockquote><p>这里踩了一个小坑，这个 gorm-adapter 的 <a href="https://github.com/casbin/gorm-adapter" target="_blank" rel="noopener">README</a> 里没有写 AddGroupingPolicy 这个 API。还是翻源码才看到的。</p></blockquote><p>规则文件中的 p 对应 AddPolicy API，g 对应 AddGroupingPolicy API。</p><h3 id="Warp-it-up"><a href="#Warp-it-up" class="headerlink" title="Warp it up"></a>Warp it up</h3><blockquote><p>TODO: 把文中的示例代码整理到一个仓库中</p></blockquote><p>一点感受：Casbin 的文档主要是 README 中的内容。Iris 的文档则主要要看 Example 这一节。有点 Example Driven Development 的感觉。虽然这么说，不过整体来说，这些资料还是可以覆盖到我们的使用场景的。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在木犀的 PaaS 云平台的设计中，需要有一个细粒度比较小的权限控制系统。不同用户对不同的资源，拥有不同的权限。土办法已经不管用了，我们需要更系统，更规范的权限控制系统。本文讲的就是如何将权限控制库 &lt;a href=&quot;https://github.com/casbin/casbin&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Casbin&lt;/a&gt; 接入 Iris Web 框架。&lt;/p&gt;
    
    </summary>
    
      <category term="Iris" scheme="http://yoursite.com/categories/Iris/"/>
    
      <category term="Go" scheme="http://yoursite.com/categories/Iris/Go/"/>
    
    
      <category term="Iris" scheme="http://yoursite.com/tags/Iris/"/>
    
      <category term="Go" scheme="http://yoursite.com/tags/Go/"/>
    
  </entry>
  
  <entry>
    <title>如何阅读大型前端开源项目的源码</title>
    <link href="http://yoursite.com/2018/05/01/react-source-reading-howto/"/>
    <id>http://yoursite.com/2018/05/01/react-source-reading-howto/</id>
    <published>2018-05-01T07:01:50.000Z</published>
    <updated>2019-06-05T07:46:13.083Z</updated>
    
    <content type="html"><![CDATA[<p>目前网上有很多「XX源码分析」这样的文章，不过这些文章分析源码的范围有限，有时候讲的内容不是读者最关心的。同时我也注意到，源码是在不断更新的，文章里写的源码往往已经过时了。因为这些问题，很多同学都喜欢自己看源码，自己动手，丰衣足食。</p><p>这篇文章主要讲的是阅读大型的前端开源项目比如 React、Vue、Webpack、Babel 的源码时的一些技巧。目的是让大家在遇到需要阅读源码才能解决的问题时，可以更快的定位到自己想看的代码。授人以鱼不如授人以渔，希望大家可以通过这篇博客，了解到阅读大型前端项目源码时的切入点。在之后遇到好奇的问题时，可以自己去探索。</p><a id="more"></a><h3 id="问题驱动——不要为了看源码而看源码"><a href="#问题驱动——不要为了看源码而看源码" class="headerlink" title="问题驱动——不要为了看源码而看源码"></a>问题驱动——不要为了看源码而看源码</h3><p>首先我们要明确一点，看源码的目的是什么？</p><p>我个人的意见是，看源码是为了解决问题。开源项目的源代码并没有什么非常特殊的地方，也都是普通的代码。这些代码的数量级一般都挺大，如果想是从源码中学到东西，直接浏览整个 Codebase 无疑是大海捞针。</p><p>但如果是带着问题去看源码，比如想了解一下 React 的合成事件系统的原理，想了解 React 的 setState 前后发生了什么，或者想了解 Webpack 插件系统的原理。也有可能是遇到了一个 bug，怀疑是框架/工具的问题。在这样的情况下，带着一个具体的目标去看源码，就会有的放矢。</p><h3 id="看最新版的源码"><a href="#看最新版的源码" class="headerlink" title="看最新版的源码"></a>看最新版的源码</h3><p>之前看到一种说法，看源码要从项目的第一个 commit 开始看。如果是为了解决前文中对框架/工具产生的困惑，那自然要看当前项目中用到的框架/工具的版本。</p><p>如果是为了学习源码，我也建议看最新的源码。因为一个项目是在不断迭代和重构的。不同版本之间可能是一次完全的重写。比如 Vue 2.x 和 React 16。重构导致了代码架构上的一些变化，Vue 2.x 引入了 Vritual DOM，Pull + Push 的数据变化检测方式让整个代码的结构变的更清晰了，所以 2.x 的代码其实比 1.x 的更容易阅读。React 16 重写了 Reconciler，引入了 fiber 这个概念，整个代码仓库结构也更清晰，所以更推荐阅读。</p><h3 id="前置条件"><a href="#前置条件" class="headerlink" title="前置条件"></a>前置条件</h3><p>看源码怎么看，当然不能一把梭了。</p><p><img src="https://raw.githubusercontent.com/zxc0328/for-picgo/master/64c45edcgy1fqw0buj08wj20go0aztie.jpg" alt=""></p><p>看源码之前需要对项目的<strong>原理</strong>有一个基本的了解。所谓原理就是，这个项目有哪些组成部分，为了达到最终的产出，要经过哪几步流程。这些流程里，业界主流的方案有哪几种。</p><p>比如前端 View 层框架，要渲染出 UI，组件要经过 mount、 render 等等步骤。数据驱动的前端框架，在 mounted 之后，就会进入一个循环，当用户交互触发组件数据变化时，会更新 UI。其中数据的检测方式又有分 Push 和 Pull 两种方案。渲染 UI 可以是全量的字符串模板替换，也可以是基于 Virtual DOM 的差量 DOM 更新。</p><p>又比如前端的一些工具，Webpack 和 Babel 这些工具都是基于插件的。基本的工作流程就是读取文件，解析代码成 AST，调用插件去转换 AST，最后生成代码。要了解 Webpack 的原理，就要知道 Webpack 基于一个叫 <a href="https://github.com/webpack/tapable" target="_blank" rel="noopener">tapable</a> 的模块系统。</p><p>那我们要如何了解这些呢？要了解这些，可以去各大网站和博客上的《XXX源码解析》系列。通过这些文章，我们可以对我们要看的框架/工具的原理有一个大致的了解。</p><h3 id="本地build"><a href="#本地build" class="headerlink" title="本地build"></a>本地build</h3><p>不过最终我们还是要直接看源码。笔者真正看源码的第一步就是把项目的代码仓库 clone 到本地。然后按项目 README 上的构建指南，在本地 build 一下。</p><p>如果是前端框架，我们可以在 HTML 中里直接引入本地 build 出的 umd bundle（记得用 development build，不然会把代码压缩，可读性差），然后写一个简单的 demo，demo 里引入本地的 build。如果是基于 Nodejs 的工具，我们可以用 npm link 把这个工具的命令 link 到本地。也可以直接看项目的 package.json 的入口文件，直接用 node 运行那个文件。 </p><p>这里要强调一下，大型的开源项目一般都会有一个 <strong>Contribution Guide</strong>，目的是让想贡献代码的开发者更快上手。里面就有讲怎么在本地构建代码。</p><p>以 React 为例，React 的 <a href="https://reactjs.org/docs/how-to-contribute.html" target="_blank" rel="noopener">Contributing Guide</a> 里就 Development Workflow 这一节。里面有这么一段话：</p><blockquote><p>The easiest way to try your changes is to run yarn build core,dom –type=UMD and then open fixtures/packaging/babel-standalone/dev.html. This file already uses react.development.js from the build folder so it will pick up your changes.</p></blockquote><p>所以 React 仓库中的 fixtures/packaging/babel-standalone/dev.html 就是一个方便的 demo 页。我们可以在这个页面快速查看我们在本地对代码的改动。</p><p>你可以尝试着在项目的入口文件中加入一句 log，看看是不是可以在控制台/终端看到这句 log。如果可以的话，恭喜你，你现在可以随便把玩这个项目了！</p><h3 id="理清目录结构"><a href="#理清目录结构" class="headerlink" title="理清目录结构"></a>理清目录结构</h3><p>在看具体的代码之前，我们需要理清项目的目录结构，这样我们才能更快的知道在哪里地方找相关功能的代码。</p><p>我们看看 React 的目录结构。React 是一个 monorepo。也就是一个仓库里包含了多个子仓库。我们在 packages 目录下可以看到很多单独的 package：</p><p><img src="https://raw.githubusercontent.com/zxc0328/for-picgo/master/64c45edcgy1fqsdkkvpjtj21k00wmaka.jpg" alt=""></p><p>在 React 16 之后，React 的代码分为 React Core，Renderer 和 Reconciler 三部分。这是因为 React 的设计让我们可以把负责映射数据到 UI 的 Reconciler 以及负责渲染 Vritual DOM 到各个终端的 Renderer 和 React Core 分开。React Core 包含了 React 的类定义和一些顶级 API。大部分的渲染和 View 层 diff 的逻辑都在 Reconciler 和 Renderer 中。</p><p>Babel 也是一个 monorepo。Babel 的核心代码是 babel-core 这个 package，Babel 开放了接口，让我们可以自定义 Visitor，在AST转换时被调用。所以 Babel 的仓库中还包括了很多插件，真正实现语法转换的其实是这些插件，而不是 babel-core 本身。</p><p><img src="https://raw.githubusercontent.com/zxc0328/for-picgo/master/64c45edcgy1fqv1g5muerj21jm14kqe3.jpg" alt=""></p><p>Vuejs 的代码比较典型，核心代码在 src 目录下，按功能模块划分。因为 Vue 也支持多平台渲染，所以把平台相关的代码都放到了 platform 文件夹下，core 文件夹中是 Vue 的核心代码，compiler 是 Vue 的模板编译器，把 HTML 风格的模板编译为 render function。</p><p><img src="https://raw.githubusercontent.com/zxc0328/for-picgo/master/64c45edcgy1fqshma4atzj21js0ietdc.jpg" alt=""></p><p>Webpack 和 Babel 一样，可以说都是基于插件的系统。Webpack 的主要源码在 lib 目录下，里面的 webpack.js 就是入口文件。</p><p>上面说了四个项目的目录结构，那我们遇到一个新的开源项目，应该怎么了解它的目录结构呢？</p><p>如果这个项目是一个 monorepo，首先我们要找到核心的那个 package，然后看里面的代码。</p><p>不是 monorepo 的话，一般来说，如果这个项目是一个 CLI 的工具，那 bin 目录下放的就是命令行界面相关的入口文件，lib 或者 src 下面就是工具的核心代码。如果这个项目是一个前端 View 层框架，那目录结构就和 Vue 类似。</p><p>作为验证，大家可以看一下打包工具 <a href="https://github.com/parcel-bundler/parcel" target="_blank" rel="noopener">parcel</a> 和前端 View 层库 <a href="https://github.com/kbrsh/moon" target="_blank" rel="noopener">moon</a> 的目录结构。目录结构这个东西往往是大同小异，多看几个项目就熟悉了。</p><h3 id="debugger-amp-amp-全局搜索大法"><a href="#debugger-amp-amp-全局搜索大法" class="headerlink" title="debugger &amp;&amp; 全局搜索大法"></a>debugger &amp;&amp; 全局搜索大法</h3><p>运行了本地的 build，了解了目录结构，接下来我们就可以开始看源码了！之前说了，我们要以问题驱动，下面我就以 React 调用 setState 前后发生了什么这个问题作为例子。</p><p>我们可以在 setState 的地方打一个断点。首先我们要找到 setState 在什么地方。这个时候之前的准备工作就派上用处了。我们知道 React 的共有 API 在 react 这个 package 下面。我们就在那个 package 里面全局搜索。我们发现这个 API 定义在 src/ReactBaseClasses.js 这个文件里。</p><p>于是我们就在这里打一个断点：</p><pre><code>Component.prototype.setState = function(partialState, callback) {  invariant(    typeof partialState === &#39;object&#39; ||      typeof partialState === &#39;function&#39; ||      partialState == null,    &#39;setState(...): takes an object of state variables to update or a &#39; +      &#39;function which returns an object of state variables.&#39;,  );  debugger;  this.updater.enqueueSetState(this, partialState, callback, &#39;setState&#39;);};</code></pre><p>然后运行本地 React build 的 demo 页面，让组件触发 setState，我们就可以在 Devtool 里看到断点了。</p><p>我们走进 this.updater.enqueueSetState 这个调用，就来到了 ReactFiberClassComponent 这个函数中的 enqueueSetState，这里调用了 enqueueUpdate 和 scheduleWork 两个函数，如果要深入 setState 之后的流程，我们只需要再点击 <img src="https://raw.githubusercontent.com/zxc0328/for-picgo/master/64c45edcgy1fqx8qcid0aj200i00q053.jpg" alt=""> 走进这两个函数里看具体的代码就可以了。</p><p>如果想看 setState 之前发生了什么，我们只需要看 Devtool 右边的调用栈：</p><p><img src="https://raw.githubusercontent.com/zxc0328/for-picgo/master/64c45edcgy1fqvtw441ytj20c60ukae9.jpg" alt=""></p><p>点击每一个 frame 就可以跳到对应的函数中，并且恢复当时的上下文。</p><p>结合一步一步的代码调试，我们可以看到框架的函数调用栈。对于每个重要的函数，我们可以在仓库里搜索到源码，进一步研究。</p><p>Node 工具的调试方法也是相似的，我们可以在运行 node 命令时加上 –inspect 参数。具体可以看<a href="https://medium.com/@paul_irish/debugging-node-js-nightlies-with-chrome-devtools-7c4a1b95ae27" target="_blank" rel="noopener"> <em>Debugging Node.js with Chrome DevTools</em></a> 这篇博客。</p><p>其实大家都知道单步调试这种办法，但在哪里打断点才是最关键的。我们在熟悉框架的原理之后，就可以在框架的<strong>关键链路</strong>上打断点，比如前端 View 层框架的声明周期钩子和 render 方法，Node 工具的插件函数，这些代码都是框架运行的必经之地，是不错的切入点。</p><p>如果是为了了解一个特定的问题，大家可以直接在自己觉得有问题的地方打断点。然后把源码运行起来，想办法让代码运行到那个地方。我们在断点可以看到局部变量等等信息，有助于定位问题。</p><h3 id="来自开发团队的资源"><a href="#来自开发团队的资源" class="headerlink" title="来自开发团队的资源"></a>来自开发团队的资源</h3><p>其实开源项目的开发团队也都致力于让更多的人参与到项目中来，降低项目的门槛。所以我们在线上其实可以找到很多来自开发团队的资源。这些资源可以帮助我们去理解项目的原理。</p><h4 id="关注核心开发者"><a href="#关注核心开发者" class="headerlink" title="关注核心开发者"></a>关注核心开发者</h4><p>每个项目都有一些核心开发者，比如 React 的 <a href="https://twitter.com/dan_abramov" target="_blank" rel="noopener">Dan Abramov</a>, <a href="https://twitter.com/acdlite" target="_blank" rel="noopener">Andrew Clark</a> 和 <a href="https://twitter.com/sebmarkbage" target="_blank" rel="noopener">Sebastian Markbåge</a>。Webpack 的 <a href="https://twitter.com/wSokra" target="_blank" rel="noopener">Tobias Koppers</a> 和 <a href="https://twitter.com/TheLarkInn" target="_blank" rel="noopener">Sean Larkin</a>。Vue 的 <a href="">Evan You</a>。我们可以在 Twitter 上关注他们，了解项目的动态。</p><h4 id="关注官方博客和演讲视频"><a href="#关注官方博客和演讲视频" class="headerlink" title="关注官方博客和演讲视频"></a>关注官方博客和演讲视频</h4><p>如果我们关注了上面的核心开发者，就会发现他们时常会发布一些和源码/项目原理有关的博客或者视频。</p><p>React 的官方博客最近就有很多和项目开发有关的博客。</p><ul><li><a href="https://reactjs.org/blog/2017/12/15/improving-the-repository-infrastructure.html" target="_blank" rel="noopener">Behind the Scenes: Improving the Repository Infrastructure</a> 这篇介绍的是 React 项目仓库的基础设施。</li><li><a href="https://reactjs.org/blog/2018/03/01/sneak-peek-beyond-react-16.html" target="_blank" rel="noopener">Sneak Peek: Beyond React 16</a></li></ul><p>Andrew Clark 一开始就写了一篇介绍 fiber 架构的<a href="https://github.com/acdlite/react-fiber-architecture" target="_blank" rel="noopener">文档</a>。<br>Dan Abramov 最近在 JSConf 上对 React 未来的一些新特性的介绍 - <a href="https://www.youtube.com/watch?v=v6iR3Zk4oDY" target="_blank" rel="noopener">Beyond React 16</a>。React 博客中的 <em>Sneak Peek: Beyond React 16</em> 也是对这次 Talk 的介绍。</p><p>Evan You <a href="https://www.youtube.com/watch?v=r4pNEdIt_l4" target="_blank" rel="noopener">介绍前端框架数据变化侦测原理的 Talk</a>。Vue 文档中也有 <a href="https://vuejs.org/v2/guide/reactivity.html" target="_blank" rel="noopener">Reactivity in Depth</a> 这样的介绍原理的章节。</p><p>Sean Larkin 的 <a href="https://www.youtube.com/watch?v=4tQiJaFzuJ8" target="_blank" rel="noopener">Everything is a plugin! Mastering webpack from the inside out</a> 介绍了 Webpack 的核心组件 Tapable。</p><p>James Kyle 的 <a href="https://www.youtube.com/watch?v=Tar4WgAfMr4" target="_blank" rel="noopener">How to Build a Compiler</a> 可以让我们了解 Babel 转译代码的基本流程。</p><h3 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h3><p>本文最核心的观点就是，看源码的目的是为了解决问题。我们鼓励大家在本地把大型项目的源码跑起来，自己随意把玩，研究。因为源码也是普通的代码，并没有太多门槛。唯一的门槛可能就来源于开源项目作者和普通开发者之间的信息不对称，普通开发者对项目的原理和目录结构不够了解。</p><p>我们可以从开发者那里获取资源，同时也可以多阅读社区里的源码分析文章，这些都有助于我们理解项目的原理，为后续的源码分析打下基础。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;目前网上有很多「XX源码分析」这样的文章，不过这些文章分析源码的范围有限，有时候讲的内容不是读者最关心的。同时我也注意到，源码是在不断更新的，文章里写的源码往往已经过时了。因为这些问题，很多同学都喜欢自己看源码，自己动手，丰衣足食。&lt;/p&gt;
&lt;p&gt;这篇文章主要讲的是阅读大型的前端开源项目比如 React、Vue、Webpack、Babel 的源码时的一些技巧。目的是让大家在遇到需要阅读源码才能解决的问题时，可以更快的定位到自己想看的代码。授人以鱼不如授人以渔，希望大家可以通过这篇博客，了解到阅读大型前端项目源码时的切入点。在之后遇到好奇的问题时，可以自己去探索。&lt;/p&gt;
    
    </summary>
    
      <category term="Source code reading" scheme="http://yoursite.com/categories/Source-code-reading/"/>
    
    
      <category term="Source code reading" scheme="http://yoursite.com/tags/Source-code-reading/"/>
    
  </entry>
  
  <entry>
    <title>Headless Chrome long image capture issue</title>
    <link href="http://yoursite.com/2018/02/12/hdchrome-long-capture/"/>
    <id>http://yoursite.com/2018/02/12/hdchrome-long-capture/</id>
    <published>2018-02-12T03:16:21.000Z</published>
    <updated>2019-06-05T07:55:54.444Z</updated>
    
    <content type="html"><![CDATA[<h3 id="The-problem"><a href="#The-problem" class="headerlink" title="The problem"></a>The problem</h3><p>Recently I had received complaint about my capture service not export complete image. It seems that this problem only occurs when the page’s is extremely long.</p><a id="more"></a><p>The broken image is like this:</p><p><img src="https://raw.githubusercontent.com/zxc0328/for-picgo/master/64c45edcgy1fmlvek2n5xj208y07aaah.jpg" alt="broken"></p><h3 id="Chromium’s-limit"><a href="#Chromium’s-limit" class="headerlink" title="Chromium’s limit"></a>Chromium’s limit</h3><p>So I Googled for the problem and I found <a href="https://github.com/GoogleChrome/puppeteer/issues/477" target="_blank" rel="noopener">a lot issues</a> on Github that target the same problem. When reading throught <a href="https://github.com/GoogleChrome/puppeteer/pull/937" target="_blank" rel="noopener">this issue</a>, I got the fact that this problem is caused by Chromium’s limit.</p><p>Since normal server don’t have a GPU inside, Headless Chrome had to use software renderer, that is, using CPU to calculate the pixels. </p><p>Chromium’s compositor has a maximum texture size when using software GL backend, this limit is 16384px. So large image will not be renderer completely.</p><h3 id="How-to-solve-it"><a href="#How-to-solve-it" class="headerlink" title="How to solve it"></a>How to solve it</h3><p>The solve for this problem is simple. Cut the page into pieces, capture these fragments in order, and composite those pieces into a whole image.</p><p>The code below use Puppeteer’s API, it’s fine to replace it with other library like CDP.</p><pre><code>await page.setViewport({ width: 1440, height: 1024});const {contentSize} = await page._client.send(&#39;Page.getLayoutMetrics&#39;);// MAGIC NUMBER, DO NOT MODIFIY THIS OR YOU WILL BE FIREDconst maxScreenshotHeight = 7000;          if (contentSize.height &gt;= maxScreenshotHeight) {            let image;            let lastBuffer;            for (let ypos = 0; ypos &lt; contentSize.height; ypos += maxScreenshotHeight) {              const height = Math.min(contentSize.height - ypos, maxScreenshotHeight);              let buffer = await page.screenshot({                clip: {                  x: 0,                  y: ypos,                  width: contentSize.width,                  height                }              });              if (ypos === 0) {                image = sharp(buffer);                lastBuffer = await image.toBuffer();              }else {                image = sharp(lastBuffer);                image = image.extend({top: 0, bottom: height, left: 0, right: 0})                image = image.overlayWith(buffer, {top: ypos, left:0})                lastBuffer = await image.toBuffer();              }            }            fileData = lastBuffer;</code></pre><p>I use <a href="https://github.com/lovell/sharp" target="_blank" rel="noopener">sharp</a> for image processing, bacause it’s recommended on Github issue.</p><blockquote><p>The magic number here should be around 16000 by default, but I had noticed that the height for each piece should be reduced when the width increased. The width I use for my capture is 1440px, you should start from 16000 and reduce this number if the image exported is still incomplete.</p></blockquote><h3 id="Future"><a href="#Future" class="headerlink" title="Future"></a>Future</h3><p>The approach may not be necessary accroding to this <a href="https://bugs.chromium.org/p/chromium/issues/detail?id=770769" target="_blank" rel="noopener">Chromium issue</a>.</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;The-problem&quot;&gt;&lt;a href=&quot;#The-problem&quot; class=&quot;headerlink&quot; title=&quot;The problem&quot;&gt;&lt;/a&gt;The problem&lt;/h3&gt;&lt;p&gt;Recently I had received complaint about my capture service not export complete image. It seems that this problem only occurs when the page’s is extremely long.&lt;/p&gt;
    
    </summary>
    
      <category term="Headless Chrome" scheme="http://yoursite.com/categories/Headless-Chrome/"/>
    
    
      <category term="Headless Chrome" scheme="http://yoursite.com/tags/Headless-Chrome/"/>
    
  </entry>
  
  <entry>
    <title>Liso源码阅读笔记</title>
    <link href="http://yoursite.com/2018/02/12/liso/"/>
    <id>http://yoursite.com/2018/02/12/liso/</id>
    <published>2018-02-12T03:16:05.000Z</published>
    <updated>2019-06-05T07:54:26.778Z</updated>
    
    <content type="html"><![CDATA[<p>本文介绍了CMU 15-441的课程项目<a href="https://www.cs.cmu.edu/~prs/15-441-F16/project1/project1.pdf" target="_blank" rel="noopener">Liso</a>的一个实现。主要介绍了请求流程、Client状态机模型、Dynamic Buffer数据结构等等。SSL相关的部分没有涉及。</p><a id="more"></a><h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><p>我看的是这位同学的<a href="https://github.com/zhuansunxt/Liso" target="_blank" rel="noopener">实现</a>。</p><h3 id="一个HTTP静态文件请求的流程"><a href="#一个HTTP静态文件请求的流程" class="headerlink" title="一个HTTP静态文件请求的流程"></a>一个HTTP静态文件请求的流程</h3><p>首先我们拿一个简单的静态文件请求来梳理一下Liso的流程。在<code>liso.c</code>的<code>main</code>函数中，有这个服务器的主循环，这个循环每次做的事情就是用<code>select</code>来查询I/O事件。<code>select</code>之后，我们首先做的是查看<code>listenfd</code>是不是有read事件，如果有就<code>accept</code>这个连接。然后把连接加入到连接池里。一个连接的初始状态是<code>READY_FOR_READ</code>。</p><p><code>listenfd</code>是连接池中的第一个item，也是一直存在于连接池中的一个描述符。</p><p>在循环的最后，我们会调用<code>handle_clients</code>对连接池中可读和可写的描述符依次进行处理。具体的方式就是遍历连接池，如果一个连接是可读的，并且状态是<code>READY_FOR_READ</code>，我们就读取这个连接中的数据。</p><p>每次从每个连接读取的数据是固定的长度，比如4KB。这是为了控制I/O的粒度，不让server在一个连接上花太多时间，使得新的连接请求被阻塞。</p><p>我们把收到的数据，放到<code>client_buffer</code>里面。然后看一下HTTP的Header是不是已经读取完全了（调用<code>handle_recv_header</code>判断读取到的数据中是否有<code>&quot;\r\n\r\n&quot;</code>）。如果HTTP的header还没有读取完全，那就继续留在<code>READY_FOR_READ</code>状态，等待下一次循环。</p><p>如果Header已经读取完全了，我们就调用<code>handle_http_request</code>处理请求，在处理请求时，我们把要返回的数据写在<code>client_buffer</code>里，然后调整这个client的state到<code>READY_FOR_WRITE</code>。</p><p>在<code>handle_clients</code>的循环中，如果我们发现一个client的状态是<code>READY_FOR_WRITE</code>，我们就会把<code>client_buffer</code>里相关的数据写到客户端，然后根据这个连接的<code>Connection</code>Header选择关闭或者保留这个连接。</p><p>这就是一个简单的HTTP静态文件请求的流程。</p><h3 id="连接池的数据结构"><a href="#连接池的数据结构" class="headerlink" title="连接池的数据结构"></a>连接池的数据结构</h3><pre><code>typedef struct {  /* Client pool global data */  fd_set master;              /* all descritors */  fd_set read_fds;            /* all ready-to-read descriptors */  fd_set write_fds;           /* all ready-to_write descriptors */  int maxfd;                  /* maximum value of all descriptors */  int nready;                 /* number of ready descriptors */  int maxi;                   /* maximum index of available slot */  /* Client specific data */  int client_fd[FD_SETSIZE];  /* client slots */  dynamic_buffer * client_buffer[FD_SETSIZE];   /* client&#39;s dynamic-size buffer */  dynamic_buffer * back_up_buffer[FD_SETSIZE];  /* store historical pending request */  size_t received_header[FD_SETSIZE];           /* store header ending&#39;s offset */  char should_be_close[FD_SETSIZE];             /* whether client should be closed when checked */  client_state state[FD_SETSIZE];               /* client&#39;s state */  char *remote_addr[FD_SETSIZE];                /* client&#39;s remote address */  /* SSL related */  client_type type[FD_SETSIZE];                 /* client&#39;s type: HTTP or HTTPS */  SSL * context[FD_SETSIZE];                    /* set if client&#39;s type is HTTPS */  /* CGI related */  int cgi_client[FD_SETSIZE];} client_pool;</code></pre><p>每个socket连接在建立之后都会被放到<code>client_pool</code>里面。连接池在概念上就是一个大的数组。实现的时候，我们把每个连接相关的属性各自设置为一个相同大小的数组。每个item的属性就是属性数组中相应index的值。比如我们可以通过<code>client_pool-&gt;client_fd[i]</code>拿到第i个请求的fd。其他属性也是类似的。</p><p><code>FD_SETSIZE</code>这个常量是这个server并发连接的最大值，这个常量的大小一般是1024。这个值是操作系统设置的。这个连接池里除了各个连接相关信息的数组之外，还有几个<code>fd_set</code>类型的全局属性，其中<code>master</code>里保存了整个连接池的描述符。这也就是我们<code>select</code>循环时监听的目标。需要注意的是CGI连接在建立之后，也会被放到这个连接池中（<code>client_fd</code>指向发起CGI请求的客户端，<code>cgi_client</code>指向CGI连接，这个连接在CGI处理完后会被我们从连接池中清除）。这样我们就可以利用<code>select</code>对CGI请求进行事件驱动的异步处理。</p><h3 id="client的状态机模型"><a href="#client的状态机模型" class="headerlink" title="client的状态机模型"></a>client的状态机模型</h3><p>我们知道有限状态机由一组状态和一组转移组成，在Liso里，一个client的状态有以下几种：</p><pre><code>typedef enum client_state {    INVALID,    READY_FOR_READ,    READY_FOR_WRITE,    WAITING_FOR_CGI,    CGI_FOR_READ,    CGI_FOR_WRITE} client_state;</code></pre><p>在讲具体的状态模型之前，我们要把连接池中的连接分为两种，一种是客户端的连接描述符，一种是CGI连接。这两种连接的在连接池中的数据结构是相同的。</p><p>在处理HTTP请求时，如果一个请求是CGI请求，我们就会fork一个进程，获取CGI的文件描述符，然后调用<code>add_cgi_fd_to_pool</code>函数将这个描述符加入到连接池中。加入操作的核心代码是：</p><pre><code>/* Update client data */p-&gt;client_fd[i] = cgi_fd;p-&gt;state[i] = state;/* CGI */p-&gt;cgi_client[i] = clientfd;</code></pre><p>所以这个连接和普通的连接一样，都有<code>client_fd</code>属性，不同的是这个连接有一个<code>cgi_client</code>属性，指向CGI连接的文件描述符。所以在处理CGI请求的时候，一个连接在连接池中有客户端连接和CGI连接两个item。CGI连接的状态仅仅只是在<code>CGI_FOR_READ</code>和<code>CGI_FOR_WRITE</code>之间转换。在CGI连接可读，并被写到客户端之后，我们会从连接池中清除这个CGI的item。所以客户端连接和CGI连接的状态机模型应该分开讲述。</p><h4 id="客户端连接状态机模型"><a href="#客户端连接状态机模型" class="headerlink" title="客户端连接状态机模型"></a>客户端连接状态机模型</h4><p><img src="https://raw.githubusercontent.com/zxc0328/for-picgo/master/64c45edcgy1flgj3kbvxjj20vu0g0adc.jpg" alt="static fsm"></p><p>客户端连接，如果是CGI连接，会比静态资源请求多一个<code>WAITING_FOR_CGI</code>的状态。这里我们需要注意的是，我们在读取一个连接的数据时，如果是一个POST请求，一般会分很多次。如果读取之后数据不够，这个连接的状态就会停留在<code>READY_FOR_READ</code>，等待下一次<code>select</code>循环。直到读取到足够的数据（根据Header里的<code>content-length</code>）之后，才把这个请求的状态转移到<code>READY_FOR_WRITE</code>。</p><h4 id="CGI连接状态机模型"><a href="#CGI连接状态机模型" class="headerlink" title="CGI连接状态机模型"></a>CGI连接状态机模型</h4><p><img src="https://raw.githubusercontent.com/zxc0328/for-picgo/master/64c45edcgy1flgj3k7yopj20uc0dwacw.jpg" alt="cgi fsm"></p><h3 id="长连接"><a href="#长连接" class="headerlink" title="长连接"></a>长连接</h3><p>我们知道HTTP的Header中有一个Connection字段。如果这个字段的值为close，在<code>READY_FOR_WRITE</code>状态时，如果我们写完了当前请求所请求的数据，我们就关闭连接。如果这个字段的值为<code>keep-alive</code>，连接会经过<code>Persist</code>转移，重新进入<code>READY_FOR_WRITE</code>状态。</p><p>在写完数据之后依然保持<code>READY_FOR_WRITE</code>状态，不关闭连接，这就是我们常说的HTTP<code>keep-alive</code>长连接。在这种情况下，server假定的是在未来的某个时间节点，这个客户端上还有数据可以读取，所以我们暂时不关闭连接。</p><p>举例说一下HTTP长连接的用途：如果一个域下有多个资源需要请求，浏览器会复用同一个TCP连接。也就是说服务器可以在一个TCP连接上进行多次HTTP请求。这样有利于减少网络延迟和服务器的并发压力。</p><h3 id="Dynamic-Buffer数据结构"><a href="#Dynamic-Buffer数据结构" class="headerlink" title="Dynamic Buffer数据结构"></a>Dynamic Buffer数据结构</h3><p>Dynamic Buffer数据结构是对普通buffer的封装。为了应对HTTP请求中长度不一定的二进制数据，我们需要一个封装良好的buffer结构，让我们方向的使用，不用担心内存管理问题。</p><pre><code>typedef struct dynamic_buffer{  char *buffer;  size_t offset;  size_t capacity;  size_t send_offset;} dynamic_buffer;</code></pre><p><code>dynamic_buffer</code>类型用struct封装了一个新的类型，<code>buffer</code>指向实际存储数据的buffer。另外还有<code>offset</code>、<code>capacity</code>和<code>send_offset</code>这些属性来辅助内存管理。<code>offset</code>让我们掌握目前buffer的实际容量是多少。<code>capacity</code>让我们可以掌握buffer占用的内存大小，方便动态的扩容。作者说这个设计是模仿的C++ vector STL。</p><h3 id="CGI相关"><a href="#CGI相关" class="headerlink" title="CGI相关"></a>CGI相关</h3><p>Liso中用来描述一个CGI执行进程的数据结构是这样的：</p><pre><code>typedef struct CGI_executor {  int clientfd;  int stdin_pipe[2];    /* { write data --&gt; stdin_pipe[1] } -&gt; { stdin_pipe[0] --&gt; stdin } */  int stdout_pipe[2];   /* { read data &lt;--  stdout_pipe[0] } &lt;-- {stdout_pipe[1] &lt;-- stdout } */  dynamic_buffer* cgi_buffer;  CGI_param* cgi_parameter;} CGI_executor;</code></pre><p>里面主要的成员除了<code>cgi_buffer</code>和用于传入环境变量的<code>cgi_parameter</code>之外，<code>stdin_pipe</code>和<code>stdout_pipe</code>两个成员值得注意。</p><h4 id="pipe"><a href="#pipe" class="headerlink" title="pipe"></a>pipe</h4><p><code>stdin_pipe</code>和<code>stdout_pipe</code>两个成员代表了两个pipe。pipe是Linux中的一种通信机制。一个pipe有两个fd组成，第一个代表read端，第二个代表write端。写入的数据会缓存在内核中，在读取时被取出。我们可以用pipe函数来创建一个pipe，pipe中的两个文件描述符在fork时也会被复制。所以pipe可以被作为一种进程间通信的手段。</p><blockquote><p>关于pipe的更多信息请参考<em>The Linux Programming Interface</em> Chapter 44</p></blockquote><p>我们的server主进程要和CGI进程通信，就是通过pipe做到的。</p><p>在CGI进程中的代码：</p><pre><code>close(cgi_pool-&gt;executors[slot]-&gt;stdout_pipe[0]);close(cgi_pool-&gt;executors[slot]-&gt;stdin_pipe[1]);dup2(cgi_pool-&gt;executors[slot]-&gt;stdout_pipe[1], fileno(stdout));dup2(cgi_pool-&gt;executors[slot]-&gt;stdin_pipe[0], fileno(stdin));</code></pre><p>我们关闭了CGI进程中<code>stdout_pipe</code>的read端，把write端重定向到CGI进程的stdout，因为主进程要从这个pipe读取CGI进程的输出。</p><p>我们关闭了CGI进程中<code>stdin_pipe</code>的write端，把read端重定向到CGI进程的stdin，因为主进程要向CGI进程中写入一些信息，比如POST请求的body。</p><p>在主进程中，我们关闭用不到的两个fd，分别是<code>stdout_pipe</code>的write端和<code>stdin_pipe</code>的read端。</p><pre><code>close(cgi_pool-&gt;executors[slot]-&gt;stdout_pipe[1]);close(cgi_pool-&gt;executors[slot]-&gt;stdin_pipe[0]);</code></pre><h4 id="select-CGI"><a href="#select-CGI" class="headerlink" title="select + CGI"></a>select + CGI</h4><p>在CGI进程被创建之后，我们调用<code>add_cgi_fd_to_pool</code>函数把这个clientfd和CGI的fd一起加入到监听的连接池里面。CGI的fd是什么呢？没错，就是<code>stdin_pipe[1]</code>和<code>stdout_pipe[0]</code>，<code>stdin_pipe[1]</code>代表着CGI进程的输入，当这个fd为可写时，我们可以向CGI进程写入数据。<code>stdout_pipe[0]</code>代表着CGI进程的输出，当这个fd为可读时，我们可以从CGI进程中读取数据。加入到连接池中意味着我们可以通过select来监听CGI进程的I/O事件。所以在select的<code>handle_clients</code>里，除了客户端可写和客户端可读之外，我们需要处理CGI可写和CGI可读两种情况。</p><p>由此我们便可以在select中根据事件，来对客户端进行处理，转移它们的状态。一个非阻塞的事件驱动Server设计已经有了雏形。</p><h3 id="如何加强Liso"><a href="#如何加强Liso" class="headerlink" title="如何加强Liso"></a>如何加强Liso</h3><p>Liso距离一个工业级的Server还有很大的差距，我们可以做的改进有：</p><ul><li>将静态文件I/O改造为基于线程池的设计。</li><li>使用<code>epoll</code>代替<code>select</code>。</li><li>支持使用DSL对服务器进行配置。</li><li>支持Gzip。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文介绍了CMU 15-441的课程项目&lt;a href=&quot;https://www.cs.cmu.edu/~prs/15-441-F16/project1/project1.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Liso&lt;/a&gt;的一个实现。主要介绍了请求流程、Client状态机模型、Dynamic Buffer数据结构等等。SSL相关的部分没有涉及。&lt;/p&gt;
    
    </summary>
    
      <category term="server" scheme="http://yoursite.com/categories/server/"/>
    
      <category term="c" scheme="http://yoursite.com/categories/server/c/"/>
    
    
      <category term="server" scheme="http://yoursite.com/tags/server/"/>
    
      <category term="c" scheme="http://yoursite.com/tags/c/"/>
    
  </entry>
  
  <entry>
    <title>Kubernete安装不完全指南</title>
    <link href="http://yoursite.com/2017/10/26/k8s-setup-1-7/"/>
    <id>http://yoursite.com/2017/10/26/k8s-setup-1-7/</id>
    <published>2017-10-26T11:21:26.000Z</published>
    <updated>2018-06-19T11:35:32.372Z</updated>
    
    <content type="html"><![CDATA[<p>安装Kubernetes向来不是一件容易的事情。之前在集群上部署好的K8s突然出了问题，于是需要重新安装。这次没有之前那次那么顺利了，出现了很多奇怪的问题。但经过一番实践和总结，总算是得出了Kubernetes国内安装的一个不完全指南。这个指南理论上适用于任意版本的K8s。</p><a id="more"></a><p>Master节点的安装过程主要可以参考<a href="https://blog.jsjs.org/?p=414" target="_blank" rel="noopener"><strong>《使用kubeadm安装kubernetes1.7》</strong></a>。主要的流程是：</p><ul><li>安装Docker</li><li>安装Kubernetes相关组件</li><li>Kubeadm init Master节点</li><li>安装Overlay Network</li><li>Join Node节点</li></ul><p>下面主要说一些关于如何确定软件版本、如何制作软件镜像源等等细微但是很要命的问题。</p><h3 id="关于Docker"><a href="#关于Docker" class="headerlink" title="关于Docker"></a>关于Docker</h3><p>目前使用<strong>Docker 1.12</strong>版本是比较稳定的。Docker这种基础设施软件没有必要追最新的。在阿里云的CentOS只要直接<code>yum install docker</code>就可以安装这个版本了。</p><h3 id="关于操作系统"><a href="#关于操作系统" class="headerlink" title="关于操作系统"></a>关于操作系统</h3><p>本文讲的是在<strong>CentOS 7</strong>上安装Kubernetes。理由是Kubernetes对CentOS支持比较好，网上资料比较多。当然Ubuntu的支持应该也是没问题的。</p><h3 id="关于K8s源"><a href="#关于K8s源" class="headerlink" title="关于K8s源"></a>关于K8s源</h3><p>因为国内的网络环境，我们在服务上明显是不能直接访问国外的镜像源的。所以我们找掌握寻找镜像源，以及在必要的时候自己制作镜像源的技能（因为云服务商的镜像源不一定同步了最新的版本）。</p><p>所以我们可以直接用阿里云的源，比如：</p><pre><code>cat &gt;&gt; /etc/yum.repos.d/kubernetes.repo &lt;&lt;EOF[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=0EOF</code></pre><p>目前阿里云的源是1.7.5的，Kubernetes目前的稳定版本是1.8.x，1.9.x马上要发布了。可见过国内的源虽然可以用，但版本上不会特别新。</p><p>第二种办法是在国外服务器上下载镜像然后上传到国内服务器。<strong>这样的好处是可以任意选择自己想要的版本</strong>。具体可以参考使用上文中的《kubeadm安装kubernetes1.7》。</p><h3 id="关于镜像"><a href="#关于镜像" class="headerlink" title="关于镜像"></a>关于镜像</h3><p>Kubernetes安装过程中一个很大的问题，在于相关组件的镜像都是托管在Google Container Registry上的。国内的镜像加速一般针对的是Dockerhub上的镜像。所以国内的服务器是没法直接安装GCR上的镜像的。</p><p>这个问题其实很好解决，首先我们可以<strong>自己在本地翻墙拉到镜像，并把镜像push到阿里云的镜像仓库</strong>。拉镜像上传的脚本如下：</p><pre><code>#!/bin/bashset -o errexitset -o nounsetset -o pipefailKUBE_VERSION=v1.7.5KUBE_PAUSE_VERSION=3.0ETCD_VERSION=3.0.17DNS_VERSION=1.14.4GCR_URL=gcr.io/google_containersALIYUN_URL=registry.cn-hangzhou.aliyuncs.com/muxiimages=(kube-proxy-amd64:${KUBE_VERSION}kube-scheduler-amd64:${KUBE_VERSION}kube-controller-manager-amd64:${KUBE_VERSION}kube-apiserver-amd64:${KUBE_VERSION}pause-amd64:${KUBE_PAUSE_VERSION}etcd-amd64:${ETCD_VERSION}k8s-dns-sidecar-amd64:${DNS_VERSION}k8s-dns-kube-dns-amd64:${DNS_VERSION}k8s-dns-dnsmasq-nanny-amd64:${DNS_VERSION})for imageName in ${images[@]} ; do  docker pull $GCR_URL/$imageName  docker tag $GCR_URL/$imageName $ALIYUN_URL/$imageName  docker login  docker push $ALIYUN_URL/$imageName  docker rmi $ALIYUN_URL/$imageNamedone</code></pre><p>K8s每个版本需要的镜像版本号在<a href="https://kubernetes.io/docs/admin/kubeadm/#custom-images" target="_blank" rel="noopener">kubeadm Setup Tool Reference Guide</a>这个文档的的Running kubeadm without an internet connection一节里有写。所以可以根据安装的实际版本来跳帧这个脚本的参数。<strong>注意把上面的镜像地址换成自己的。muxi是你创建的一个namespace，而不是仓库名</strong>。</p><p>而Kubernetes也提供了镜像地址相关的配置项，一共有三个：</p><p>一个配置文件：</p><pre><code>cat &gt; /etc/systemd/system/kubelet.service.d/20-pod-infra-image.conf &lt;&lt;EOF[Service]Environment=&quot;KUBELET_EXTRA_ARGS=--pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/muxi/pause-amd64:3.0&quot;EOF</code></pre><p>两个环境变量：</p><pre><code>export KUBE_REPO_PREFIX=&quot;registry.cn-hangzhou.aliyuncs.com/muxi&quot;export KUBE_ETCD_IMAGE=&quot;registry.cn-hangzhou.aliyuncs.com/muxi/etcd-amd64:3.0.17&quot;</code></pre><h3 id="Trouble-Shooting"><a href="#Trouble-Shooting" class="headerlink" title="Trouble Shooting"></a>Trouble Shooting</h3><p>解决了获取镜像的问题之后，K8s集群的搭建应该就问题不大了。我们可以通过：</p><pre><code>kubectl get pods --all-namepaceskubectl get nodes kubectl get cs </code></pre><p>这几个命令来看集群的运行情况。</p><p>但有一些特殊的问题还是会存在，所以接下来我们就看看如何解决这些问题：</p><h4 id="Kubeadm-init卡住"><a href="#Kubeadm-init卡住" class="headerlink" title="Kubeadm init卡住"></a>Kubeadm init卡住</h4><pre><code>systemctl status kubelet</code></pre><p>首先中断初始化，查看kubelet的状态。如果出现cgroupfs相关问题，那就需要同步Docker和kubelet的cgroupfs设置。将两者设置为一样的。具体可以看<a href="https://github.com/kubernetes/kubernetes/issues/43805#issuecomment-320965626" target="_blank" rel="noopener">这里</a>。笔者用的Docker 1.12和Kubernetes 1.7的cgroupfs默认都是systemd，所以没有问题。</p><h4 id="Flannel下Pod不能访问网络问题"><a href="#Flannel下Pod不能访问网络问题" class="headerlink" title="Flannel下Pod不能访问网络问题"></a>Flannel下Pod不能访问网络问题</h4><p>之前遇到过k8s+flannel这个组合下Pod不能访问外网的问题。解决方案如下：</p><p>在Master节点创建一个busybox Pod：</p><p><em>busybox.yaml</em></p><pre><code>apiVersion: v1kind: Podmetadata:  name: busybox  namespace: defaultspec:  containers:  - image: busybox    command:      - sleep      - &quot;3600&quot;    imagePullPolicy: IfNotPresent    name: busybox</code></pre><p>然后运行<code>kubectl create -f busybox.yaml</code>创建这个Pod。</p><p>在Master节点运行<code>ping 8.8.8.8</code>，或者ping另外的公网IP。如果没有成功，则用<code>traceroute</code>查看请求的路由列表。一般来说路由列表到容器的网关之后就断了。我们记下这个网关的IP。在镜像中运行<code>kubectl exec busybox -- ifconfig</code>，查看eth0设备的IP，这个IP应该就是之前traceroute得到的IP。所以问题出在这个网卡上。</p><p>在Master节点运行<code>ifconfig</code>，我们看到cni0网卡的IP和之前Pod里的默认网卡的网段是重叠的。所以Pod中的请求就会走这个设备。</p><p>Pod访问公网应该走的是节点上的Docker0设备。cni0是Flannel的虚拟网卡，这个网络自然是不通外网的。为了解决这个问题，我们运行：</p><pre><code>/sbin/iptables -t nat -I POSTROUTING -s 10.24.1.0/24 -j MASQUERADE</code></pre><p>其中10.24.1.0/24就是cni0设备的IP。</p><h3 id="Links"><a href="#Links" class="headerlink" title="Links"></a>Links</h3><p>几个不错的社区和博客。大家遇到问题的时候可以去浏览一下看看。说不定会有解决方案。</p><ul><li><a href="http://tonybai.com/articles/" target="_blank" rel="noopener">Tonybai的博客</a></li><li><a href="https://www.kubernetes.org.cn/" target="_blank" rel="noopener">K8s中文社区</a></li><li><a href="http://dockone.io/" target="_blank" rel="noopener">Docker.io</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;安装Kubernetes向来不是一件容易的事情。之前在集群上部署好的K8s突然出了问题，于是需要重新安装。这次没有之前那次那么顺利了，出现了很多奇怪的问题。但经过一番实践和总结，总算是得出了Kubernetes国内安装的一个不完全指南。这个指南理论上适用于任意版本的K8s。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="http://yoursite.com/categories/Kubernetes/"/>
    
      <category term="Cloud" scheme="http://yoursite.com/categories/Kubernetes/Cloud/"/>
    
    
      <category term="Cloud" scheme="http://yoursite.com/tags/Cloud/"/>
    
      <category term="Kubernetes" scheme="http://yoursite.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>React 16 Fiber源码速览</title>
    <link href="http://yoursite.com/2017/09/28/react-16-source/"/>
    <id>http://yoursite.com/2017/09/28/react-16-source/</id>
    <published>2017-09-28T14:50:36.000Z</published>
    <updated>2018-06-19T11:44:13.824Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文的写作有一部分没有完成，打算针对React 16.3再对本文进行修改，请大家留意</p></blockquote><p>React 16在近期发布了。除了将备受争议的BSD+Patents协议改为MIT协议之外，React 16还带来了许多新特性，比如：</p><ul><li>允许在render函数中返回节点数组和字符串。</li></ul><pre><code>render() {  // 再也不用在外面套一个父节点了  return [    // 别忘了加上key    &lt;li key=&quot;A&quot;&gt;First item&lt;/li&gt;,    &lt;li key=&quot;B&quot;&gt;Second item&lt;/li&gt;,    &lt;li key=&quot;C&quot;&gt;Third item&lt;/li&gt;,  ];}</code></pre><ul><li>提供更好的错误处理。</li><li>支持自定义DOM属性。</li></ul><a id="more"></a><p>但最关键的一点还是：</p><p><img src="http://wx2.sinaimg.cn/large/64c45edcgy1fjzoreufb4j20xm0kcjwa.jpg" alt="twitter"></p><p>没错，React 16是一次<strong>重写</strong>，在保持API不变的情况下，将核心架构改为了代号为Fiber的异步渲染架构。新架构带来了的变化有：</p><ul><li>体积减小</li></ul><p><img src="http://wx1.sinaimg.cn/large/64c45edcgy1fjzoretq6dj210q0bu776.jpg" alt="file size"></p><ul><li><p><a href="https://medium.com/@aickin/whats-new-with-server-side-rendering-in-react-16-9b0d78585d67" target="_blank" rel="noopener">服务端渲染速度大幅提升</a></p></li><li><p>更responsive的界面</p></li></ul><h3 id="一次预谋已久的重写"><a href="#一次预谋已久的重写" class="headerlink" title="一次预谋已久的重写"></a>一次预谋已久的重写</h3><p>Fiber这个架构并不是突然冒出来的。Facebook的工程师在设计React之初就设想未来的UI渲染会是异步的。从<code>setState()</code>的设计和React内部的事务机制可以看出这点。</p><p>在去年，React的开发者<a href="https://github.com/acdlite" target="_blank" rel="noopener">Andrew Clark</a>在社区中放出了Fiber架构的一个<a href="https://github.com/acdlite/react-fiber-architecture" target="_blank" rel="noopener">文档</a>。描述了Fiber架构的基本信息。同时表示Facebook的工程师正在实现这个新架构。今年3月的React Conf 2017上，Lin Clark做了<a href="https://www.youtube.com/watch?v=ZCuYPiUIONs" target="_blank" rel="noopener">A Cartoon Intro to Fiber</a>这个分享，介绍了Fiber架构的工作原理。今年9月，Fiber架构随着React 16正式发布。Fiber架构的代码放在原来的React仓库之中，并且可以通过运行时的判断来切换新老架构，方便测试和部署。因此Fiber的开发是一个渐进的过程。<a href="http://isfiberreadyyet.com" target="_blank" rel="noopener">这个网站</a>实时展示了Fiber通过的测试用例，随着所有用例的通过，Fiber也正式发布了。有趣的是，在React 16发布之前，Fiber架构的React就已经运行在Facebook的产品中了。FB的工程师表示看到新架构在线上产品运行起来，是很激动人心的。具体的情况可以看这篇博客：<a href="https://code.facebook.com/posts/1716776591680069/react-16-a-look-inside-an-api-compatible-rewrite-of-our-frontend-ui-library/" target="_blank" rel="noopener">React 16: A look inside an API-compatible rewrite of our frontend UI library</a>。</p><h3 id="Fiber概念简介"><a href="#Fiber概念简介" class="headerlink" title="Fiber概念简介"></a>Fiber概念简介</h3><p>本文的题目是React 16 Fiber源码速览，所以关注的主要是Fiber相关的代码。在分析源码之前，首先介绍一些基本概念。</p><blockquote><p>推荐看上文中提到的<a href="https://www.youtube.com/watch?v=ZCuYPiUIONs" target="_blank" rel="noopener">A Cartoon Intro to Fiber</a>。这个分享比较系统和形象解释了Fiber架构的工程流程，并且使用了React源码中的术语。有助于理解Fiber的概念和源码。下文中的配图也来自这个分享。</p></blockquote><h4 id="reconciler-VS-renderer"><a href="#reconciler-VS-renderer" class="headerlink" title="reconciler VS renderer"></a>reconciler VS renderer</h4><p><img src="http://wx1.sinaimg.cn/large/64c45edcgy1fk0jck2eo4j20gk09a76l.jpg" alt="Reconciler VS Renderer"></p><p>Reconciler就是我们所说的Virtul DOM，用于计算新老View的差异。React 16之前的reconciler叫Stack reconciler。Fiber是React的新reconciler。Renderer则是和平台相关的代码，负责将View的变化渲染到不同的平台上，DOM、Canvas、Native、VR、WebGL等等平台都有自己的renderer。我们可以看出reconciler是React的核心代码，是各个平台共用的。因此这次React的reconciler更新到Fiber架构是一次重量级的核心架构的更换。</p><p>由reconciler和renderer两个概念引出的是phase的概念。Phase指的是React组件渲染时的阶段。第一阶段是reconciliation，这一阶段做的是Fiber的update，然后产出的是effect list（可以想象成将老的View更新到新的状态所需要做的DOM操作的列表）。这一个阶段是没有副作用的，因此这个过程可以被打断，然后恢复执行。第二阶段是commit阶段。Reconciliation产生的effect list只有在commit之后才会生效，也就是真正应用到DOM中。这一阶段往往不会执行太长时间，因此是同步的，这样也避免了组件内视图层结构和DOM不一致。</p><h4 id="Fiber是什么"><a href="#Fiber是什么" class="headerlink" title="Fiber是什么"></a>Fiber是什么</h4><p>React源码中的注释说：</p><blockquote><p>A Fiber is work on a Component that needs to be done or was done. There can be more than one per component.</p></blockquote><p>简单的说，一个Fiber就是一个POJO对象，代表了组件上需要做的工作。一个React Element可以对应一个或多个Fiber节点。</p><p>在render函数中创建的React Element树在第一次渲染的时候会创建一颗结构一模一样的Fiber节点树。不同的React Element类型对应不同的Fiber节点类型。一个React Element的工作就由它对应的Fiber节点来负责。我们如果在console中打印React 16的组件实例，会发现有一个<code>_reactInternalFiber</code>属性指向它对应的Fiber实例。</p><p>虽然React的代码中其实没有明确的Virtul DOM概念，但Fiber和我们概念中的Virtul DOM树是等价的。</p><p>Fiber带来了一个给React的渲染带来了重要的变化。React内部有事务的概念。之前React渲染相关的事务是连续的，一旦开始就会run to completion。现在React的事务则是由一系列Fiber的更新组成的，因此React可以在多个帧中断断续续的更新Fiber，最后commit变化。</p><p>那为什么说一个React Element可以对应不止一个Fiber呢？因为Fiber在update的时候，会从原来的Fiber（我们称为current）clone出一个新的Fiber（我们称为alternate）。两个Fiber diff出的变化（side effect）记录在alternate上。所以一个组件在更新时最多会有两个Fiber与其对应，在更新结束后alternate会取代之前的current的成为新的current节点。</p><h4 id="Fiber节点的数据结构"><a href="#Fiber节点的数据结构" class="headerlink" title="Fiber节点的数据结构"></a>Fiber节点的数据结构</h4><p>下面介绍Fiber类型的重要属性：</p><pre><code>{    tag: TypeOfWork, // fiber的类型，下一节会介绍    alternate: Fiber|null, // 在fiber更新时克隆出的镜像fiber，对fiber的修改会标记在这个fiber上    return: Fiber|null, // 指向fiber树中的父节点    child: Fiber|null, // 指向第一个子节点    sibling: Fiber|null, // 指向兄弟节点    effectTag: TypeOfSideEffect, // side effect类型，下文会介绍    nextEffect: Fiber | null, // 单链表结构，方便遍历fiber树上有副作用的节点    pendingWorkPriority: PriorityLevel, // 标记子树上待更新任务的优先级}</code></pre><p>在实际的渲染过程中，Fiber节点构成了一颗树。这棵树在数据结构上是通过单链表的形式构成的，Fiber节点上的<code>chlid</code>和<code>sibling</code>属性分别指向了这个节点的第一个子节点和相邻的兄弟节点。这样就可以遍历整个Fiber树了。</p><p>Fiber树的图示如下：</p><p><img src="http://wx3.sinaimg.cn/large/64c45edcgy1fkc8x8n8x2j20fa0co428.jpg" alt="fiber tree"></p><h4 id="TypeOfWork"><a href="#TypeOfWork" class="headerlink" title="TypeOfWork"></a>TypeOfWork</h4><p>这是源码中的typeOfWork，代表React中不同类型的fiber节点。</p><pre><code>{  IndeterminateComponent: 0, // Before we know whether it is functional or class  FunctionalComponent: 1,  ClassComponent: 2,  HostRoot: 3, // Root of a host tree. Could be nested inside another node.  HostPortal: 4, // A subtree. Could be an entry point to a different renderer.  HostComponent: 5,  HostText: 6,  CoroutineComponent: 7,  CoroutineHandlerPhase: 8,  YieldComponent: 9,  Fragment: 10,}s</code></pre><p>对几个常用的类型作一下解释：</p><p><strong>ClassComponent</strong></p><p>就是应用层面的React组件。ClassComponent是一个继承自React.Component的类的实例。</p><p><strong>HostRoot</strong></p><p>ReactDOM.render()时的根节点。</p><p><strong>HostComponent</strong></p><p>React中最常见的抽象节点，是ClassComponent的组成部分。具体的实现取决于React运行的平台。在浏览器环境下就代表DOM节点，可以理解为所谓的虚拟DOM节点。HostComponent中的Host就代码这种组件的具体操作逻辑是由Host环境注入的。</p><h4 id="TypeOfSideEffect"><a href="#TypeOfSideEffect" class="headerlink" title="TypeOfSideEffect"></a>TypeOfSideEffect</h4><blockquote><p>说一下这是以二进制位表示的。可以多个叠加。</p></blockquote><pre><code>{  NoEffect: 0,            PerformedWork: 1,     Placement: 2, // 插入           Update: 4, // 更新             PlacementAndUpdate: 6,   Deletion: 8, // 删除     ContentReset: 16,    Callback: 32,        Err: 64,           Ref: 128,          };</code></pre><h4 id="Priority"><a href="#Priority" class="headerlink" title="Priority"></a>Priority</h4><p>Priority指的是Fiber中一个work的优先级。这是React源码中的对Priority类型的定义：</p><pre><code>{  NoWork: 0, // No work is pending.  SynchronousPriority: 1, // For controlled text inputs. Synchronous side-effects.  TaskPriority: 2, // Completes at the end of the current tick.  HighPriority: 3, // Interaction that needs to complete pretty soon to feel responsive.  LowPriority: 4, // Data fetching, or result from updating stores.  OffscreenPriority: 5, // Won&#39;t be visible but do the work in case it becomes visible.}</code></pre><p>我们可以把Priority分为同步和异步两个类别，同步优先级的任务会在当前帧完成，包括SynchronousPriority和TaskPriority。异步优先级的任务则可能在接下来的几个帧中被完成，包括HighPriority、LowPriority以及OffscreenPriority。</p><h3 id="React-16-Fiber源码目录结构"><a href="#React-16-Fiber源码目录结构" class="headerlink" title="React 16 Fiber源码目录结构"></a>React 16 Fiber源码目录结构</h3><p>React库的入口、组件的基类<code>ReactComponent</code>、<code>ReactElement.createElement</code>函数等等所有平台公用的代码位于<code>src/isomorphic</code>下。</p><p>我们关注的Fiber代码位于<code>src/renderers/shared/fiber</code>下。我们先来看看<code>src/renderers</code>下面有什么：</p><p><img src="http://wx2.sinaimg.cn/large/64c45edcgy1fk1ka90rh0j20s109xacl.jpg" alt="renderers"></p><p>可以看到<code>src/renderers</code>下的代码就是上文介绍的renderer，分dom、native、art等等平台。那我们再看看<code>src/renderers/shared</code>目录下有什么：</p><p><img src="http://wx2.sinaimg.cn/large/64c45edcgy1fk1kj1kekxj20s10ezq6q.jpg" alt="renderers/shared"></p><p><code>src/renderers/shared</code>其实就是reconciler相关的代码了。可以看到里面有fiber和stack新老两大reconciler（在笔者发文时，Stack reconciler已经完成了它的使命，相关的代码已经被移除了）。</p><p>最后让我们来看看<code>src/renderers/shared/fiber</code>下的代码：</p><p><img src="http://wx2.sinaimg.cn/large/64c45edcgy1fk1m7cifluj20sh0pk10v.jpg" alt="fiber"></p><p>这些就是React fiber的核心代码了。Fiber节点的定义在<code>ReactFiber.js</code>中，Fiber的reconciler构造函数在<code>ReactFiberReconciler.js</code>中，Fiber节点的工作流程由<code>ReactFiberBeginWork.js</code>、<code>ReactFiberCommitWork.js</code>和<code>ReactFiberCompleteWork.js</code>组成。Fiber的子节点reconcile逻辑在<code>ReactChildFiber.js</code>中，<code>ReactFiberScheduler.js</code>则是调度相关的逻辑。接下来就让我们通过具体的场景，来分析React 16的源码吧！</p><h3 id="阅读React源码须知"><a href="#阅读React源码须知" class="headerlink" title="阅读React源码须知"></a>阅读React源码须知</h3><p>下面简单介绍一下在React源码中，起辅助作用的代码。以免大家在看源码时被这些代码所迷惑。</p><h4 id="flow-type"><a href="#flow-type" class="headerlink" title="flow type"></a>flow type</h4><p>React使用了flow作为静态类型检查工具。所以React源码中都是带有类型声明的。这对熟悉Java或者C++这些静态类型语言的同学应该不陌生。<strong>类型声明对于快速理解源码也是有很大帮助的</strong>。</p><h4 id="if-DEV"><a href="#if-DEV" class="headerlink" title="if (__DEV__)"></a><code>if (__DEV__)</code></h4><p>React源码中常常有<code>if (__DEV__)</code>这样的代码，比如：</p><pre><code>if (__DEV__) {    warning(      shouldUpdate !== undefined,      &#39;%s.shouldComponentUpdate(): Returned undefined instead of a &#39; +        &#39;boolean value. Make sure to return true or false.&#39;,      getComponentName(workInProgress) || &#39;Unknown&#39;,    );  }</code></pre><p>这些代码是为了更好的开发者体验而编写的。React中的友好的报错，render性能测试等等代码都是写在<code>if (__DEV__)</code>中的。在production build的时候，这些代码不会被打包。因此我们可以毫无顾虑的提供专为开发者服务的代码。React的最佳实践之一就是在开发时使用development build，在生产环境使用production build。</p><p>大家在刚开始接触源码时可以跳过<code>if (__DEV__)</code>中的代码，专注于理解核心的部分。</p><h4 id="源码阅读小技巧"><a href="#源码阅读小技巧" class="headerlink" title="源码阅读小技巧"></a>源码阅读小技巧</h4><p>如果读者想在阅读文本之后打算自己深入探索React源码，我可以给出一些阅读源码的小技巧。如果对于React中某个方法的调用过程感兴趣，可以在本地用create-react-app新建一下小demo项目，然后直接在node_modules中的react-dom.development.js和react.development.js两个文件里的对应方法<strong>打断点</strong>。这样在中断的时候就可以看到整个<strong>调用栈</strong>了，Chrome种可以通过点击调用栈切换到其中任何一帧的状态。如果发现调用过程中有自己感兴趣的函数，可以clone React的整个仓库，用编辑器对想要查看的函数进行<strong>全局搜索</strong>，找到那个函数的源码进行阅读。此外还有一个小tip，如果对某个特性的实现感兴趣，可以去<strong>搜索React的pull request和issue列表</strong>，说不定可以找到当初实现这个特性时候提的PR，PR中一般会写实现时的一些考虑。另外React源码的注释也是非常详尽的，有些已经等于简单的文档了，所以<strong>仔细的阅读注释</strong>也是理解源码的捷径之一。</p><h4 id="本文源码的时效性"><a href="#本文源码的时效性" class="headerlink" title="本文源码的时效性"></a>本文源码的时效性</h4><p>React 16.0发布后，新架构的很多特性还没有完全开放，因此React这段时间还在一个积极的开发过程中，源码变动会比较大。本文是分析的源码是React v16.0的源码。大家在阅读时Github上的React源码时要注意，目前的master分支的React源码和本文中的源码会有一些差异。比如在本文发布时，React的目录结构就进行了调整，源码从src中转移到了packages目录下，按react、react-reconciler、react-dom等等NPM模块的方式划分。还有一些fiber的实现也在进行一些小的重构。比如在performWork相关的代码中加入performWorkOnRoot和renderRoot这几个函数，通过准确的命名让函数的作用更清晰。又比如Priority的概念直接被expirationTime取代了，workLoop中直接根据expirationTime来判断任务的执行时机。所以<strong>推荐大家阅读master分支下的最新代码</strong>，因为React 16在代码质量上的确还处于一个未完成的状态，随着进一步的开发，源码的可读性会更高。</p><h3 id="确定源码分析的入口"><a href="#确定源码分析的入口" class="headerlink" title="确定源码分析的入口"></a>确定源码分析的入口</h3><h3 id="React-16组件源码分析：用户触发的setState开启的一次渲染"><a href="#React-16组件源码分析：用户触发的setState开启的一次渲染" class="headerlink" title="React 16组件源码分析：用户触发的setState开启的一次渲染"></a>React 16组件源码分析：用户触发的<code>setState</code>开启的一次渲染</h3><p>我们知道，React的渲染是由<code>setState</code>触发的，所以就让我们从<code>setState</code>入手，来分析React 16的组件渲染流程。</p><h4 id="setState"><a href="#setState" class="headerlink" title="setState"></a>setState</h4><p><code>setState</code>方法是React基类上的一个方法。因此位于<code>src/isomorphic</code>下的<code>modern/class/ReactBaseClasses.js</code>：</p><p><img src="http://wx2.sinaimg.cn/large/64c45edcgy1fk1m7bwt6vj20fd05pdgr.jpg" alt="setState"></p><p>我们看到<code>setState</code>调用了<code>this.updater.enqueueSetState</code>。updater是renderer在渲染的时候注入的对象，这个对象由reconciler提供。具体的逻辑可以看<code>ReactDOM.render</code>相关的代码，这里就不展开了。</p><h4 id="enqueueSetState"><a href="#enqueueSetState" class="headerlink" title="enqueueSetState"></a>enqueueSetState</h4><p>既然updater是reconciler提供的，那我们就可以在fiber的代码中找到它。updater就位于<code>src/renderers/shared/fiberReactFiberCompleteWork.js</code>中。</p><p><img src="http://wx3.sinaimg.cn/large/64c45edcgy1fk1m7by675j20ch06tjsg.jpg" alt="enqueueSetState"></p><p>这里只截取了一部分的updater代码，可以看到updater提供了<code>enqueueSetState</code>方法，这个方法首先从全局拿到React组件实例对应的fiber，然后拿到了fiber的优先级。最后调用了<code>addUpdate</code>向队列中推入需要更新的fiber，并调用<code>scheduleUpdate</code>触发调度器调度一次新的更新。</p><p>熟悉React源码的朋友应该知道，<code>setState</code>的流程到这里为止，和React 15的流程基本是一样的。从下面开始，我们就可以看到Fiber架构的不同之处了。</p><h4 id="addUpdate"><a href="#addUpdate" class="headerlink" title="addUpdate"></a>addUpdate</h4><p>我们首先来看<code>addUpdate</code>函数，这个函数向Fiber的更新队列里加入一次更新：</p><pre><code>function addUpdate(  fiber: Fiber,  partialState: PartialState&lt;any, any&gt; | null,  callback: mixed,  priorityLevel: PriorityLevel,): void {  const update = {    priorityLevel,    partialState,    callback,    isReplace: false,    isForced: false,    isTopLevelUnmount: false,    next: null,  };  insertUpdate(fiber, update);}</code></pre><p><code>addUpdate</code>函数组装了一个update，然后将fiber和update传入了insertUpdate函数中。我们先来看一下这里用到的两个类型，Update和UpdateQueue：</p><pre><code>type UpdateQueue = {  first: Update | null,  last: Update | null,  hasForceUpdate: boolean,  callbackList: null | Array&lt;Callback&gt;,  // Dev only  isProcessing?: boolean,};</code></pre><pre><code>type Update = {  priorityLevel: PriorityLevel,  partialState: PartialState&lt;any, any&gt;,  callback: Callback | null,  isReplace: boolean,  isForced: boolean,  isTopLevelUnmount: boolean,  next: Update | null,};</code></pre><p>我们可以看到，UpdateQueue是一个单向链表，有first和last指针指向链表的头部和尾部。其中的每一个Update都有一个next属性指向下一个Update。这样的数据结构在React 16中是很常见的。</p><p>之前说到，在更新时，一个React element会有一个current fiber和一个alternate fiber。我们又把alternate fiber叫working in progress fiber。这两个fiber都有一个Update Queue。这两个Queue里面的item的引用是相同的，也就是所谓的persistent structure。区别在于，working in progress fiber会在更新完一个队列项之后将其从队列中移除。所以working in progress update queue永远是current queue的一个子集。在更新完成之后，working in progress fiber取代current fiber成为新的current fiber。如果更新中断（有更高优先级的更新插入），current fiber的update queue就可以作为备份，使得之前中断的更新可以重新开始。</p><p>再看<code>insertUpdate</code>，这个函数处理了将一个update插入到current queue和work-in-progress queue两个队列中的逻辑：</p><p><img src="http://wx4.sinaimg.cn/large/64c45edcgy1fkbtqzlproj21eq210nik.jpg" alt="insertUpdate"></p><h4 id="scheduleUpdate"><a href="#scheduleUpdate" class="headerlink" title="scheduleUpdate"></a>scheduleUpdate</h4><p>看完了<code>addUpdate</code>相关的逻辑，我们再来看<code>scheduleUpdate</code>：</p><p><img src="http://wx4.sinaimg.cn/large/64c45edcgy1fkbz4oej2tj21do3c91kx.jpg" alt="scheduleUpdate"></p><h4 id="performWork"><a href="#performWork" class="headerlink" title="performWork"></a>performWork</h4><p>performWork的作用就是“刷新”待更新队列，执行待更新的事务：</p><p><img src="http://wx2.sinaimg.cn/large/64c45edcgy1fkc077e5vgj20g2244gun.jpg" alt="performWork"></p><p>performWork的代码很长，其中很大一部分是错误处理代码，这些代码和React16中的新特性有关，官方博客的介绍如下：</p><blockquote><p>Previously, runtime errors during rendering could put React in a broken state, producing cryptic error messages and requiring a page refresh to recover. To address this problem, React 16 uses a more resilient error-handling strategy. By default, if an error is thrown inside a component’s render or lifecycle methods, the whole component tree is unmounted from the root. This prevents the display of corrupted data. However, it’s probably not the ideal user experience.<br>Instead of unmounting the whole app every time there’s an error, you can use error boundaries. Error boundaries are special components that capture errors inside their subtree and display a fallback UI in its place. Think of error boundaries like try-catch statements, but for React components.</p></blockquote><p>我们需要关注的函数，一个是<code>workLoop</code>，这个函数是React更新pendingWork队列的主循环。一个是<code>scheduleDeferredCallback</code>，这个函数会在未来安排一次更新，来处理<code>workLoop</code>中没有做完的事务。</p><h4 id="workLoop"><a href="#workLoop" class="headerlink" title="workLoop"></a>workLoop</h4><p><em>图片注释还需要打磨</em></p><p>我们来看<code>workLoop</code>的代码：</p><p><img src="http://wx4.sinaimg.cn/large/64c45edcgy1fkc0p3sx5fj21e93z97wh.jpg" alt="workLoop"></p><p>除了图中所注释的，<code>workLoop</code>中有一个值得注意的细节。我们看到，loop中首先判断nextUnitOfWork的优先级是不是高于或等于TaskPriority。如果不是，则进入另一个分支，这个分支和前一个在对nextUnitOfWork的处理上有着微妙的区别。之前在介绍Priority的时候我们说到过，TaskPriority以及更高的优先级属于同步优先级，这些更新会在nextTick之前完成。所以loop中的两个分支其实就是对同步和异步的任务做了不同的处理。两个分支的区别主要是第二个分支使用了deadline.timeRemaining()来判断是否还有时间继续处理任务。</p><p>在之前的分析中，我们没有关注deadline这个参数，workLoop中的这个参数是从performWork中传入的，而performWork中的deadline参数是由scheduleUpdateImpl传入的。scheduleUpdateImpl给同步优先级的任务的deadline参数传入的是null。这是符合常理的，因为同步优先级的任务会一定会在一次workLoop中执行完毕。scheduleUpdateImpl中的异步优先级的任务在scheduleDeferredCallback中处理，我们看这个函数的类型：</p><pre><code>scheduleDeferredCallback(    callback: (deadline: Deadline) =&gt; void,  ): number | void,</code></pre><p>deadline出现了！所以异步任务的deadline是在被scheduleDeferredCallback调用时传入的。</p><h4 id="scheduleDeferredCallback"><a href="#scheduleDeferredCallback" class="headerlink" title="scheduleDeferredCallback"></a>scheduleDeferredCallback</h4><p>让我们来看看<code>scheduleDeferredCallback</code>这个函数。全局搜索一番，我们发现这个函数是在renderer初始化时被注入的。</p><p>React 16抽象出了一个叫<code>ReactFiberReconciler</code>的工厂函数。这个函数接收一个<code>HostConfig</code>类型的参数，返回一个Reconciler。每个renderer初始化时需要传入当前平台相关的配置，也就是一个<code>HostConfig</code>实例，才能拿到一个自定义的Reconciler。</p><blockquote><p>这里说一点题外话，React抽象出这个工厂函数意味着React标准化了自定义Renderer的接口。Renderer通过<code>ReactFiberReconciler</code>这个API就可以将自定义Renderer接入FiberReconciler。<a href="https://github.com/nitin42/Making-a-custom-React-renderer" target="_blank" rel="noopener">Making-a-custom-React-renderer</a>就利用了这个函数来打造自定义Renderer。</p></blockquote><p><code>HostConfig</code>的类型签名是这样的：</p><pre><code>export type HostConfig&lt;T, P, I, TI, PI, C, CX, PL&gt; = {  getRootHostContext(rootContainerInstance: C): CX,  getChildHostContext(parentHostContext: CX, type: T, instance: C): CX,  getPublicInstance(instance: I | TI): PI,  createInstance(    type: T,    props: P,    rootContainerInstance: C,    hostContext: CX,    internalInstanceHandle: OpaqueHandle,  ): I,  appendInitialChild(parentInstance: I, child: I | TI): void,  finalizeInitialChildren(    parentInstance: I,    type: T,    props: P,    rootContainerInstance: C,  ): boolean,  prepareUpdate(    instance: I,    type: T,    oldProps: P,    newProps: P,    rootContainerInstance: C,    hostContext: CX,  ): null | PL,  commitUpdate(    instance: I,    updatePayload: PL,    type: T,    oldProps: P,    newProps: P,    internalInstanceHandle: OpaqueHandle,  ): void,  commitMount(    instance: I,    type: T,    newProps: P,    internalInstanceHandle: OpaqueHandle,  ): void,  shouldSetTextContent(type: T, props: P): boolean,  resetTextContent(instance: I): void,  shouldDeprioritizeSubtree(type: T, props: P): boolean,  createTextInstance(    text: string,    rootContainerInstance: C,    hostContext: CX,    internalInstanceHandle: OpaqueHandle,  ): TI,  commitTextUpdate(textInstance: TI, oldText: string, newText: string): void,  appendChild(parentInstance: I, child: I | TI): void,  appendChildToContainer(container: C, child: I | TI): void,  insertBefore(parentInstance: I, child: I | TI, beforeChild: I | TI): void,  insertInContainerBefore(    container: C,    child: I | TI,    beforeChild: I | TI,  ): void,  removeChild(parentInstance: I, child: I | TI): void,  removeChildFromContainer(container: C, child: I | TI): void,  scheduleDeferredCallback(    callback: (deadline: Deadline) =&gt; void,  ): number | void,  prepareForCommit(): void,  resetAfterCommit(): void,  // Optional hydration  canHydrateInstance?: (instance: I | TI, type: T, props: P) =&gt; boolean,  canHydrateTextInstance?: (instance: I | TI, text: string) =&gt; boolean,  getNextHydratableSibling?: (instance: I | TI) =&gt; null | I | TI,  getFirstHydratableChild?: (parentInstance: I | C) =&gt; null | I | TI,  hydrateInstance?: (    instance: I,    type: T,    props: P,    rootContainerInstance: C,    hostContext: CX,    internalInstanceHandle: OpaqueHandle,  ) =&gt; null | PL,  hydrateTextInstance?: (    textInstance: TI,    text: string,    internalInstanceHandle: OpaqueHandle,  ) =&gt; boolean,  didNotHydrateInstance?: (parentInstance: I | C, instance: I | TI) =&gt; void,  didNotFindHydratableInstance?: (    parentInstance: I | C,    type: T,    props: P,  ) =&gt; void,  didNotFindHydratableTextInstance?: (    parentInstance: I | C,    text: string,  ) =&gt; void,  useSyncScheduling?: boolean,};</code></pre><p>这里主要包括一些平台相关的代码，比如节点的操作（<code>insertBefore</code>和<code>appendChild</code>等等），还有一些配置项，比如<code>useSyncScheduling</code>。我们看到<code>scheduleDeferredCallback</code>就在其中。我们来看看renderer初始化的代码：</p><p>在React DOM的入口中：</p><pre><code>scheduleDeferredCallback: ReactDOMFrameScheduling.rIC,</code></pre><p>在React Native的入口中：</p><pre><code>scheduleDeferredCallback: global.requestIdleCallback,</code></pre><p>我们可以看到<code>scheduleDeferredCallback</code>的实现和平台相关。在Native环境下，它是React Native的js runtime提供的<code>global.requestIdleCallback</code>，在浏览器环境下，它是<code>ReactDOMFrameScheduling.rIC</code>。</p><h4 id="Cooperative-Scheduling-amp-amp-requestIdleCallback"><a href="#Cooperative-Scheduling-amp-amp-requestIdleCallback" class="headerlink" title="Cooperative Scheduling &amp;&amp; requestIdleCallback"></a>Cooperative Scheduling &amp;&amp; requestIdleCallback</h4><p><code>window.requestIdleCallback</code>的函数签名和<code>scheduleDeferredCallback</code>是一模一样的。requestIdleCallback的callback接收一个<code>IdleDeadline</code>类型的参数。这个<code>IdleDeadline</code>和React中的<code>deadline</code>都有一个<code>timeRemaining</code>方法。</p><p>requestIdleCallback的W3C规范叫Cooperative Scheduling of Background Tasks。React官方在介绍fiber时也提到了Cooperative Scheduling这种技术。从源码来看，React主要利用了浏览器提供的requestIdleCallback API来实现这一特性。</p><p>相比于利用setTimout这样的API实现task scheduling，requestIdleCallback带来的Cooperative Scheduling让开发者让浏览器在空闲时间调用callback，并且在callback中可以获取到当前帧剩余的时间。利用这个信息我们可以合理的安排当前帧需要做的工作，如果工作太多而时间不够，就再调用requestIdleCallback来做剩余的工作。</p><p>requestIdleCallback的回调具体执行的时间点是在一帧开始，JavaScript执行完，浏览器执行渲染流程之后，到这帧结束之前。图示如下：</p><p><img src="http://wx1.sinaimg.cn/large/64c45edcgy1fkc7kiv42qj20kh03vq3a.jpg" alt="idleCallback"></p><p><code>deadline</code>中的<code>timeRemaining</code>的最大值是50ms，以免浏览器长期空闲时，callback的任务一直执行，使得UI不能及时响应用户输入。</p><h4 id="ReactDOMFrameScheduling-rIC"><a href="#ReactDOMFrameScheduling-rIC" class="headerlink" title="ReactDOMFrameScheduling.rIC"></a>ReactDOMFrameScheduling.rIC</h4><p><code>ReactDOMFrameScheduling.rIC</code>的逻辑是，如果浏览器实现了requestIdleCallback，就返回原生API。如果没有实现，就返回一个polyfill。这个polyfill的实现非常有趣，可以学到很多有意思的黑科技。</p><p>我们来看看<code>ReactDOMFrameScheduling.rIC</code>的实现：</p><p>虽然Chrome和Firefox都已经实现了requestIdleCallback，但某些浏览器还是需要polyfill，所以我们重点关注一下requestIdleCallback的polyfill的实现。</p><p>预估一个比较低的frame rate。requestAnimationFrame获取一帧开始，时间戳，触发一个message事件，postMessage在layout paint和composite之后被调用。deadline通过frame rate - rafTime可以得到</p><pre><code>| frame start time                                      deadline |[requestAnimationFrame] [layout] [paint] [composite] [postMessage]</code></pre><p>通过requestAnimationFrame直接的时间差获取过去两帧的准确frame rate，动态调整当前帧的frame rate。</p><h4 id="默认优先级"><a href="#默认优先级" class="headerlink" title="默认优先级"></a>默认优先级</h4><p>既然一次更新是同步还是异步是由优先级决定的，那我们在用户代码中通过<code>setState</code>来schedule的一次update的优先级是多少呢？</p><p>我们回顾一下<code>enqueueSetState</code>的代码：</p><p><img src="http://wx3.sinaimg.cn/large/64c45edcgy1fk1m7by675j20ch06tjsg.jpg" alt="enqueueSetState"></p><p><code>addUpdate</code>和<code>scheduleUpdate</code>的<code>priorityLevel</code>是通过<code>getPriorityContext(fiber, false)</code>获取的。</p><p>我们来看看<code>getPriorityContext</code>的实现：</p><p><img src="http://wx3.sinaimg.cn/large/64c45edcgy1fkd4vrvbycj20ox0ian05.jpg" alt="getPriorityContext"></p><p>所以我们得出了一个重要的结论。在React 16中，<strong>异步渲染默认是关闭的</strong>。用户代码的优先级是同步的。</p><h4 id="performUnitOfWork"><a href="#performUnitOfWork" class="headerlink" title="performUnitOfWork"></a>performUnitOfWork</h4><p>讲完了deadline对象的由来，我们回到workLoop，看看React是的reconcilation是如何进行的。我们可以看到首先被调用的是performUnitOfWork，这个函数做的就是所谓的reconcilation阶段的工作了。然后React将调用commitAllWork进入commit阶段，将reconcilation结果真正应用到DOM中。</p><p><img src="http://wx4.sinaimg.cn/large/64c45edcgy1fkc84mx1vij20fa0gq7af.jpg" alt="performUnitOfWork"></p><p>React 16保持了之前版本的事务风格，一个“work”会被分解为begin和complete两个阶段来完成。我们先关注beginWork</p><h4 id="beginWork"><a href="#beginWork" class="headerlink" title="beginWork"></a>beginWork</h4><p><img src="http://wx2.sinaimg.cn/large/64c45edcgy1fk6jdy25scj20wq1kvqdw.jpg" alt="beginWOrk"></p><p><code>beginWork</code>函数根据fiber节点不同的tag，调用对应的update方法。可以说是一个入口函数。真正的逻辑要看update开头的这一些了函数。</p><h4 id="updateClassComponent-amp-amp-updateHostComponent"><a href="#updateClassComponent-amp-amp-updateHostComponent" class="headerlink" title="updateClassComponent &amp;&amp; updateHostComponent"></a>updateClassComponent &amp;&amp; updateHostComponent</h4><p>上一节中讲到，<code>beginWork</code>中不同tag的元素有不同的update系列方法，我们重点关注的是对ClassComponent和HostComponent两种component的更新方法。ClassComponent对应的是React组件实例，HostComponent对应的是一个视图层节点，在浏览器环境中就等于DOM节点。</p><p>我们先关注<code>updateClassComponent</code>函数：</p><p><img src="http://wx2.sinaimg.cn/large/64c45edcgy1fkq4dx9ljkj20ox0jyn2j.jpg" alt="updateClassComponent"></p><p><code>updateHostComponent</code>这里就不再详细分析了。因此HostComponent没有生命周期钩子需要处理，这个函数主要做的就是调用<code>reconcileChildren</code>对子节点进行diff。</p><h4 id="reconcileChildren"><a href="#reconcileChildren" class="headerlink" title="reconcileChildren"></a>reconcileChildren</h4><p><code>reconcileChildren</code>实现的就是江湖上广为流传的Virtul DOM diff。这年头人人都看过一两个Virtul DOM diff的实现，那React 16的diff是如何实现的呢？</p><p><img src="http://wx4.sinaimg.cn/large/64c45edcgy1fkurhf1dgwj20p60sy44v.jpg" alt="reconcileChildren"></p><p><code>reconcileChildren</code>这个函数里调用了三个功能相似的函数：<code>mountChildFibersInPlace</code>、<code>reconcileChildFibers</code>和<code>reconcileChildFibersInPlace</code>。在源码中我们发现，这三个函数其实是同一个函数，通过传入不同的参数“重载”而来的。</p><pre><code>exports.reconcileChildFibers = ChildReconciler(true, true);exports.reconcileChildFibersInPlace = ChildReconciler(false, true);exports.mountChildFibersInPlace = ChildReconciler(false, false);</code></pre><p>ChildReconciler是一个工厂函数，它接收shouldClone, shouldTrackSideEffects两个参数。reconcileChildFibers函数的目的是产出effect list，所以shouldClone, shouldTrackSideEffects两个参数都是true。mountChildFibersInPlace是组件初始化时用的，所以不用clone fiber来diff，也不用产出effect list。reconcileChildFibersInPlace是在之前reconcile被中断的fiber树上继续工作，因此shouldClone参数为false。</p><p>ChildReconciler内部有很多helper函数，最终返回的函数叫reconcileChildFibers，这个函数实现了对子fiber节点的reconciliation。下面我们关注reconcileChildFibers函数的实现。</p><h4 id="reconcileChildFibers"><a href="#reconcileChildFibers" class="headerlink" title="reconcileChildFibers"></a>reconcileChildFibers</h4><p>图的注释：</p><ul><li>总的，这个函数根据newChild的类型调用不同的方法。newChild可能是一个元素，也可能是一个数组（React16新特性）</li><li>如果是reconcile单个元素，以reconcileSingleElement为例比较key和type，如果相同，复用fiber，删除多余的元素（currentFirstChild的sibling），如果不同，调用createFiberFromElement，返回新创建的。</li><li>如果是string，reconcileSingleTextNode</li><li>如果是array，reconcileChildrenArray</li><li>如果是空，deleteRemainingChildren删除老的子元素</li></ul><p>React的reconcile算法采用的是层次遍历，这种算法是建立在一个节点的插入、删除、移动等操作都是在<strong>节点树的同一层级中进行</strong>这个假设下的。所以reconcile算法的核心就是如何diff两个子节点数组。</p><h4 id="reconcileChildrenArray"><a href="#reconcileChildrenArray" class="headerlink" title="reconcileChildrenArray"></a>reconcileChildrenArray</h4><p>React16的diff算法采用和来自社区的两端同时比较法同样结构的算法。</p><blockquote><p>关于diff算法演化历史可以看司徒正美的<a href="https://segmentfault.com/a/1190000011235844" target="_blank" rel="noopener">这篇博客</a></p></blockquote><p>因为fiber树是单链表结构，没有子节点数组这样的数据结构。也就没有可以供两端同时比较的尾部游标。所以React的这个算法是一个简化的两端比较法，只从头部开始比较。</p><p>下面我们来看一下代码：</p><p>图片</p><p>从头部遍历。第一次遍历新数组，对上了，新老index都++，比较新老数组哪些元素是一样的，（通过updateSlot，比较key），如果是同样的就update。第一次遍历玩了，如果新数组遍历完了，那就可以把老数组中剩余的fiber删除了。</p><p>如果老数组完了新数组还没完，那就把新数组剩下的都插入。</p><p>如果这些情况都不是，就把所有老数组元素按key放map里，然后遍历新数组，插入老数组的元素，这是移动的情况。</p><p>最后再删除没有被上述情况涉及的元素（也就是老数组中有新数组中无的元素，上面的删除只是fast path，特殊情况）</p><h4 id="completeUnitOfWork"><a href="#completeUnitOfWork" class="headerlink" title="completeUnitOfWork"></a>completeUnitOfWork</h4><blockquote><p>注：这里effect list链表插入的想法只是猜测，需要进一步确认。</p></blockquote><p><code>completeUnitOfWork</code>是complete阶段的入口。complete阶段的作用就是在一个节点diff完成之后，对它进行一些收尾工作，主要是更新props和调用生命周期方法等等。<code>completeUnitOfWork</code>主要的逻辑是调用<code>completeWork</code>完成收尾，然后将当前子树的effect list插入到HostRoot的effect list中。具体的让我们来看代码：</p><p><img src="http://wx4.sinaimg.cn/large/64c45edcgy1fks860npkqj20p41foalg.jpg" alt="completeUnitOfWork"></p><h4 id="completeWork"><a href="#completeWork" class="headerlink" title="completeWork"></a>completeWork</h4><p>complete阶段主要工作都是在<code>completeWork</code>中完成的。这个函数很长，需要仔细梳理。</p><p><img src="http://wx3.sinaimg.cn/large/64c45edcgy1fkurhf2r9jj20ou3c9x1u.jpg" alt="completeWork"></p><p>可见completeWork主要是完成reconciliation阶段的扫尾工作，重点是对HostComponent的props进行diff，并标记更新。</p><p>到这里，我们就讲完了reconciliation阶段。这个阶段主要负责产出effect list。所以可以说reconcile的过程相当于是一个纯函数，输入是fiber节点，输出一个effect list。side-effects是在commit阶段被应用到UI中的，这样就将side-effects从reconciliation中隔离开了。因为纯函数的可预测性，让我们可以随时中断reconciliation阶段的执行，而不用担心side-effects给让组件状态和实际UI产生不一致。</p><p>commit这个阶段有点像Git的commit概念。在缓冲区中的代码改动只有在commit之后才会被添加到Git的Object store中。</p><p>下面我们就来关注commit阶段的实现。看看effect list是如何被“提交”到UI中的。</p><h4 id="commitAllWork"><a href="#commitAllWork" class="headerlink" title="commitAllWork"></a>commitAllWork</h4><p>reconciliation阶段结束之后，我们需要将effect list更新到UI中。这就是commit节点的工作。commit阶段的入口是<code>commitAllWork</code>函数，我们来看看它的实现：</p><p><img src="http://wx1.sinaimg.cn/large/64c45edcgy1fkorwi0836j20oz2f71b8.jpg" alt="commitAllWork"></p><p>这里需要注意的是，React 16中的生命周期方法是在reconciliation和commit两个阶段中被调用的，commit阶段的<code>commitAllLifeCycles</code>函数中的生命周期方法包括<code>componentDidMount</code>、<code>componentDidUpdate</code>和<code>componentWillUnmount</code>三个。</p><p><img src="http://wx2.sinaimg.cn/large/64c45edcgy1fknn7fnubmj20eb08wjt7.jpg" alt="lifeCycle"></p><h4 id="reconciliation-commit流程总结"><a href="#reconciliation-commit流程总结" class="headerlink" title="reconciliation+commit流程总结"></a>reconciliation+commit流程总结</h4><p>经过上述对reconciliation和commit两个阶段的源码分析，是不是觉得有些混乱？我总结了一张reconciliation+commit过程中的函数调用图，希望可以帮助你理清这两个阶段的函数调用流程。从图中我们可以看出，<code>workLoop</code>中调用了<code>performUnitOfWork</code>和<code>commitAllWork</code>，分别作为reconciliation和commit两个阶段的入口。<code>performUnitOfWork</code>中又分为begin和complete两个阶段来处理。</p><p><img src="http://wx4.sinaimg.cn/large/64c45edcgy1fkota00kyhj20o402ydg3.jpg" alt="callStack"></p><h3 id="展望-amp-amp-结语"><a href="#展望-amp-amp-结语" class="headerlink" title="展望&amp;&amp;结语"></a>展望&amp;&amp;结语</h3><h4 id="潜伏的大招——异步渲染"><a href="#潜伏的大招——异步渲染" class="headerlink" title="潜伏的大招——异步渲染"></a>潜伏的大招——异步渲染</h4><p>在上文中，我们知道，React 16中默认没有开启异步渲染。用户的setState都是和React 15一样，在一个tick内完成的。fiber可以解决的问题，比如将优先级低的任务分散在多个帧中完成，在每一帧中留足够的时间给响应用户输入和渲染这样优先级高的任务。在默认不开启异步渲染的情况下，是不能做到的。因此我们期待未来版本的React可以开启这个杀手特性。</p><p>我们在阅读源码的过程中，看到了一些没有被文档记录的组件类型，比如CoroutineComponent和YieldComponent。这也许意味着未来React会把渲染的时机掌控权交给用户。我们可以定义一个CoroutineComponent，在reconcile完成后交出控制权给用户。由用户主动调用commit来让组件继续渲染。因为React将组件的渲染分为reconcile和commit两个阶段，reconcile又是没有副作用的，由多个院子操作组成。因此这样的设想是完全可行的。以上只是笔者的推测，丢一个A Clark的链接。</p><h4 id="React-16的设计给前端框架带来的思考"><a href="#React-16的设计给前端框架带来的思考" class="headerlink" title="React 16的设计给前端框架带来的思考"></a>React 16的设计给前端框架带来的思考</h4><p>这次React更新核心架构，让我们看到Facebook的工程师再次用技术推进了用户体验的极限。淘宝FED的口号是用技术为体验提供无限可能，笔者觉得这句话用来形容React也是很合适的。在React上，我们看到了一些借鉴自操作系统中的设计。Fiber可以被比作是一个轻量级线程。有自己的数据，也有优先级的分别。React的作用就是调度fiber，使得优先级高的任务优先执行，同时也保证低优先级的任务会在未来一段时间执行完毕。在diff算法的设计上，React借鉴了社区的经验，这是对社区的一种认可。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文的写作有一部分没有完成，打算针对React 16.3再对本文进行修改，请大家留意&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;React 16在近期发布了。除了将备受争议的BSD+Patents协议改为MIT协议之外，React 16还带来了许多新特性，比如：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;允许在render函数中返回节点数组和字符串。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;render() {
  // 再也不用在外面套一个父节点了
  return [
    // 别忘了加上key
    &amp;lt;li key=&amp;quot;A&amp;quot;&amp;gt;First item&amp;lt;/li&amp;gt;,
    &amp;lt;li key=&amp;quot;B&amp;quot;&amp;gt;Second item&amp;lt;/li&amp;gt;,
    &amp;lt;li key=&amp;quot;C&amp;quot;&amp;gt;Third item&amp;lt;/li&amp;gt;,
  ];
}
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;提供更好的错误处理。&lt;/li&gt;
&lt;li&gt;支持自定义DOM属性。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="React" scheme="http://yoursite.com/categories/React/"/>
    
      <category term="Source code reading" scheme="http://yoursite.com/categories/React/Source-code-reading/"/>
    
    
      <category term="Source code reading" scheme="http://yoursite.com/tags/Source-code-reading/"/>
    
      <category term="React" scheme="http://yoursite.com/tags/React/"/>
    
  </entry>
  
  <entry>
    <title>Table组件中slot内容的跨级传递</title>
    <link href="http://yoursite.com/2017/09/19/table-component-slot-passing/"/>
    <id>http://yoursite.com/2017/09/19/table-component-slot-passing/</id>
    <published>2017-09-19T07:00:09.000Z</published>
    <updated>2018-06-19T11:38:56.591Z</updated>
    
    <content type="html"><![CDATA[<p>在开发MUI的<a href="https://github.com/Muxi-Studio/MUI/tree/dev/src/components/table" target="_blank" rel="noopener">Table组件</a>时，我们遇到了一个问题。用户在顶层组件中嵌套的内容，需要被保存到组件的数据中，并且在表格内部渲染出来。</p><p>通常，Vue内嵌内容是使用slot进行渲染的。在父组件的模板中，在子组件的标签中嵌入模板，然后在子组件的内部，使用<code>&lt;slot/&gt;</code>标签进行渲染。但现在我们的需求是非父子组件的slot渲染，这就要求我们换一种思路去保存和调用slot。</p><p>要理解以下的内容，<strong>请确保你阅读了<a href="https://vuejs.org/v2/guide/render-function.html" target="_blank" rel="noopener">Render Functions &amp; JSX</a>，理解了Vue的VNode、render function、模板等概念以及这些概念之间的关系</strong>。</p><a id="more"></a><h3 id="slot方案"><a href="#slot方案" class="headerlink" title="slot方案"></a>slot方案</h3><p>首先要明确一个点，所谓的slot就指的是一个组件在声明时候的内嵌内容。Vue的模板都会被编译成VNode节点树，slot指的是一个VNode的<code>children</code>属性这个数组里包含的VNode节点集合，这些内容由组件声明时的内嵌内容编译而来。</p><p>我们可以通过<code>this.$slots.default</code>拿到默认的子VNode列表。如果内嵌内容上没有声明<code>name</code>属性，那这些内容都归属于<code>default</code>这个属性。</p><p>所以Slot其实就是一个VNode数组，我们可以把这个数组作为<code>prop</code>传入子节点进行渲染。</p><blockquote><p><code></code>这种语法会把vnode作为一个对象去序列化，这不是我们所期望的。所以我们需要用<code>v-bind</code>去传递VNodes的引用。</p></blockquote><p>想要渲染slot，可以使用<code>render function</code>。之前讲过，slot其实就是VNode的children，所以我们在<code>render function</code>中<code>createElement</code>的时候把slot的引用作为children传入就可以了。</p><pre><code>render(createElement) {        return createElement(&#39;div&#39;, this.content)    }</code></pre><blockquote><p>在组件初始化时给this.$slots赋值，然后在模板中使用slot渲染或许也是一种办法，但不一定行的通，也比较hacky。</p></blockquote><p>但我们发现这样不能达到目的。VNode是Vue中对一个DOM节点的内部表示，VNode是有状态的，一个VNode同时只能渲染出一个DOM节点实例。也就是说一个VNode在渲染之后不能再次渲染，除非先把这个VNode从文档中移除，然后才可以再次渲染。</p><p>所以，因为我们的表格中的VNodes是会被每一个row复用的，现在这种用法只能渲染第一行的slot内容。</p><p>解决方案就是，用一个<code>deepClone</code>函数clone VNode，在每次渲染时初始化新的VNodes实例。</p><pre><code>render(createElement) {        return createElement(&#39;div&#39;, deepClone(this.content, createElement))    }</code></pre><h3 id="scopedSlots方案"><a href="#scopedSlots方案" class="headerlink" title="scopedSlots方案"></a>scopedSlots方案</h3><p>这样似乎就可以解决问题了，但我们发现Table的自定义内容常常是一个按钮这样的可以交互的组件，会有事件绑定，如果我们要在子组件中给slot动态传入属性，这是办不到的。</p><p>所以slot就不能满足我们的需求了，更好的解决方案就是scopedSlots。</p><p>要了解什么是scopedSlots，我们首先将scopedSlots的模板：</p><pre><code>&lt;template scoped=&quot;prop&quot;&gt;  &lt;div&gt;&lt;/div&gt;&lt;/template&gt;</code></pre><p>进行编译，结果是：</p><pre><code>function anonymous() {  with(this){return _c(&#39;div&#39;,{scopedSlots:_u([{key:&quot;default&quot;,fn:function(prop){return [_c(&#39;div&#39;)]}}])})}}</code></pre><p>这种形式是我们之前没有遇到过的，scopedSlots被编译后，生成了一个函数，而且scopedSlot是被存放在VNode的<code>data</code>属性中，而不是在<code>children</code>中。</p><p>仔细观察这个函数，这个函数接收一个参数，然后返回一个VNode，这个VNode的属性是从这个参数中获取的。那scopedSlots的原理就很清楚了，<strong>scopedSlots就是一个lazy evaluation的函数，在需要渲染的时候，接收scope对象，然后渲染。这样就可以达到一个类似动态作用域的效果</strong>。</p><p>既然scopedSlots是一个函数，我们在render function里面只要调用这个函数，并且传入对应的scope对象作为参数就可以了：</p><pre><code>render(createElement) {        const prop = {            index: this.id        }        return createElement(&#39;div&#39;, [            this.content.call(this, prop)        ])    }</code></pre><p>这种形式顺便解决了之前slot无法重复利用VNode的问题，<strong>因为scopedSlots函数每次返回的都是一个新的VNode节点</strong>。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在开发MUI的&lt;a href=&quot;https://github.com/Muxi-Studio/MUI/tree/dev/src/components/table&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Table组件&lt;/a&gt;时，我们遇到了一个问题。用户在顶层组件中嵌套的内容，需要被保存到组件的数据中，并且在表格内部渲染出来。&lt;/p&gt;
&lt;p&gt;通常，Vue内嵌内容是使用slot进行渲染的。在父组件的模板中，在子组件的标签中嵌入模板，然后在子组件的内部，使用&lt;code&gt;&amp;lt;slot/&amp;gt;&lt;/code&gt;标签进行渲染。但现在我们的需求是非父子组件的slot渲染，这就要求我们换一种思路去保存和调用slot。&lt;/p&gt;
&lt;p&gt;要理解以下的内容，&lt;strong&gt;请确保你阅读了&lt;a href=&quot;https://vuejs.org/v2/guide/render-function.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Render Functions &amp;amp; JSX&lt;/a&gt;，理解了Vue的VNode、render function、模板等概念以及这些概念之间的关系&lt;/strong&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="Vue" scheme="http://yoursite.com/categories/Vue/"/>
    
    
      <category term="Vue" scheme="http://yoursite.com/tags/Vue/"/>
    
  </entry>
  
  <entry>
    <title>简单的前端网络层Service封装</title>
    <link href="http://yoursite.com/2017/08/16/fe-model-service/"/>
    <id>http://yoursite.com/2017/08/16/fe-model-service/</id>
    <published>2017-08-16T06:01:31.000Z</published>
    <updated>2018-06-19T11:41:42.382Z</updated>
    
    <content type="html"><![CDATA[<p>我们编写前端组件时，常常需要拉取数据。最原始的办法就是在组件中调用网络库去请求数据。但这样有一些问题：发送请求的一些代码需要<strong>重复的编写</strong>。</p><a id="more"></a><p>比如fetch的<code>json()</code>方法，拿到返回数据之后进行错误处理的代码。一个典型的使用场景是这样的：</p><pre><code>fetch(&#39;/api/v2.0&#39; + this.Url + &#39;/?page=&#39; + this.page_num)  .then(res =&gt; {    return res.json()  })  .then(res =&gt; {    if (res.code === 200) {      this.items = res.blogs      this.pages_count = res.pages_count      this.page_num = res.page      this.blog_num = res.blog_num    }else {      util.message(&quot;Error:&quot;, res.message)    }  })} </code></pre><p>而且在发送请求这个操作中，带有请求的URL等等和组件业务逻辑无关系不大的数据，我们希望可以集中管理这些请求的路由，并且集中处理错误。</p><p>下面就介绍一种最简单的前端网络层封装，我将其称为Service。</p><h3 id="简单的Fetch封装"><a href="#简单的Fetch封装" class="headerlink" title="简单的Fetch封装"></a>简单的Fetch封装</h3><p>为了避免在每次调用fetch时都要设置各种header和参数，我们可以对fetch做一个简单的封装：</p><pre><code>function Fetch(url, opt = {}) {  // 设置请求方法 opt.method = opt.method || &#39;GET&#39;; // 处理要发送的数据 if (opt.data) {    if (/GET/i.test(opt.method)) {      url = `${url}&amp;${obj2query(opt.data)}`;    } else {      opt.headers = {        &#39;Accept&#39;: &#39;application/json&#39;,        &#39;Content-Type&#39;: &#39;application/json&#39;,      };      opt.body = JSON.stringify(opt.data);    }  }  return fetch(url, opt)    .then(response =&gt; {        return response.json();    })}</code></pre><p>使用示例：</p><pre><code>Fetch(&#39;/api/dash/message/list&#39;, {   method: &#39;GET&#39;,   data})</code></pre><p>上面演示的封装只处理了json格式的body和URL参数。在平时的开发中，我们会遇到更复杂的情况，比如需要特殊的header，或者是需要上传文件等等。我们可以通过给<code>Fetch</code>函数传入不同的<code>opt.type</code>来达到差异化的处理。有同学会说，在这个函数里根据不同的type写死业务逻辑，是不是不够解耦？其实不是，<code>Fetch</code>在这里已经是一个业务逻辑的封装了。</p><h3 id="错误处理"><a href="#错误处理" class="headerlink" title="错误处理"></a>错误处理</h3><p>API请求的错误处理是一个很重要的话题，我们希望在服务端返回错误代码时，前端应用可以优雅的提示这个错误，而不是让应用直接报错。换句话说，我们希望catch这个错误，并且进行处理。</p><p>这个错误处理的逻辑如果写在每个<code>Fetch</code>请求返回的Promise后面，无疑是不现实的。因为这样会造成大量代码的重复。所以进行错误处理的最佳场所就是刚才我们封装的<code>Fetch</code>函数了。</p><p>我们在拿到<code>response</code>之后先进行判断，然后再返回结果：</p><pre><code>// Fetch的json返回时，对状态码进行判断，做不同的处理。// 比如在服务端错误时使用alert或者notification组件进行全局提示。return response.json().then((json) =&gt; {   switch (json.code) {    case 200:      return json.result;    case 502:      util.message(json.message, &#39;err&#39;);      throw json.message;  }}）</code></pre><h3 id="Service层封装"><a href="#Service层封装" class="headerlink" title="Service层封装"></a>Service层封装</h3><p><code>Fetch</code>其实还是一个比较底层的封装，Service才是前端组件之间调用的逻辑。这个Service类似传统MVC架构中的Model。提供接口并返回数据。一个Service是一组相关业务逻辑接口的集合。</p><p>比如一个新闻应用，那首页的feed流是一个Service。单篇文章相关的接口可以放到一个Service。用户相关的接口可以放到一个Serivce。</p><pre><code>let service = {  getNews(nid) {        return fetch(`/api/news/`, {            method: &#39;GET&#39;,            data: {                id: nid,            }        });  },  getNewsList() {        return fetch(`/api/news/all/`, {            method: &#39;GET&#39;        });  },  getComment( nid ) {        return fetch(`/api/news/${nid}/comment/`, {            method: &#39;GET&#39;        })    },  sendComment( data ) {      return fetch(`/api/news/${data.nid}/comment/`, {          method: &#39;POST&#39;,          data: data      })  },}</code></pre><p>在组件代码中调用Service（如果应用引入了单独的Model层，那就可以在Model层中调用Service）：</p><pre><code>import NewsService from &#39;../services/news&#39;module.export = {  mounted() {    NewsService.getNews(this.id)      .then( (result) =&gt; {        this.news = result;      })  }}</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我们编写前端组件时，常常需要拉取数据。最原始的办法就是在组件中调用网络库去请求数据。但这样有一些问题：发送请求的一些代码需要&lt;strong&gt;重复的编写&lt;/strong&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="FE" scheme="http://yoursite.com/categories/FE/"/>
    
    
      <category term="FE" scheme="http://yoursite.com/tags/FE/"/>
    
  </entry>
  
  <entry>
    <title>现代前端MVVM组件开发的基本理论</title>
    <link href="http://yoursite.com/2017/08/16/i-have-a-theory/"/>
    <id>http://yoursite.com/2017/08/16/i-have-a-theory/</id>
    <published>2017-08-16T01:54:53.000Z</published>
    <updated>2018-06-19T11:39:55.127Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>此文在写作中。这篇文章意在整理自己目前对MVVM组件开发的理解。在写作过程中我发现，我自以为已经形成了对组件开发的一套理论，但其实这套理论还有很多不完善的地方。最近又翻到了波神分享-<em>漫谈Web前端的『组件化』</em>的<a href="http://leeluolee.github.io/fequan-netease/#/" target="_blank" rel="noopener">PPT</a>，深感要形成理论，还是需要数年的积累才行。所以这篇文章就作为我阶段性的成果，不具有太大的参考价值。</p></blockquote><a id="more"></a><p>如果要给前端开发下一个定义，那就是在浏览器环境下面向对象的GUI客户端程序开发。</p><p>既然是基于面向对象的，那就要有类和对象。在Web前端开发中，最基本的类就是组件的基类（比如Vue和React的构造函数），所有的组件都是基类的一个实例。基类有属性和方法，同时还有一些生命周期钩子函数（其实就是基类上定义的一些方法，会在基类初始化的特定时刻被调用，并且允许实例重载这个方法）。</p><p>既然是GUI客户端程序开发，那Web开发中自然有着MVC之类的分层。MVC、MVP、MVVM都是经典的GUI客户端程序的设计模式<a href="https://github.com/livoras/blog/issues/11" target="_blank" rel="noopener">[1]</a>。目前Web开发中最流行的架构就是MVVM，MVVM已经成为了Web前端开发的一个事实标准。</p><p>谈到事实标准，客户端的开发，比如Android、iOS和Windows，都需要使用系统自带的原生UI组件为基础，配合相关的库，进行开发。</p><p><img src="http://wx2.sinaimg.cn/large/64c45edcgy1fj9wd4ul2mj21kw17n4ch.jpg" alt="iOS丰富的组件"><br><em>iOS丰富的原生组件</em></p><p>在GUI开发领域，<strong>组件</strong>是一种独立的、可复用的交互元素的封装。</p><p>Web开发最大的特点就是，因为历史原因，Web前端当初是作为展示文档的一种渠道来设计的，所以Web前端只有非常少数的几个原生组件，比如表单组件，可以使用。其他的组件都是需要开发者自行封装的。历史上出现过基于jQuery和Backbone等等框架/库的组件。这些组件在当时起到了很重要的作用。在Web标准大幅发展的今天，Web平台（浏览器）有没有提供标准的组件API呢？</p><p>答案是肯定的。目前Web前端组件的官方标准就是<strong>Web Components</strong>。Web Components标准由Custom Elements、HTML Templates、Shadow DOM和HTML Imports四部分组成。Web Components解决了<strong>组件的封装、组合以及复用</strong>问题。</p><p>但在组件的逻辑编写上，随着Web应用越来越复杂，jQuery为代表的命令式编程范式已经不能满足开发的需要。Knockout、ember、angular等MVVM框架出现后，使得声明式的编程范式成为可能。这些框架也渐渐成为了前端开发中的主流。</p><p>Web Components解决了组件的封装和复用问题，但Web Components的逻辑依然是命令式的。而且Web Components目前的浏览器兼容性还不太好，不能直接用于生产。</p><p>当前主流的Web前端框架/库Vue、React和Angular在某种程度上都提供了<strong>组件封装</strong>和<strong>声明式编程范式</strong>两个重要特性。这些框架都有自己的组件封装标准，都遵循数据驱动的范式。值得注意的是Web Components标准在某种程度上对这些框架的组件封装和组合方式有一定的影响，特别是Vuejs，在很多地方参考了Web Components，并实现了Web Components中的slot特性。在未来Web Components标准真正落地时，这些框架都可以和Web Components实现无缝的整合。</p><p>因此我们可以说，实现了声明式编程范式和组件封装、复用和组合的现代MVVM组件框架，就是目前Web前端开发的<strong>事实标准</strong>。并且这个标准在未来很长一段时间内都会持续保持稳定。</p><p>Web前端没有一套官方的原生UI组件（这里指的是官方提供的组件实现，比如日期选择组件、ListView组件等待，不是指Web Components这样的底层标准），以后也不会有，因为Web是一个开放的平台，不像其他的客户端程序的操作系统由一家公司所控制。但目前在我们的开发中，已经可以总结出一套成熟的组件。比如<a href="https://ant.design/docs/react/introduce" target="_blank" rel="noopener">Ant Design</a>中的众多组件。这些组件都是基于MVVM前端框架开发的。</p><p>前端这么多年的发展，到现在已经进入了一个比较成熟的时期了。有了成熟的模块标准和包管理系统，也有了通用的组件模型。所以假设Web也有一个官方的组件标准，那融合了Web Components和声明式编程的MVVM组件已经非常接近了。一个技术想要被称为工业级，只有形成统一的标准。Web前端的组件，虽然不会有统一的标准，但如果在MVVM组件作为事实的标准的前提下去开发，会减少很多不必要的麻烦。</p><p>写这篇博客，也是因为想总结一下目前这个时期的Web开发中的一些事实标准。相比于客户端，前端的门槛其实要更高一些，因为我们要使用堪称刀耕火种的方式去应对日益复杂的业务场景。但在当下，对于刚刚进入前端领域的同学来说，可以把一些范式作为前端的标准来学习，形成对现代前端开发的理解。在现在这个时间点进入前端行业的同学，已经没有必要再了解jQuery时代的开发范式了。</p><p>本文的题目是<em>现代前端MVVM组件开发的基本理论</em>，下面就分别介绍MVVM组件开发中几个关键的理论。</p><h2 id="MVVM"><a href="#MVVM" class="headerlink" title="MVVM"></a>MVVM</h2><p>Web Components并没有规定开发者应该如何去给一个组件的逻辑分层。一个组件里可能包含数据、表现（UI）和业务逻辑。在编写组件时，这些部分都需要被严格的解耦，并且规定各个部分之间的通信方式。</p><p>MVVM下，组件由如下部分构成：</p><blockquote><p>组件 = 视图 + 数据 + 业务逻辑</p></blockquote><p>MVVM分为View、Model和ViewModel三个部分。分别对应视图、数据和业务逻辑三部分。拿Vue举例，Vue的模板和样式属于View层。Vue的组件实例属于ViewModel，Vue的Model层，在没有引入全局Model层的情况下，就是Vue的data属性中的内容。如果开发者引入了全局的Model层，比如Redux或者MobX，那Model就是一个和Vue组件脱离的对象。</p><p>MVVM中，各个部分的关系是这样的：</p><p><img src="http://wx3.sinaimg.cn/large/64c45edcgy1fjfvsl1r13j20es08074q.jpg" alt="mvvm"></p><h2 id="组件的生命周期"><a href="#组件的生命周期" class="headerlink" title="组件的生命周期"></a>组件的生命周期</h2><p>客户端的组件会有生命周期函数，比如iOS的ViewController就有<code>viewDidLoad</code>、<code>viewWillDisappear</code>等等声明周期钩子。前端组件和客户端的的组件一样，都有着生命周期。一个前端组件在应用中，会首先初始化（创建一个新的组件实例），接着加载数据并首次渲染，然后进入一个响应数据变化并重新渲染的循环，最后如果这个组件要从应用中移除，那么组件就会被销毁。</p><p>组件在各个生命周期阶段会调用一些钩子函数，开发者如果想在组件特定的时刻执行一些逻辑，就可以在组件中实现这些钩子函数。</p><p>接下去总结一下MVVM通常都会有的生命周期。<br>首先，组件进入<strong>初始化阶段</strong>，在这个阶段主要就是创建组件实例，并调用init钩子函数。</p><p>然后进入<strong>初始化数据并首次渲染阶段</strong>，这个阶段，如果是Push类型的框架（关于Push和Pull在后文会提到），比如Vue，就需要对数据进行处理（对data进行递归遍历，修改getter和setter，然后调用Render Function进行依赖搜集），然后首次渲染（Vriual DOM patch）。如果是Pull类型的框架，Angular和Regular需要遍历View的AST，然后生成Watcher列表，然后进行首次的脏检查，随后View就被渲染到页面。React就启动一次渲染流程，包括调用Render Function和一次patch。最终达到的效果就是组件首次渲染到页面中。一般在此时也会有一个钩子函数被调用，开发者可以在此时执行一些需要确保UI已经渲染作为前提的逻辑。</p><p>接着进入<strong>响应数据变化并渲染阶段</strong>，这个阶段中，组件已经首次渲染了，接下来如果数据发生变化，那组件就会重新渲染，保持组件的UI和组件的状态保持同步。</p><p>如果组件的销毁方法被调用，组件就进入<strong>销毁阶段</strong>。</p><p>之所以要先说明这几个阶段，是因为理解组件的生命周期对于理解后文讲的数据侦测、渲染、模板等有着密不可分的关系。</p><h2 id="组合"><a href="#组合" class="headerlink" title="组合"></a>组合</h2><p>Composition over inheritence<a href="">[x]</a>。</p><p>在UI的开发中我们常常会复用一些代码。</p><h4 id="内嵌组件"><a href="#内嵌组件" class="headerlink" title="内嵌组件"></a>内嵌组件</h4><h4 id="Mixin"><a href="#Mixin" class="headerlink" title="Mixin"></a>Mixin</h4><h2 id="数据驱动"><a href="#数据驱动" class="headerlink" title="数据驱动"></a>数据驱动</h2><p>这里有必要解释一下这套MVVM中，各模块之间数据的流动。View不能直接通知Model更新，而是通知ViewModel用户的交互，由ViewModel来修改Model中的数据。Model的数据变化之后，会直接触发View的更新。</p><p>此处应该有图。</p><p>最关键的一点就是，<strong>ViewModel不应该直接对View进行操作，而是应该通过修改Model中的数据，让Model驱动View进行更新</strong>。所以我们不提倡在ViewModel中进行DOM操作，因为DOM操作其实就是直接更新View层。并不是DOM操作有多低效。主要是因为既然我们采用了MVVM的范式，就应该去遵守这个范式，才能发挥出这个范式的威力。</p><h2 id="数据变化侦测机制：Pull-vs-Push"><a href="#数据变化侦测机制：Pull-vs-Push" class="headerlink" title="数据变化侦测机制：Pull vs Push"></a>数据变化侦测机制：Pull vs Push</h2><p>这部分的内容我个人认为是非常精辟的。用Pull和Push两种方式准确的分类了现在主流的几个前端框架使用的数据变化侦测机制。</p><blockquote><p>这部分的内容我第一次是听波神和我讲的，后来看了尤雨溪dotJS的演讲，也有类似的内容。</p></blockquote><h4 id="Pull类：脏检查"><a href="#Pull类：脏检查" class="headerlink" title="Pull类：脏检查"></a>Pull类：脏检查</h4><p>Angular和Regular的数据变化侦测机制属于是数据层的脏检查，React则是View层脏检查。</p><h4 id="Push类：依赖搜集"><a href="#Push类：依赖搜集" class="headerlink" title="Push类：依赖搜集"></a>Push类：依赖搜集</h4><p>Vue的Model记录了各个数据和不同部分View的对应关系。在ViewModel修改Model之后，Model会自动通知需要更新的View进行更新。</p><h4 id="Pull和Push的理解"><a href="#Pull和Push的理解" class="headerlink" title="Pull和Push的理解"></a>Pull和Push的理解</h4><p>至于Pull和Push的形象理解，Pull可以理解为从需要更新的整个组件树中<strong>拉取</strong>所有的状态，和旧状态进行比对，然后去更新。Push可以理解为，框架已经知道了变化的数据，然后将更新的信号<strong>推送</strong>给需要更新的组件。</p><p>也不用太纠结Pull和Push的字面意思，总之数据变化侦测机制的区别就在于脏检查机制不知道哪些数据变了，所以需要进行数据的对比。而依赖搜集机制在数据变化那一刻就知道哪些数据变化了，也知道哪些组件依赖这些数据。</p><h3 id="模板技术"><a href="#模板技术" class="headerlink" title="模板技术"></a>模板技术</h3><p>波神对模板技术有一篇很全面的总结<a href="http://leeluolee.github.io/2014/10/10/template-engine/" target="_blank" rel="noopener">[x]</a>。模板的表达能力来看，可以分为基于DSL（包括HTML）的，或者基于JavaScript的。Regular的模板是自己的DSL。Vue和React最终都是使用Render Function来生成View的AST的。也就是说Vue和React可以让开发者直接编写View结构的AST。Vue在Render Function之外提供了基于HTML的模板。这个模板的表达能力和传统的DOM based的模板是一样的。可以说是Render Function能力的一个子集。Vue让开发者自行选择使用何种技术。React的JSX因为是直接写在Render Function中的，所以很难称为是模板，只能说是一种语法糖。React只允许开发者直接编写Render Function。</p><h3 id="渲染"><a href="#渲染" class="headerlink" title="渲染"></a>渲染</h3><p>在Web开发中，UI一般是用DOM或者Canvas这样的底层机制来实现的。组件的View层，实际上就是一个树结构，里面的节点是对View中元素的抽象表示。一个节点可能代表一个DOM节点，也可能代表一个组件的根节点。如果组件的View是由自定义的DSL表示的，那可能还会有其他带有语义的元素，比如if和list等等流程控制节点。</p><p>现在最流行的抽象方式是将View的结构表示为Virtual DOM树。Virtual DOM是对DOM节点的轻量级抽象表示。Render Function中可能会有一些逻辑，Virtual DOM将组件的状态传入Render Function后得到的一个树结构。所以我们就得到了那个著名的等式：</p><p><code>UI = f(state)</code></p><p>这里的UI就可以类比为Virtual DOM，函数f就是Render Function。</p><h3 id="组件通信方式"><a href="#组件通信方式" class="headerlink" title="组件通信方式"></a>组件通信方式</h3><h4 id="父子组件通信"><a href="#父子组件通信" class="headerlink" title="父子组件通信"></a>父子组件通信</h4><p>父组件到子组件 props<br>子组件到父组件，emit event或者父组件传callback到子组件。</p><h4 id="非相邻组件通信"><a href="#非相邻组件通信" class="headerlink" title="非相邻组件通信"></a>非相邻组件通信</h4><p>使用一个event bus。</p><h2 id="数据层解决方案"><a href="#数据层解决方案" class="headerlink" title="数据层解决方案"></a>数据层解决方案</h2><h2 id="路由"><a href="#路由" class="headerlink" title="路由"></a>路由</h2><p>// todo </p><h2 id="提问时间"><a href="#提问时间" class="headerlink" title="提问时间"></a>提问时间</h2><p>在这篇文章中，我主要是讲了现代前端MVVM组件开发中一些重要的话题，并且对于每个话题我都对当前几个主流框架在这些方面的实现做了分类。我认为，这些可以说是目前前端开发的一个事实标准了。有了这方面的系统的知识，我们就可以回答下面的问题。我个人觉得，问一些对比类型的问题，更可以看出一个人是否有思考过自己使用的技术方案。对于市场上各个技术方案的对比和总结，就是得出一个技术体系的方法。</p><p>Q：Web Components解决了什么问题？和现代的前端框架相比有什么不同？</p><p>A:</p><p>Q：如何实现一个基于脏检查的数据绑定方案？</p><p>A：<a href="https://zhuanlan.zhihu.com/p/24990192" target="_blank" rel="noopener">双向绑定的简单实现——基于“脏检测”</a></p><p>Q：如何实现一个Virtul DOM算法？</p><p>A：<a href="https://github.com/livoras/blog/issues/13" target="_blank" rel="noopener">深度剖析：如何实现一个 Virtual DOM 算法</a></p><p>Q：如何实现一个基于依赖搜集的数据绑定方案？</p><p>A： <a href="https://zhuanlan.zhihu.com/p/24475845" target="_blank" rel="noopener">250行实现一个简单的MVVM</a>和<a href="https://zhuanlan.zhihu.com/p/25003235" target="_blank" rel="noopener">数据动态绑定的简单实现——基于ES5对象的getter/setter机制</a></p><p>Q：Vue的模板有哪些局限？如何解决？</p><p>A：HTML模板的表达能力不足，可以手写Render Function<a href="">[x]</a>。</p><p>Q：React-Redux这样的connector具体实现了什么？</p><p>A：订阅store变化，注册回调，回调中调用mapState函数。</p><p>Q：如何实现两个不相邻组件的通信？</p><p>A：</p><p>Q：JSX是模板技术吗？它和传统模板技术的区别是什么？</p><p>A：</p><p>Q：Vue和React的区别在什么地方？</p><p>A：</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;此文在写作中。这篇文章意在整理自己目前对MVVM组件开发的理解。在写作过程中我发现，我自以为已经形成了对组件开发的一套理论，但其实这套理论还有很多不完善的地方。最近又翻到了波神分享-&lt;em&gt;漫谈Web前端的『组件化』&lt;/em&gt;的&lt;a href=&quot;http://leeluolee.github.io/fequan-netease/#/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;PPT&lt;/a&gt;，深感要形成理论，还是需要数年的积累才行。所以这篇文章就作为我阶段性的成果，不具有太大的参考价值。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Mvvm" scheme="http://yoursite.com/categories/Mvvm/"/>
    
    
      <category term="Mvvm" scheme="http://yoursite.com/tags/Mvvm/"/>
    
  </entry>
  
  <entry>
    <title>聊聊UI组件设计-Modal</title>
    <link href="http://yoursite.com/2017/08/14/component-talk-modal/"/>
    <id>http://yoursite.com/2017/08/14/component-talk-modal/</id>
    <published>2017-08-14T11:23:20.000Z</published>
    <updated>2018-06-19T11:36:33.295Z</updated>
    
    <content type="html"><![CDATA[<p>在人机交互中，有一个概念叫做<a href="https://en.wikipedia.org/wiki/Modality_(human–computer_interaction" target="_blank" rel="noopener">Modality</a>)，中文叫模态。模态，顾名思义，就是模拟。计算机可以模拟人通过各种通道接收的信息，比如视觉、听觉、触觉等等通道。视觉就通过显示器输出，听觉通过音响、触觉通过振动。同理，人也可以模拟计算机接收到的电信号，人可以通过键盘、触摸板等待设备来模拟0/1信号。</p><p>模态可以是但通道的，也可以是多通道的（比如玩游戏时有声音、视觉、和振动反馈）。今天我们要将的计算机软件中的Modal组件，就是计算机向人建立的单通道信息交互方式。</p><a id="more"></a><h3 id="Modal在交互设计中的作用"><a href="#Modal在交互设计中的作用" class="headerlink" title="Modal在交互设计中的作用"></a>Modal在交互设计中的作用</h3><p>具体到产品的交互设计中（交互设计和人机交互不是一回事，准确的说两者有交叉），Modal这个组件是很常用的，那么Modal在交互上的意义是什么呢？</p><p>Modal的弹出，其实就是计算机和人之间建立了一个信息传递的通道，这个通道是独占的，在关闭这个通道之前，不能进行其他的交互。计算机程序建立这个通道，为的是传递一些信息。但传递信息有很多的方式，为什么要使用独占通道的Modal来传达呢？</p><p>因为Modal传递的信息，是为了让用户<strong>提供关键信息</strong>，这个信息的回复（Modal的输入）可以是是一个true or false的选择，也可以是较为复杂的数据结构。Modal是一个浮层，所以用户在Modal弹出时，不能再点击应用的其他部分。用户必须要做出决定，是输入信息，或者取消（关闭Modal）。因此，Modal会中断用户当前的工作流。</p><h3 id="实现：Modal组件的特点"><a href="#实现：Modal组件的特点" class="headerlink" title="实现：Modal组件的特点"></a>实现：Modal组件的特点</h3><ul><li>全局一般只能显示一个Modal，因为Modal显示时会需要用户做出决定后再消失（点击取消也是一种决定，即为对操作的否定）。</li><li>Modal一般提供确定和取消两个按钮。</li><li>Modal中标题和底部按钮直接的内容，一般是可以自由组合的。一种特殊情况是Modal中组合了input，那这种Modal类型被称为prompt。</li><li>Modal如果涉及异步的操作，则需要有一个confirm loading状态。这个状态下用户不能再次点击confirm。</li></ul><p>这里推荐一篇很好的总结文章<a href="http://www.ui.cn/detail/224467.html" target="_blank" rel="noopener">覆盖层设计(上)-对话框&amp;浮层</a>来自网易UEDC。系统的总结了交互设计中浮层相关的设计。</p><h3 id="不同UI库的Modal设计"><a href="#不同UI库的Modal设计" class="headerlink" title="不同UI库的Modal设计"></a>不同UI库的Modal设计</h3><p>下面讲讲正题，Modal作为一个Web前端组件的设计方式。</p><p>根据Modal在前端代码中的调用方式，可以分为声明式和命令式两种。Modal的声明式使用是指在前端模板中声明Modal。命令式则是在前端代码中调用一个函数，来显式的调用Modal。</p><h4 id="声明式"><a href="#声明式" class="headerlink" title="声明式"></a>声明式</h4><p>Ant Design中的<a href="https://ant.design/components/modal/" target="_blank" rel="noopener">Modal</a>是典型的声明式组件。Modal被声明在模板中，在父组件初始化之时便存在了。</p><pre><code>render() {    return (      &lt;div&gt;        &lt;Modal          title=&quot;Basic Modal&quot;          visible={this.state.visible}          onOk={this.handleOk}          onCancel={this.handleCancel}        &gt;          &lt;p&gt;Some contents...&lt;/p&gt;          &lt;p&gt;Some contents...&lt;/p&gt;          &lt;p&gt;Some contents...&lt;/p&gt;        &lt;/Modal&gt;      &lt;/div&gt;    );  }</code></pre><p>Modal出现时只是visible这个flag被设置为<code>true</code>。而不是在那个时候初始化Modal组件。要在Modal中组合内嵌内容，只要在模板中的Modal标签中组合内容即可。在用户点击Modal的取消时，只要将visible设为<code>false</code>。所以这里不涉及Modal的销毁问题。Modal的回收是和父组件一起的。</p><h4 id="命令式"><a href="#命令式" class="headerlink" title="命令式"></a>命令式</h4><p>Ant Design中的Modal组件也提供了几个静态的方法，用于在组件中手动初始化一个Modal，并且提供了<code>destroy</code>方法来手动销毁。</p><pre><code> const modal = Modal.success({    title: &#39;This is a notification message&#39;,    content: &#39;This modal will be destroyed after 1 second&#39;, });</code></pre><p>Element UI中的Modal组件的API则是标准的命令式。</p><pre><code>this.$prompt(&#39;请输入邮箱&#39;, &#39;提示&#39;, {          confirmButtonText: &#39;确定&#39;,          cancelButtonText: &#39;取消&#39;,          inputPattern: /[\w!#$%&amp;&#39;*+/=?^_`{|}~-]+(?:\.[\w!#$%&amp;&#39;*+/=?^_`{|}~-]+)*@(?:[\w](?:[\w-]*[\w])?\.)+[\w](?:[\w-]*[\w])?/,          inputErrorMessage: &#39;邮箱格式不正确&#39;        }).then(({ value }) =&gt; {          this.$message({            type: &#39;success&#39;,            message: &#39;你的邮箱是: &#39; + value          });        }).catch(() =&gt; {          this.$message({            type: &#39;info&#39;,            message: &#39;取消输入&#39;          });               });</code></pre><p>Element UI的Modal API设计很有特色，首先调用创建Modal的方法后返回的是一个Promise。如果用户点击确定，那Promise就resolve。如果用户点击取消，那Promise就reject。我们可以在创建Modal的API调用之后链式的编写逻辑。</p><h4 id="自定义属性"><a href="#自定义属性" class="headerlink" title="自定义属性"></a>自定义属性</h4><p>Modal的属性中比较重要的就是内嵌的内容。在声明式定义的Modal的内嵌内容可以声明式的写在模板中。命令式的Modal则需要将内嵌内容作为初始化的属性传入。相比之下声明式的要更自然一些。</p><p>至于Modal另一组重要的属性，确认回调和取消回调。命令式的API在这方面更自然一些。特别是Element UI的基于Promise的调用。声明式的API则是在模板中声明属性，在View Controller中声明方法。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在人机交互中，有一个概念叫做&lt;a href=&quot;https://en.wikipedia.org/wiki/Modality_(human–computer_interaction&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Modality&lt;/a&gt;)，中文叫模态。模态，顾名思义，就是模拟。计算机可以模拟人通过各种通道接收的信息，比如视觉、听觉、触觉等等通道。视觉就通过显示器输出，听觉通过音响、触觉通过振动。同理，人也可以模拟计算机接收到的电信号，人可以通过键盘、触摸板等待设备来模拟0/1信号。&lt;/p&gt;
&lt;p&gt;模态可以是但通道的，也可以是多通道的（比如玩游戏时有声音、视觉、和振动反馈）。今天我们要将的计算机软件中的Modal组件，就是计算机向人建立的单通道信息交互方式。&lt;/p&gt;
    
    </summary>
    
      <category term="Component" scheme="http://yoursite.com/categories/Component/"/>
    
    
      <category term="Component" scheme="http://yoursite.com/tags/Component/"/>
    
  </entry>
  
  <entry>
    <title>Headless Chrome截图服务实战</title>
    <link href="http://yoursite.com/2017/08/03/headlesschrome/"/>
    <id>http://yoursite.com/2017/08/03/headlesschrome/</id>
    <published>2017-08-03T01:26:25.000Z</published>
    <updated>2019-06-05T08:15:18.790Z</updated>
    
    <content type="html"><![CDATA[<h3 id="TL-DR"><a href="#TL-DR" class="headerlink" title="TL;DR"></a>TL;DR</h3><p>给太长不看同学的内容速览：</p><ul><li>Headless是Chrome 59中加入的一种新的运行模式</li><li>Headless Chrome可以替代PhantomJS，并且更加强大</li><li>可以通过Chrome DevTools Protocol这个协议对远程的Chrome浏览器进行调试</li><li>chrome-remote-interface是Nodejs下Chrome DevTools Protocol的封装</li><li>可以使用<code>Emulation.setVisibleSize</code>对<strong>整个页面</strong>进行截屏</li></ul><a id="more"></a><h3 id="PhantomJS的问题"><a href="#PhantomJS的问题" class="headerlink" title="PhantomJS的问题"></a>PhantomJS的问题</h3><p>之前有数报表的导出图片功能是用PhantomJS做的。PhantomJS有两个很大的问题：第一是，它的渲染引擎和JavaScript引擎基于Qt5，版本不是很高，所以渲染的时候会有一些兼容问题，而且JavaScript引擎也相对比较古老（最新的PhantomJS release是2.1版，这个版本基于Qt5.5。Qt5.5使用的Chromium内核版本是40，Chromium现在最新版本是62）。第二，PhantomJS现在已经处于一种维护不多的状态（Github上有1901个open issues）。</p><p>作为一个个人项目，PhantomJS在各种自动化测试以及页面自动化操作中被广泛使用，达到了很高的高度。但因为以上两个缺点，使用PhantomJS将不会是长久之计。</p><h3 id="Chrome的Headless模式"><a href="#Chrome的Headless模式" class="headerlink" title="Chrome的Headless模式"></a>Chrome的Headless模式</h3><p>Headless Chrome其实不是一个全新的工具，而是普通的Chrome浏览器的headless模式。headless就是指Chrome的UI部分是不运行的。</p><p>所以只要你的机器上安装了Chrome 59+，你就可以使用Headless Chrome。相比之前<code>npm install</code>时经常要从bitbucket下载PhantomJS binary的麻烦事，Headless Chrome要方便不少，毕竟Web开发者一般都安装了Chrome。</p><p>你可以在命令行中用headless模式启动Chrome：</p><pre><code>chrome \  --headless \                   # Runs Chrome in headless mode.  --disable-gpu \                # Temporarily needed for now.  --remote-debugging-port=9222 \  https://www.chromestatus.com   # URL to open. Defaults to about:blank.</code></pre><p>我们可以直接在Chrome的CLI中进行一些操作，比如截屏：</p><pre><code>chrome --headless --disable-gpu --screenshot --window-size=1280,1696 https://www.chromestatus.com/</code></pre><p>但一般我们很少会这样直接使用Headless Chrome。对这部分有兴趣的同学可以看<a href="https://developers.google.com/web/updates/2017/04/headless-chrome" target="_blank" rel="noopener">官方文档</a>，这里就不多说了。</p><p>Headless Chrome的最大的优点就是，它就是Chrome，所以可以保持Evergreen，也就是持续的更新。并且我们可以在Headless模式使用Chrome带来的所有的现代Web平台的特性。所以Headless Chrome就成为了PhantomJS的完美升级版替代品。</p><h3 id="强大的Chrome-DevTools-Protocol"><a href="#强大的Chrome-DevTools-Protocol" class="headerlink" title="强大的Chrome DevTools Protocol"></a>强大的Chrome DevTools Protocol</h3><p>要在脚本中和Chrome进行交互，需要用Chrome DevTools Protocol这个协议。所以这里首先介绍一下这个协议。</p><p>简单的说，我们可以在启动Chrome的时候开启一个用于远程调试的端口。然后我们可以在浏览器或者其他客户端中和Chrome建立socket连接，并使用Chrome DevTools Protocol进行通信。</p><p>Chrome DevTools Protocol通信的格式是JSON。比如我们想截屏，就可以发一个消息：</p><pre><code>{    id:30,    method:&quot;Page.captureScreenshot&quot;,    params: {        format:&quot;png&quot;,        quality:100    }}</code></pre><p>返回的消息：</p><pre><code>{    id:30,    data:&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADIA...&quot;    }</code></pre><p>具体的API可以参考<a href="https://chromedevtools.github.io/devtools-protocol/" target="_blank" rel="noopener">文档</a>，我们从文档里可以看到，Chrome DevTools Protocol包含的范围非常广。简单的说，我们平时在Chrome DevTool里面可以做到的事情，能获取的数据，我们使用Chrome DevTools Protocol都可以做到。因为Chrome DevTool其实就是基于这个协议进行开发的一个B/S架构的工具。当然这个协议也应该是随着Chrome DevTool的开发，被标准化。现在不仅仅是Chrome，其他浏览器也支持部分Chrome DevTools Protocol。</p><p>我们在Chrome的拓展里也可以调用这一套API。所以Chrome拓展的潜力是很大的。可以通过合理使用Chrome DevTools Protocol获得更接近自带DevTool的debug体验。也可以对内存、DOM、渲染等数据进行二次的分析和利用。</p><h3 id="Nodejs服务"><a href="#Nodejs服务" class="headerlink" title="Nodejs服务"></a>Nodejs服务</h3><p>直接使用Chrome DevTools Protocol还是比较麻烦的。社区已经有了封装好的Nodejs包<a href="https://github.com/cyrus-and/chrome-remote-interface" target="_blank" rel="noopener">chrome-remote-interface</a>可以直接使用。我们可以直接像调用JavaScript API那样来和Chrome进行通信。</p><p>下面就演示一下如何在Node中进行Chrome的截屏：</p><pre><code>const CDP = require(&#39;chrome-remote-interface&#39;);const Koa = require(&#39;koa&#39;);const app = new Koa();const viewportWidth = 1440;const viewportHeight = 900;const delay = 500;app.use(async ctx =&gt; {  ctx.body = await capture(ctx.request.query.url);});app.listen(3000);const capture = function (url) {  return new Promise((resolve, reject) =&gt; {    CDP.New().then((target) =&gt; {      return CDP({ target });    }).then(async (client) =&gt; {      const { Page } = client;      await Page.enable();      await Page.navigate({ url: url });      Page.loadEventFired(() =&gt; {        setTimeout(async () =&gt; {          const { data } = await Page.captureScreenshot();          resolve(Buffer.from(data, &#39;base64&#39;));          const id = client.target.id;          client.close();          CDP.Close({ id });        }, delay);      });    }).catch((err) =&gt; {      console.error(err);    });  })}</code></pre><p>chrome-remote-interface将一个socket通信的来回封装成了一个异步的函数调用，返回一个Promise。在Node7.8+的环境，我们可以用async/await来轻松的进行流程控制。这里是具体的<a href="https://github.com/cyrus-and/chrome-remote-interface/wiki" target="_blank" rel="noopener">Demo</a>和<a href="https://github.com/cyrus-and/chrome-remote-interface#api" target="_blank" rel="noopener">API文档</a>，基本和Chrome DevTools Protocol里的接口是一一对应的。</p><p>这个服务监听3000端口，在请求中拿到url参数，调用<code>capture</code>函数截屏。我们这里默认Chrome的调试端口是127.0.0.1:9222。如果需要调整，可以在初始化CDP实例的时候传入参数。</p><p>我们调用<code>CDP.New()</code>初始化一个新的Tab，等待Page加载完成，打开url，然后等待页面的加载事件触发之后执行回调，在回调里调用截屏API并且获取数据。最后我们关闭这个Tab。整个截图的流程就是这样。</p><h3 id="Tip：截取整个页面"><a href="#Tip：截取整个页面" class="headerlink" title="Tip：截取整个页面"></a>Tip：截取整个页面</h3><p>我遇到的一个很大的问题就是，页面比较长，我希望截取的图片的高度就是页面的高度，也就是说我希望给整个页面截屏。</p><p>一开始我截屏的结果是这样的：</p><p><img src="https://raw.githubusercontent.com/zxc0328/for-picgo/master/64c45edcgy1fi6ilca424j20ct08y0t3.jpg" alt=""></p><p>这和想象的不一样啊，下面的大半截都没有了 <img src="https://raw.githubusercontent.com/zxc0328/for-picgo/master/64c45edcgy1fi6if8s30uj203d023q2q.jpg" alt="xiaohuangji"></p><p>所以要如何截取整个页面的呢？我在chrome-remote-interface的wiki里看到了一篇<a href="https://medium.com/@dschnr/using-headless-chrome-as-an-automated-screenshot-tool-4b07dffba79a?1" target="_blank" rel="noopener">文章</a>。然而截屏的时候报错，说<code>Emulation.forceViewport</code>不存在.原来这个API已经在新版的Chrome中被废弃了。</p><p>最终我找到了一篇<a href="https://jonathanmh.com/taking-full-page-screenshots-headless-chrome/" target="_blank" rel="noopener">神奇的博客</a>，完美的解决了我的问题，核心代码是这样的：</p><pre><code>Page.loadEventFired(async() =&gt; {    if (fullPage) {      const {root: {nodeId: documentNodeId}} = await DOM.getDocument();      const {nodeId: bodyNodeId} = await DOM.querySelector({        selector: &#39;body&#39;,        nodeId: documentNodeId,      });      const {model: {height}} = await DOM.getBoxModel({nodeId: bodyNodeId});      await Emulation.setVisibleSize({width: device.width, height: height});      await Emulation.setDeviceMetricsOverride({width: device.width, height:height, screenWidth: device.width, screenHeight: height, deviceScaleFactor: 1, fitWindow: false, mobile: false});      await Emulation.setPageScaleFactor({pageScaleFactor:1});    }  });  setTimeout(async function() {    const screenshot = await Page.captureScreenshot({format: &quot;png&quot;, fromSurface: true});    const buffer = new Buffer(screenshot.data, &#39;base64&#39;);    fs.writeFile(&#39;desktop.png&#39;, buffer, &#39;base64&#39;, function(err) {      if (err) {        console.error(err);      } else {        console.log(&#39;Screenshot saved&#39;);      }    });      client.close();  }, screenshotDelay);}).on(&#39;error&#39;, err =&gt; {  console.error(&#39;Cannot connect to browser:&#39;, err);});</code></pre><p>原理就是用Emulation这个API，去调整页面的一些属性。其中核心的方法是<code>Emulation.setVisibleSize</code>，文档里对这个方法的说明是：</p><blockquote><p>Resizes the frame/viewport of the page. Note that this does not affect the frame’s container (e.g. browser window). Can be used to produce screenshots of the specified size.</p></blockquote><p>然后我们就可以愉快的给整个页面截屏了，比如这样：</p><p><img src="https://raw.githubusercontent.com/zxc0328/for-picgo/master/64c45edcgy1fi6i22r84kj21402hdjux.jpg" alt="full image"></p><h3 id="Docker镜像"><a href="#Docker镜像" class="headerlink" title="Docker镜像"></a>Docker镜像</h3><p>最后我们需要部署这两个服务，社区里有自制的Headless Chrome的Docker镜像，比如<a href="https://github.com/yukinying/chrome-headless-browser-docker" target="_blank" rel="noopener">yukinying/chrome-headless-browser-docker</a>。注意如果我们需要调整Chrome的默认window大小，可以修改<a href="https://github.com/yukinying/chrome-headless-browser-docker/blob/master/chrome/Dockerfile" target="_blank" rel="noopener">Dockerfile</a>然后自行build镜像。</p><pre><code>ENTRYPOINT [&quot;/usr/bin/dumb-init&quot;, &quot;--&quot;, \            &quot;/usr/bin/google-chrome-unstable&quot;, \            &quot;--disable-gpu&quot;, \            &quot;--window-size=1440,900&quot;  # 在ENTRYPOINT命令参数中加上window-size            &quot;--headless&quot;, \            &quot;--remote-debugging-address=0.0.0.0&quot;, \            &quot;--remote-debugging-port=9222&quot;, \            &quot;--user-data-dir=/data&quot;]</code></pre><p>Node服务的镜像就比较简单了：</p><pre><code>FROM node:latest# Create app directoryRUN mkdir -p /usr/src/appWORKDIR /usr/src/appCOPY . /usr/src/appRUN npm install koa chrome-remote-interfaceEXPOSE 3000CMD [ &quot;node&quot;, &quot;index.js&quot; ]</code></pre><p>生产中应该用pm2这样的Process Manager来保持进程的运行。依赖也应该写在package.json里。</p><p>服务之间的关系如下：</p><p><img src="https://raw.githubusercontent.com/zxc0328/for-picgo/master/64c45edcgy1fi6j3jqs0cj20fj07saaf.jpg" alt="service"></p><h3 id="Links"><a href="#Links" class="headerlink" title="Links"></a>Links</h3><p>如果大家想使用Headless Chrome的话，最好还是去浏览一下相关的文档，因为这些内容都属于比较新的东西，变化也比较快。本文主要是简单的介绍一下这方面实现的可行性。下面是相关的链接：</p><ul><li><a href="https://developers.google.com/web/updates/2017/04/headless-chrome" target="_blank" rel="noopener">Getting Started with Headless Chrome</a></li><li><a href="https://chromedevtools.github.io/devtools-protocol/" target="_blank" rel="noopener">Chrome DevTools Protocol Viewer</a></li><li><a href="https://github.com/cyrus-and/chrome-remote-interface" target="_blank" rel="noopener">chrome-remote-interface</a></li><li><a href="https://jonathanmh.com/taking-full-page-screenshots-headless-chrome/" target="_blank" rel="noopener">Taking Full Page Screenshots with Headless Chrome</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;TL-DR&quot;&gt;&lt;a href=&quot;#TL-DR&quot; class=&quot;headerlink&quot; title=&quot;TL;DR&quot;&gt;&lt;/a&gt;TL;DR&lt;/h3&gt;&lt;p&gt;给太长不看同学的内容速览：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Headless是Chrome 59中加入的一种新的运行模式&lt;/li&gt;
&lt;li&gt;Headless Chrome可以替代PhantomJS，并且更加强大&lt;/li&gt;
&lt;li&gt;可以通过Chrome DevTools Protocol这个协议对远程的Chrome浏览器进行调试&lt;/li&gt;
&lt;li&gt;chrome-remote-interface是Nodejs下Chrome DevTools Protocol的封装&lt;/li&gt;
&lt;li&gt;可以使用&lt;code&gt;Emulation.setVisibleSize&lt;/code&gt;对&lt;strong&gt;整个页面&lt;/strong&gt;进行截屏&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Headless Chrome" scheme="http://yoursite.com/categories/Headless-Chrome/"/>
    
    
      <category term="Headless Chrome" scheme="http://yoursite.com/tags/Headless-Chrome/"/>
    
  </entry>
  
  <entry>
    <title>前端微服务实践-以木犀通行证为例</title>
    <link href="http://yoursite.com/2017/06/05/fe-microservice/"/>
    <id>http://yoursite.com/2017/06/05/fe-microservice/</id>
    <published>2017-06-05T07:01:58.000Z</published>
    <updated>2018-06-19T11:36:01.129Z</updated>
    
    <content type="html"><![CDATA[<p>在前端，长期以来困扰我们的一个问题就是，如何分发部署我们的前端代码？之前我们的前端代码是放在后端容器中部署的，因此如果需要更新就需要后端工程师去重启容器，更新代码。现在我们采取的方法是，将前端作为一个单独的服务，使用容器来部署。这样前端代码的部署和其他的代码就没有区别了。前端工程师可以自由的控制前端代码的部署，整个部署流程也变的非常标准化。前端微服务让前端部署变成了一件让人享受的事情。下面就以<a href="https://github.com/Muxi-Studio/MuxiAuth-fe" target="_blank" rel="noopener">木犀通行证</a>为例来讲讲具体的实现。</p><a id="more"></a><h3 id="Nodejs服务"><a href="#Nodejs服务" class="headerlink" title="Nodejs服务"></a>Nodejs服务</h3><p>如果只是在容器中放一些前端的静态文件，那不能叫前端微服务。前端微服务是指用Nodejs实现的View层。包括了同步路由以及前端模板。静态文件可以放在前端容器中分发，也可以上传到CDN分发。</p><p>Nodejs实现的这个View层（传统后端MVC中的View），主要负责渲染同步路由的模板。API服务则是由其他的服务提供，大家各司其职。前端接管View层有很多好处，前端渲染以及各种网络应用层的优化，都可以由前端自己来控制。目前Nodejs在大公司中早已频繁被用在服务最前端的那一层中了（后端一般是Java）。</p><p>具体实现来说，我们选用koa2作为Web框架，大致实现是这样的：</p><pre><code>const send = require(&#39;koa-send&#39;);const Koa = require(&#39;koa&#39;);const Router = require(&#39;koa-router&#39;);const userAgent = require(&#39;koa-useragent&#39;);const path = require(&#39;path&#39;)const swig = require(&#39;swig&#39;);const router = new Router();const app = new Koa();const templateRoot = path.join(__dirname, &quot;../dist/template/main&quot;)app.use(userAgent);router.get(&#39;/&#39;, function(ctx, next){    if (!ctx.userAgent.isMobile) {        let template = swig.compileFile(path.resolve(templateRoot, &quot;auth.html&quot;));        ctx.body = template({})    } else {        let template = swig.compileFile(path.resolve(templateRoot, &quot;auth_phone.html&quot;));        ctx.body = template({})    }});router.get(/^\/static(?:\/|$)/, async (ctx) =&gt; {     await send(ctx, ctx.path, {         root: path.join(__dirname, &quot;../dist&quot;)     });})app    .use(router.routes())    .use(router.allowedMethods());app.listen(3000);console.log(&#39;listening on port 3000&#39;);</code></pre><p>使用<code>koa-router</code>来写同步路由，路由中返回对应模板（这里的路由还对移动端或者桌面端流量进行了区分）。然后这里还对静态文件路由做了处理，如果用的是CDN分发静态文件，这里就不需要处理了。</p><h3 id="Dockerfile"><a href="#Dockerfile" class="headerlink" title="Dockerfile"></a>Dockerfile</h3><p><code>Dockerfile</code>是Docker的配置文件。我们要把这个Nodejs服务build成Docker镜像进行部署，因此要写一个<code>Dockerfile</code>，大致是这样的：</p><pre><code>FROM node:latest# Create app directoryRUN mkdir -p /usr/src/appWORKDIR /usr/src/appCOPY . /usr/src/app# Build static fileRUN npm installRUN npm run buildWORKDIR /usr/src/app/server# Bundle app sourceEXPOSE 3000CMD [ &quot;npm&quot;, &quot;start&quot; ]</code></pre><p>Dockerfile具体的写法可以看官方文档和这篇博客。木犀通行证中的Dockerfile主要是，构建了静态文件，然后启动了node服务进程。这样只要在任何有Docker的环境下<code>docker run</code>这个镜像，就可以运行这个服务了。</p><h3 id="在阿里云镜像仓库Build镜像"><a href="#在阿里云镜像仓库Build镜像" class="headerlink" title="在阿里云镜像仓库Build镜像"></a>在阿里云镜像仓库Build镜像</h3><p>镜像仓库类似是Github，是分发镜像的一个工具。阿里云的镜像仓库可以从Github仓库进行镜像构建，然后我们就可以拿到一个仓库的URL。在构建时我们可以指定版本号（和Git里的tag对应）。这样在部署时就可以部署这个镜像的某个版本。</p><p>阿里云的镜像仓库的用法这里就不详细说了。可以自己去阿里云上尝试一下。</p><h3 id="在Kubernetes部署"><a href="#在Kubernetes部署" class="headerlink" title="在Kubernetes部署"></a>在Kubernetes部署</h3><p><em>MAE发布后本节需要更新</em></p><p>目前这一步是由专门的系统管理员来负责的。开发者只要将镜像仓库的地址和tag告诉系统管理员就可以了。管理员会在集群上更新并部署新版本。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>这个新的部署流程和之前相比，不同的地方主要是现在需要写Nodejs，一个View层服务。需要写Dockerfile。最终交付部署的是Docker镜像。希望大家在实践的过程中能有自己的思考。同时也对云计算时代的软件分发和部署有更多的了解。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在前端，长期以来困扰我们的一个问题就是，如何分发部署我们的前端代码？之前我们的前端代码是放在后端容器中部署的，因此如果需要更新就需要后端工程师去重启容器，更新代码。现在我们采取的方法是，将前端作为一个单独的服务，使用容器来部署。这样前端代码的部署和其他的代码就没有区别了。前端工程师可以自由的控制前端代码的部署，整个部署流程也变的非常标准化。前端微服务让前端部署变成了一件让人享受的事情。下面就以&lt;a href=&quot;https://github.com/Muxi-Studio/MuxiAuth-fe&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;木犀通行证&lt;/a&gt;为例来讲讲具体的实现。&lt;/p&gt;
    
    </summary>
    
      <category term="Mircoservices" scheme="http://yoursite.com/categories/Mircoservices/"/>
    
    
      <category term="Mircoservices" scheme="http://yoursite.com/tags/Mircoservices/"/>
    
  </entry>
  
  <entry>
    <title>木犀后端开发工作流（2017年6月版）</title>
    <link href="http://yoursite.com/2017/06/05/muxi-be-workflow/"/>
    <id>http://yoursite.com/2017/06/05/muxi-be-workflow/</id>
    <published>2017-06-05T07:01:43.000Z</published>
    <updated>2018-06-19T11:43:18.883Z</updated>
    
    <content type="html"><![CDATA[<p>木犀后端的技术经历了一个不断演进的过程，从最初的LNMP式的简单直接的部署方式，到Docker容器部署，到现在的Kubernetes集群部署。相比当年艰难的调试由于操作系统环境不同而导致的各种部署问题，现在的部署流程可以说是相当简单而且可靠的。这也要求我们有一套标准化的开发部署流程。</p><a id="more"></a><h3 id="三个环境"><a href="#三个环境" class="headerlink" title="三个环境"></a>三个环境</h3><p>在互联网公司，一个产品一般都部署了好几个版本，比如开发、测试、预发布等等。不同的版本对应开发周期不同时间点的产品状态。QA一般就是在部署好的这些环境中进行发布前测试的。</p><p>我们对环境的要求没有那么高，只要有本地、测试、线上三个环境就可以了。下面分别讲讲从本地开发，到部署测试版本，到新版本上线过程中一系列的标准流程。</p><h3 id="开发分支与Pull-Request"><a href="#开发分支与Pull-Request" class="headerlink" title="开发分支与Pull Request"></a>开发分支与Pull Request</h3><p>本地开发其实没有太多的限制，一般就直接在本地运行代码进行开发。需要注意的是我们的仓库中一般有主分支和开发分支，这个开发分支可以是按版本号，每次新版本时从主分支checkout出来。或者是一个持续使用的开发分支。所有人都<strong>不能直接</strong>向主分支提交代码。但可以直接向开发分支提交代码。</p><p>如果想讲开发分支中的代码合并到主分支，就要发起一个<strong>Pull Request</strong>。Pull Request在负责人code review（看情况）以及<strong>CI测试通过</strong>之后才能merge。</p><h3 id="单元测试与CI"><a href="#单元测试与CI" class="headerlink" title="单元测试与CI"></a>单元测试与CI</h3><p>对于有明确输入输出的后端API来说，单元测试是必要的软件质量保障。也是协助开发的一个手段。</p><p>大家在本地提交代码之前先自己跑过测试，通过之后再提交。Pull Request时还会跑一遍CI，来确保代码功能的正确。比如这样：</p><p><img src="http://wx2.sinaimg.cn/large/64c45edcly1fgqdkad2a4j213u02mq37.jpg" alt="CI pass"></p><p>Github上使用的比较多的是Travis CI。Docker based的项目可以参考<a href="https://github.com/Muxi-X/muxi_site/blob/dev-branch/.travis.yml" target="_blank" rel="noopener">这个CI配置</a>。</p><blockquote><p>这里简单介绍一下CI。CI指持续集成，我们说的CI一般是指云端的CI runner。Travis CI本质上其实就是一个云服务。提供了一个虚拟环境来运行你指定的脚本。Travis支持很多语言环境，但最近Travis支持了Docker，所以环境也就不是问题了。要注意我们写测试时要写清进程exit时的状态码，非0的状态码代表非正常退出。Travis就是根据这个来判断测试或者其他错误是否发生的。这决定了这次CI运行是否成功。</p></blockquote><h3 id="测试环境的具体配置"><a href="#测试环境的具体配置" class="headerlink" title="测试环境的具体配置"></a>测试环境的具体配置</h3><p>在开发基本完成，代码merge到主分支之后，我们就可以尝试部署一个测试版本了。测试环境和线上的环境差别不大。测试环境部署在我们的测试集群（几台专有网络阿里云学生机）。</p><p>大家可以随意选一台机器然后部署。部署的时候，除了数据库之外的一般都用Docker部署。如果有多个容器需要部署，我们一般用Docker-compose来一键build&amp;run。</p><p>数据库一般就使用某台机器上直接安装的数据库。在初次部署时大家要记得在容器中执行初始化数据库和用户角色命令（当然也可以写成脚本）。</p><p>测试环境有几个用处，首先是给前端提供联调的API。然后是给产品经理和设计师提供一个线上版本来进行初步测试。因此我们需要给测试环境配置一个域名。但如果直接在DnsPod等等线上解析，要解析的域名数量会很大，很不方便。所以我们采用修改本地hosts文件的办法。</p><p>比如：</p><pre><code>120.77.8.149 test.share.muxixyz.com</code></pre><p><strong>注意域名是<code>muxixyz.com</code>的子域名</strong>，因为阿里云会检测DNS解析的域名是否备案。如果随便写一个域名是不行的（当然理论上你用baidu.com或者其他知名的域名也行）。</p><p>这个hosts文件的配置要写在Github文档中。Tower项目里最好也写一个操作指南文档给设计师和PM看。</p><h3 id="上线"><a href="#上线" class="headerlink" title="上线"></a>上线</h3><p><em>MAE发布后本节需要更新</em></p><p>上线一个版本，比如1.2版。首先在主分支打上这个tag。然后在阿里云的镜像仓库里build这个tag的镜像。</p><p>最后由负责部署的同学，SSH到集群更新Deployment配置文件中的image版本号。升级Deployment即可。</p><p>这一步在将来会由开发应用的同学自行在MAE上操作完成。目前暂时还是需要有人在服务器上部署。开发的同学只要交付镜像就可以了。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>相比于之前的工作流，目前的工作流不同的地方主要是，需要写单元测试，PR需要CI通过才能merge，大型的应用必须要部署测试环境并用本地DNS解析访问，进行部署时是用Docker镜像而不是在服务器build。这套工作流需要大家用<strong>微服务</strong>的观点去看待将来的开发。随着时间的推移，这套工作流也会不断的变化，大致还是朝Cloud Native的方向发展。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;木犀后端的技术经历了一个不断演进的过程，从最初的LNMP式的简单直接的部署方式，到Docker容器部署，到现在的Kubernetes集群部署。相比当年艰难的调试由于操作系统环境不同而导致的各种部署问题，现在的部署流程可以说是相当简单而且可靠的。这也要求我们有一套标准化的开发部署流程。&lt;/p&gt;
    
    </summary>
    
      <category term="Muxistudio" scheme="http://yoursite.com/categories/Muxistudio/"/>
    
      <category term="Workflow" scheme="http://yoursite.com/categories/Muxistudio/Workflow/"/>
    
    
      <category term="Muxistudio" scheme="http://yoursite.com/tags/Muxistudio/"/>
    
      <category term="Workflow" scheme="http://yoursite.com/tags/Workflow/"/>
    
  </entry>
  
  <entry>
    <title>云端木犀-MAE初步构想</title>
    <link href="http://yoursite.com/2017/05/27/mae/"/>
    <id>http://yoursite.com/2017/05/27/mae/</id>
    <published>2017-05-27T13:29:43.000Z</published>
    <updated>2019-06-05T08:09:08.934Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>2019/6/5 今年我们有同学拿到了腾讯云和 Azure 的实习，所以未来在云计算上我们会有更多的领路人，来带领团队在这个方向进行探索。MAE 虽然还没做出来，但 k3s 的发现让我们在服务器上实现了自由，相信很快就会设计出下一代的 MAE 架构。</p></blockquote><p>Muxi App Engine，简称MAE，是木犀的私有PaaS方案，也是木犀云的重要组成部分。MAE主要基于Docker和Kubernetes，为木犀所有应用的构建、部署、监控和扩容提供了一个统一的入口，让我们能专注于服务本身的开发。同时MAE也为木犀提供了一套标准化的运维流程，使得团队开发中的工程化程度进一步提高。</p><p>说的这么厉害，那如果你是一个技术小白，我应该如何来解释MAE呢？</p><a id="more"></a><h3 id="TD-LR"><a href="#TD-LR" class="headerlink" title="TD;LR"></a>TD;LR</h3><p>比如我们有一个应用，华师匣子。华师匣子是由很多的服务构成的，比如成绩服务，课表服务，图书馆服务等等。每个服务都实现了对应的接口。我们使用Docker来运行这些服务。Docker是一种容器技术。我们可以简单的理解为一种沙盒环境。这些容器的存在，已经很大程度上方便了我们的部署。因为容器可以实现系统资源的隔离，使得服务器上可以同时运行很多不同的服务，而相互不打扰。</p><p>但手动部署容器，还是太复杂了。我们要登录服务器手动部署容器。容器如果出现问题，我们也需要亲自去重启。如果我们需要横向拓展，部署多个相同的容器以应对高负载，也需要一个个去手动部署。这个时候就需要一个调度者来帮我们自动完成这个任务。</p><p>我们可以把MAE理解为容器的调度者。我们在MAE中新建一个应用和下属的服务，填写相关的信息。比如我们只要提供Docker镜像的地址，就可以一键部署。MAE会帮我们将容器部署到合适的服务器上。如果容器因为某些原因崩溃了，MAE会自动重启容器。如果我们需要横向拓展，那只要在控制台里填写一下需要拓展的数量就可以了。如果需要更新代码，我们只需要提供镜像的新版本号，MAE会自动终止旧版本的容器，新建新版本的容器。一切都是这么简单。可以自动化的事情，我们都会做到自动化。</p><p>MAE提供Web UI和CLI。Web UI主要用于日常的使用以及查看监控数据。CLI适合在shell脚本等自动化环境下使用。</p><p>MAE带来的最大变革是，今后我们的应用从一开始就应该按Cloud Native的思路去编写。要拥抱云计算，我们必须编写Cloud Native的应用，具体的说，使用微服务架构，写无状态的功能单元，容器技术，将数据库等等持久化的组件作为单独的部分等等，都是Cloud Native的体现。只有这样，我们的应用才能和目前公有云和私有云的基础设施完美结合。</p><p>下面就是纯粹的技术讨论了，请耐心阅读。</p><h3 id="MAE的技术选型"><a href="#MAE的技术选型" class="headerlink" title="MAE的技术选型"></a>MAE的技术选型</h3><p>简单的说，就是Docker和Kubernetes。Docker是容器技术的实现，Kubernetes主要提供了容器编排管理的功能。上一节中说到的大部分自动化功能，都是Kubernetes实现的。MAE中需要我们研发的主要是MAE API服务、Web UI还有CLI程序。除了这些，还有就是在MAE中实现一套最适合我们的<strong>对应用的抽象</strong>。这套抽象是非常重要的。Kubernetes的概念并不是所有人都可以理解的，也没有必要对使用者暴露最底层的概念。PaaS的用户是从是应用和服务这些逻辑上的概念去看待问题的。所以MAE就提供了针对应用和服务的抽象，并且和Kubernetes整合起来。</p><h3 id="MAE的组成部分"><a href="#MAE的组成部分" class="headerlink" title="MAE的组成部分"></a>MAE的组成部分</h3><p><img src="https://raw.githubusercontent.com/zxc0328/for-picgo/master/64c45edcly1fg0dnygziij212a0ue4e7.jpg" alt="mae parts"></p><p>MAE的组成，从上到下，大致有三层：</p><ul><li><strong>MAE服务层</strong> MAE服务层是暴露给用户的一些服务。MAE API Server是MAE是中枢。负责和底层的集群通信，保存应用配置等等。MAE Web UI提供了一个Web界面，用户可以通过Web UI对MAE发出指令，查看监控数据。MAE CLI是一个命令行程序，提供了从命令行和API Server通信的渠道。</li><li><strong>逻辑应用层</strong> 这一层是抽象的应用层。也就是我们概念上的应用。因为实际的集群中是没有应用概念的（当然Kubernetes的Services+Namespace已经非常接近了），所以我们需要在这里提供对应的抽象。我们可以在MAE中新建应用，然后配置这个应用对应的服务。MAE中的服务（以后简称MAE服务，区别于Kubernetes Service），其实就对应一个微服务。一个应用由至少一个微服务构成。MAE服务是用户可以控制的部署的最小单元。我们可以对某个MAE服务单独进行拓展。比较特殊的MAE服务就是Nginx入口服务，这个服务为所有应用提供反向代理，同时也作为一个MAE下的服务，被MAE部署。</li><li><strong>Kubernetes层</strong> Kubernetes这层就是底层的实现层了。包括了Service，Deployment和Pods。其中Service和Deployment在上层共同支撑了MAE服务。Pods则属于最底层的调度单元。在MAE层是完全不可见的。一个Pod由至少一个容器构成。</li></ul><h3 id="MAE的流量分发"><a href="#MAE的流量分发" class="headerlink" title="MAE的流量分发"></a>MAE的流量分发</h3><p>那么作为一个分布式系统，一个用户的请求究竟是经过怎样的路径，到达最底层的Kubernetes Pod的呢？</p><p><img src="https://raw.githubusercontent.com/zxc0328/for-picgo/master/64c45edcly1fg0etf4yg9j20ku0r4k1f.jpg" alt="mae request"></p><p>首先DNS把域名解析到Kubernetes的Master节点的公网IP上，然后部署在Master节点上的Nginx入口服务接管，Nginx根据MAE应用设置的域名和URL规则，将这个请求转发到对应应用的某个服务上。Kubernetes的服务都是可以通过<code>&lt;Master内网IP&gt;:&lt;Service Port&gt;</code>来进行访问的。然后Kubernetes proxy用iptables规则，将请求转发到某个节点上的Pod。</p><p>由于Kubernetes proxy提供了均衡负载，我们不用再操心如何分配流量到服务下属的多个Pod中的某一个这样的问题。今后可以做的优化是，实现Kubernetes Master节点的高可用，也就是同时部署多个Master节点。这样的话就需要在Master节点之上再实现一个均衡负载。</p><h3 id="MAE的实现细节"><a href="#MAE的实现细节" class="headerlink" title="MAE的实现细节"></a>MAE的实现细节</h3><p>MAE做的抽象，一个是应用，应用之下是服务。对于这两个抽象，应该各自保存一些什么样的数据，这属于MAE的实现细节。</p><p>每个应用需要的信息有，应用名，域名，Nginx转发规则，应用下属的服务列表。</p><p>每个服务需要的信息有：服务名，当前镜像版本，镜像仓库地址，Github仓库地址，Kubernetes Service和Deployment需要的全部信息，当前服务属于哪个应用，授权管理当前服务的用户列表。</p><p>因为服务是部署的最小单元，因此相对来说服务是MAE中比较核心的一个部分。MAE需要将数据库中保存的服务信息，自动转化为Kubernetes需要的<code>.yaml</code>文件。将数据库中保存的应用信息，自动转化为nginx的配置文件。这是实现上需要去考虑的一个问题。</p><p>另外，现在还需要仔细考虑的一点，<strong>MAE在全局/应用/服务这几个层面分别需要哪些监控数据</strong>。</p><h3 id="MAE时代的部署工作流"><a href="#MAE时代的部署工作流" class="headerlink" title="MAE时代的部署工作流"></a>MAE时代的部署工作流</h3><p>部署服务之前，首先我们要构建镜像（构建之前可以引入CI，测试通过才可以构建镜像）。给镜像打上版本号，然后发布到云端的镜像仓库（可以用阿里云/蜂巢/Daocloud）。之后我们就可以在MAE中为某个服务新建一次部署了，填上新的版本号，点击部署，就启动了一次部署了。得益于Kubernetes超强的部署能力，我们可以回滚、暂停、继续每一次部署。</p><p><img src="https://raw.githubusercontent.com/zxc0328/for-picgo/master/64c45edcly1fg0era6a66j214q0mmwoc.jpg" alt="mae deployment"></p><p>MAE的API Server把服务目前的配置转换为<code>.yaml</code>格式，向Kubernetes API Server发送请求。然后Kubernetes会进行相应的处理。和Service相关的就调整Service，和Deployment相关的就调整Deployment。最终服务更新到目标状态，部署完成。</p><h3 id="MAE的物理节点组成"><a href="#MAE的物理节点组成" class="headerlink" title="MAE的物理节点组成"></a>MAE的物理节点组成</h3><p>MAE的逻辑组成已经介绍了，那MAE和具体的云主机之间是什么关系呢。请看下图：</p><p><img src="https://raw.githubusercontent.com/zxc0328/for-picgo/master/64c45edcly1fg0ftnkesej213a0o07g6.jpg" alt="mae nodes"></p><p>图中的一个框对应一台云主机。其中Master节点目前只打算部署在一台机器上。今后会做高可用（具体要看kubeadm的支持情况，自己部署HA也是可以的，参见<a href="http://tonybai.com/2017/05/15/setup-a-ha-kubernetes-cluster-based-on-kubeadm-part1/" target="_blank" rel="noopener">这篇博客</a>）。Kubernetes Node是负载Pod调度的机器，也就是分布式系统主从节点中的Slave节点。Kubernetes的Pod可能会被调度到其中任意一台机器上。因此应用在物理上运行在哪个节点，在MAE中并没有太多的意义。</p><p>MAE的Server在理论上会单独部署在一台服务器上，MAE Server如果出现问题，其实并不影响当前集群的正常运转。Kubernetes的Master节点才是真正负责调度和管理的，因此才会有做高可用的打算。</p><h3 id="MAE时代的前端服务化"><a href="#MAE时代的前端服务化" class="headerlink" title="MAE时代的前端服务化"></a>MAE时代的前端服务化</h3><p>MAE要求一个应用由几个服务构成。这给了我们一个机会，去改进目前的前端代码部署流程。目前的前端代码是放在后端容器中部署的。每次部署需要后端工程师参与，或者使用配置复杂的Webhook。前端代码部署时需要重启后端容器，因此无法实现无副作用的前端部署。MAE架构下，我们将前端作为一个单独的服务。这个服务主要接受的是从Nginx入口服务转发而来的需要返回HTML的请求，也就是我们一般所说的View层，或者说同步路由层。技术上我们选用Nodejs来实现前端的服务。</p><p>所以今后前端工程师的产出就是前端代码以及Nodejs服务端代码（主要是路由）。两者在同一个仓库中，部署在同一个容器中。</p><p>这样的好处是，前端代码部署时只需要构建前端服务的镜像，然后在MAE单独部署就可以了。和后端完全解耦。前端工程师也可以借助MAE提供的强大的运维能力，来优化自己的工作流。</p><p>前端工程师接管View层，给我们的应用带来了更大的可能性。服务端渲染前端组件变成了非常自然的选择。前端工程师控制的范围扩大，提供了更多发挥的空间。比如前端工程师可以对静态资源缓存，CSRF等等进行更好的控制。</p><h3 id="MAE的主要API以及CLI工具命令"><a href="#MAE的主要API以及CLI工具命令" class="headerlink" title="MAE的主要API以及CLI工具命令"></a>MAE的主要API以及CLI工具命令</h3><p>API在Web UI框架确定之后就可以比较清楚的写成文档了，这里只列一下主要的API。</p><h4 id="应用层API"><a href="#应用层API" class="headerlink" title="应用层API"></a>应用层API</h4><ul><li>应用列表/信息</li><li>应用网络配置更新</li><li>监控信息</li></ul><h4 id="服务层API"><a href="#服务层API" class="headerlink" title="服务层API"></a>服务层API</h4><ul><li>服务列表/信息</li><li>部署服务新版本</li><li>横向拓展服务</li><li>回滚、暂停部署</li><li>监控信息</li></ul><h4 id="CLI"><a href="#CLI" class="headerlink" title="CLI"></a>CLI</h4><p>CLI提供了和主要API对应的命令。命令需要验证的话，可以通过<code>mae login</code>这样的命令来进行。</p><h3 id="木犀云的其他产品展望"><a href="#木犀云的其他产品展望" class="headerlink" title="木犀云的其他产品展望"></a>木犀云的其他产品展望</h3><h4 id="Muxi-Database-Service-MDS"><a href="#Muxi-Database-Service-MDS" class="headerlink" title="Muxi Database Service(MDS)"></a>Muxi Database Service(MDS)</h4><p>提供Mongo，Redis等云数据库服务。实现了数据自动备份，多节点高可用等特性。</p><h4 id="Muxi-Storage-Service-MSS"><a href="#Muxi-Storage-Service-MSS" class="headerlink" title="Muxi Storage Service(MSS)"></a>Muxi Storage Service(MSS)</h4><p>基于Ceph的分布式对象存储。负责大文件的存储。比如图片、文档等。</p><h4 id="鹊桥"><a href="#鹊桥" class="headerlink" title="鹊桥"></a>鹊桥</h4><p>木犀接口管理平台。提供了接口的云端管理和Mock服务。</p><h4 id="Muxi-UI-MUI"><a href="#Muxi-UI-MUI" class="headerlink" title="Muxi UI(MUI)"></a>Muxi UI(MUI)</h4><p>基于Vuejs的UI组件库。适用于中后台前端应用的快速开发。</p><h3 id="写在最后-Why-Cloud"><a href="#写在最后-Why-Cloud" class="headerlink" title="写在最后: Why Cloud?"></a>写在最后: Why Cloud?</h3><p>为什么木犀要拥抱云计算？为什么我们要自建私有PaaS平台？</p><p>首先，在当下，计算能力，已经和水电煤一样，成为了一种基础设施。作为小团队，使用现成的基础设施，从成本上以及灵活性上都是最佳的。</p><p>虽然我们使用了IaaS服务，但我们还是可以把云主机当做物理主机来使用，我们完全可以实施云计算出现之前时代的传统运维。运维工程师负责服务器的环境，开发工程师把代码交给运维工程师部署。数据库等服务和业务逻辑部署在同一台机器上，等等等等。很明显，坚持这种做法，将云计算理解为虚拟主机，是非常不明智的。</p><p>既然已经用上了IaaS，那就要利用现有的微服务理论和Docker等等容器技术，打造更加原生的云端体验。我们将代码拆成一个一个单元，将有状态和无状态的服务分离。部署时容器让我们不用在意服务端的环境隔离。Kubernetes让我们不用手动管理容器的生命周期。</p><p>开发MAE是为了解决目前团队部署流程中存在的问题。自建的PaaS平台可以最大程度提供个性化的使用体验。也给我们机会去对Kubernetes等开源技术进行探索和研究，并且用到生产环境之中。</p><p>围绕木犀云而进行的一系列的研究，是木犀拥抱云计算的最好方式。我们不仅要享受云计算的好处，同时也要参与其中，深入的理解技术细节。相信未来我们在云计算上的发展会有无限的可能。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;2019/6/5 今年我们有同学拿到了腾讯云和 Azure 的实习，所以未来在云计算上我们会有更多的领路人，来带领团队在这个方向进行探索。MAE 虽然还没做出来，但 k3s 的发现让我们在服务器上实现了自由，相信很快就会设计出下一代的 MAE 架构。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Muxi App Engine，简称MAE，是木犀的私有PaaS方案，也是木犀云的重要组成部分。MAE主要基于Docker和Kubernetes，为木犀所有应用的构建、部署、监控和扩容提供了一个统一的入口，让我们能专注于服务本身的开发。同时MAE也为木犀提供了一套标准化的运维流程，使得团队开发中的工程化程度进一步提高。&lt;/p&gt;
&lt;p&gt;说的这么厉害，那如果你是一个技术小白，我应该如何来解释MAE呢？&lt;/p&gt;
    
    </summary>
    
      <category term="Cloud" scheme="http://yoursite.com/categories/Cloud/"/>
    
      <category term="Muxistudio" scheme="http://yoursite.com/categories/Cloud/Muxistudio/"/>
    
    
      <category term="Cloud" scheme="http://yoursite.com/tags/Cloud/"/>
    
      <category term="Muxistudio" scheme="http://yoursite.com/tags/Muxistudio/"/>
    
  </entry>
  
  <entry>
    <title>使用Kubeadm 1.6部署Kubernetes</title>
    <link href="http://yoursite.com/2017/05/24/k8s-setup/"/>
    <id>http://yoursite.com/2017/05/24/k8s-setup/</id>
    <published>2017-05-24T11:52:27.000Z</published>
    <updated>2018-06-19T11:28:53.554Z</updated>
    
    <content type="html"><![CDATA[<p>本文介绍了如何用Kubeadm 1.6版在Ubuntu 16.04系统上快速部署一个Kubernetes集群。</p><a id="more"></a><h3 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h3><p>阿里云ECS 华南1 可用区A Ubuntu 16.04.2 LTS (GNU/Linux 4.4.0-63-generic x86_64) 专有网络</p><table><thead><tr><th>节点类型</th><th style="text-align:center">配置</th><th style="text-align:right">内网IP</th></tr></thead><tbody><tr><td>MASTER</td><td style="text-align:center">1 CPU 1GB RAM</td><td style="text-align:right">172.18.214.46</td></tr><tr><td>Node</td><td style="text-align:center">1 CPU 2GB RAM</td><td style="text-align:right">172.18.214.47</td></tr></tbody></table><h3 id="依赖安装-amp-amp-代理设置"><a href="#依赖安装-amp-amp-代理设置" class="headerlink" title="依赖安装&amp;&amp;代理设置"></a>依赖安装&amp;&amp;代理设置</h3><p>首先要在两个节点都安装Docker和Kubernetes相关的组件。因为相关的镜像都在墙外，所以这里需要挂代理或者自行寻找墙内的源。笔者选择的是挂代理的方案，给Ubuntu配置HTTP代理可以参考<a href="http://dearmadman.com/2015/08/30/use-shadowsocks-in-ubuntu/" target="_blank" rel="noopener">这篇博客</a>。给Docker配置代理可以参考<a href="https://docs.docker.com/engine/admin/systemd/#httphttps-proxy" target="_blank" rel="noopener">官方文档</a></p><p>安装的步骤是按照官网的文档<a href="https://kubernetes.io/docs/getting-started-guides/kubeadm/" target="_blank" rel="noopener">Installing Kubernetes on Linux with kubeadm</a>来的：</p><pre><code># 升级包管理的镜像列表apt-get update &amp;&amp; apt-get install -y apt-transport-https# 将docker和kubernetes相关的镜像源加入列表curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.listdeb http://apt.kubernetes.io/ kubernetes-xenial mainEOFapt-get update# 安装docker和kubernetes相关组件apt-get install -y docker-engineapt-get install -y kubelet kubeadm kubectl kubernetes-cni</code></pre><h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><p>在MASTER节点运行：</p><p><code>kubeadm init</code></p><p>如果一切正常，最后会有如下的输出：</p><pre><code>Your Kubernetes master has initialized successfully!To start using your cluster, you need to run (as a regular user):  sudo cp /etc/kubernetes/admin.conf $HOME/  sudo chown $(id -u):$(id -g) $HOME/admin.conf  export KUBECONFIG=$HOME/admin.confYou should now deploy a pod network to the cluster.Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:  http://kubernetes.io/docs/admin/addons/You can now join any number of machines by running the following on each nodeas root:  kubeadm join --token 67e1ac.eac65cabb7d2801c 172.18.214.46:6443</code></pre><h3 id="设置环境变量"><a href="#设置环境变量" class="headerlink" title="设置环境变量"></a>设置环境变量</h3><p>上一步中kubeadm会生成配置文件，输出的消息中要求我们设置环境变量<code>KUBECONFIG</code>为配置文件的路径。以便后续的使用。</p><pre><code>sudo cp /etc/kubernetes/admin.conf $HOME/sudo chown $(id -u):$(id -g) $HOME/admin.confexport KUBECONFIG=$HOME/admin.conf</code></pre><h3 id="Pod网络设置：weaver"><a href="#Pod网络设置：weaver" class="headerlink" title="Pod网络设置：weaver"></a>Pod网络设置：weaver</h3><p>到这里，我们已经初始化了一个单节点的Kubernetes集群。要想在集群中加入真正负载应用的Node，我们需要初始化一个Overlay Network。</p><p>Overlay Network的选择有很多，比如Flannel和Calico。但经过我个人的踩坑和<a href="http://tonybai.com/2016/12/30/install-kubernetes-on-ubuntu-with-kubeadm/" target="_blank" rel="noopener">参考博客</a>后，最终选择了weaver。</p><p>在Master节点运行：</p><pre><code>kubectl apply -f https://git.io/weave-kube-1.6</code></pre><p>稍候片刻，运行</p><pre><code>kubectl get pods -o wide --all-namespaces</code></pre><p>查看pods的运行情况：</p><pre><code>NAMESPACE     NAME                                              READY     STATUS    RESTARTS   AGE       IP              NODEkube-system   etcd-izwz9ap4sedl64wboiyh6cz                      1/1       Running   0          55m       172.18.214.46   izwz9ap4sedl64wboiyh6czkube-system   kube-apiserver-izwz9ap4sedl64wboiyh6cz            1/1       Running   0          54m       172.18.214.46   izwz9ap4sedl64wboiyh6czkube-system   kube-controller-manager-izwz9ap4sedl64wboiyh6cz   1/1       Running   0          55m       172.18.214.46   izwz9ap4sedl64wboiyh6czkube-system   kube-dns-3913472980-l8ghd                         3/3       Running   0          55m       10.32.0.2       izwz9ap4sedl64wboiyh6czkube-system   kube-proxy-n5332                                  1/1       Running   0          55m       172.18.214.46   izwz9ap4sedl64wboiyh6czkube-system   kube-scheduler-izwz9ap4sedl64wboiyh6cz            1/1       Running   0          54m       172.18.214.46   izwz9ap4sedl64wboiyh6czkube-system   weave-net-l86wx                                   2/2       Running   0          48m       172.18.214.46   izwz9ap4sedl64wboiyh6cz</code></pre><p>如果weave-net和kube-dns这两个pod都处于Running的状态。说明网络初始化成功。</p><blockquote><p>weave-net如果出现CrashLoopBackOff的错误，可以参考<a href="http://tonybai.com/2016/12/30/install-kubernetes-on-ubuntu-with-kubeadm-2/" target="_blank" rel="noopener">这里</a>的解决方案</p></blockquote><h3 id="加入Node"><a href="#加入Node" class="headerlink" title="加入Node"></a>加入Node</h3><p>在Node上运行之前Master上输出的：</p><pre><code>kubeadm join --token 67e1ac.eac65cabb7d2801c 172.18.214.46:6443</code></pre><p>设置配置文件路径的环境变量：</p><pre><code>export KUBECONFIG=/etc/kubernetes/kubelet.conf</code></pre><p>稍后，查看Node的运行情况：</p><pre><code>kubectl get nodesNAME                      STATUS    AGE       VERSIONizwz9972b5w4h8a4f1h9z7z   Ready     2h        v1.6.4izwz9ap4sedl64wboiyh6cz   Ready     4h        v1.6.4</code></pre><p>两个节点都显示Ready，说明加入成功。</p><blockquote><p>默认情况Master是不承担负载的，如果要Master节点也参与Pod调度，可以运行<code>kubectl taint nodes --all node-role.kubernetes.io/master-</code></p></blockquote><h3 id="示例应用"><a href="#示例应用" class="headerlink" title="示例应用"></a>示例应用</h3><p>节点部署就绪，我们来试着部署一个应用吧：</p><pre><code>kubectl create namespace sock-shopkubectl apply -n sock-shop -f &quot;https://github.com/microservices-demo/microservices-demo/blob/master/deploy/kubernetes/complete-demo.yaml?raw=true&quot;</code></pre><p>这两行命令会部署sock-shop相关的deployment和service。这些service共同组成了一个逻辑上的袜子商店网站。</p><p>等所有Pods都是Running状态了，我们可以运行：</p><pre><code>kubectl -n sock-shop get svc front-end</code></pre><p>查看front-end服务的端口：</p><pre><code>NAME        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGEfront-end   10.103.179.228   &lt;nodes&gt;       80:30001/TCP   1h</code></pre><p>端口是30001，然后我们就可以用<code>http://&lt;MASTER_IP&gt;:30001</code>来访问服务了:</p><p><img src="http://wx1.sinaimg.cn/large/64c45edcly1ffysdi2jjfj21kw0yhnpe.jpg" alt="socks-shop"></p><h3 id="查看和升级部署"><a href="#查看和升级部署" class="headerlink" title="查看和升级部署"></a>查看和升级部署</h3><p>在任意节点运行：</p><pre><code>kubectl get depolyment --all-namespaces</code></pre><p>可以看到当前的depolyment：</p><pre><code>NAMESPACE     NAME           DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGEkube-system   kube-dns       1         1         1            1           3hsock-shop     carts          1         1         1            1           1hsock-shop     carts-db       1         1         1            1           1hsock-shop     catalogue      1         1         1            1           1hsock-shop     catalogue-db   1         1         1            1           1hsock-shop     front-end      1         1         1            1           1hsock-shop     orders         1         1         1            1           1hsock-shop     orders-db      1         1         1            1           1hsock-shop     payment        1         1         1            1           1hsock-shop     queue-master   1         1         1            1           1hsock-shop     rabbitmq       1         1         1            1           1hsock-shop     shipping       1         1         1            1           1hsock-shop     user           1         1         1            1           1hsock-shop     user-db        1         1         1            1           1h</code></pre><p>如果我们想将其中一个服务横向拓展，比如payment服务，我们只需要：</p><pre><code>kubectl --namespace=sock-shop scale deployment payment --replicas 2</code></pre><p>一个新的payment pod就被初始化，并被分配到合适的节点上运行。</p><p>关于更多deployment相关的更新、回滚的信息请参考<a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/" target="_blank" rel="noopener">官方文档</a>。</p><h3 id="一些思考"><a href="#一些思考" class="headerlink" title="一些思考"></a>一些思考</h3><p>Kubernetes相对来说还是很容易上手的一个容器集群管理方案。只要我们开发的时候是按cloud native的思路去写，部署就是一件非常简单的事情。可以说几个配置文件就搞定了。Kubernetes接管了部署的更新和回滚，让运维变的轻松、可靠。比如部署的时候不会在新容器没有启动之前就终止旧容器。如果部署出了问题需要回滚，也可以进行一键式的回滚。部署也可以暂定，继续。这样的一套方案相比于手动管理容器，简直就是鸟枪换炮式的升级。</p><p>不仅仅是后端服务，我们的前端代码，也应该融入这套体系之中。前端作为一个单独的服务部署。这样可以更好的解耦。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文介绍了如何用Kubeadm 1.6版在Ubuntu 16.04系统上快速部署一个Kubernetes集群。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="http://yoursite.com/categories/Kubernetes/"/>
    
    
      <category term="Kubernetes" scheme="http://yoursite.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>基于Travis CI和Github的前端云构建</title>
    <link href="http://yoursite.com/2017/05/22/fe-cloud-building/"/>
    <id>http://yoursite.com/2017/05/22/fe-cloud-building/</id>
    <published>2017-05-22T08:04:00.000Z</published>
    <updated>2018-06-19T02:51:15.504Z</updated>
    
    <content type="html"><![CDATA[<p>最近在思考团队里前端代码部署的问题，之前采用的方案是在本地构建，推到Github上一个专门放build后前端代码的仓库，然后Github的Webhook去触发后端的部署逻辑。代码就从这个仓库里拉取。</p><p>这种方案看起来没什么大问题，但总觉得比较awkward。首先这套方案不够自动化，需要大量的人工操作。然后Github的Webhook其实并不是特别好用，如果后期要和我们内部的私有云平台对接起来，还要经过一些桥接才可以。</p><p>本来呢，因为最近学了docker的缘故，我想写一个简单的Node服务，用来自动构建代码，然后通知服务端部署。每个应用就是一个单独的容器，这样环境就可以隔离。这个方案想来也不错。直到我仔细研究了一下Travis CI，才发现这个CI真是不简单。云端构建的任务用Travis CI就可以完美的实现。</p><a id="more"></a><h3 id="关于CI"><a href="#关于CI" class="headerlink" title="关于CI"></a>关于CI</h3><p>CI是持续集成的意思，持续集成里主要包括构建和测试代码。之前对Travis CI的印象是可以跑测试，仔细看了之后才发现Travis CI其实是一个云服务，提供了一个虚拟的Linux环境。你可以运行自定义的脚本。这个Linux环境的自由度还是非常大的。对于前端构建来说，Travis CI的网络环境可以快速安装npm包，这是一个非常大的优势。</p><h3 id="travis-yml文件"><a href="#travis-yml文件" class="headerlink" title=".travis.yml文件"></a><code>.travis.yml</code>文件</h3><p>Travis CI的配置文件其实就是让你写几个生命周期hook，内容一般是shell命令。比如<code>install</code>这个hook里主要写一些安装依赖的逻辑，<code>script</code>这个hook里主要是写测试和构建的逻辑，<code>deploy</code>这个hook里是写部署的逻辑。另外这几个hook都有各自的<code>before</code>和<code>after</code>版本。总而言之自由度是很大的。</p><p>一个示例<code>.travis.yml</code>文件。虽然我们不能直接<code>.travis.yml</code>中写逻辑，但我们可以运行任意的脚本，所以可以看出<code>.travis.yml</code>的能力基本等价于shell脚本。</p><pre><code>language: node_jsnode_js:  - &quot;7&quot;install:  - npm installscript:  - npm run buildafter_script:  - tar -cvf bundle.tar ./dist  - node deploy.js</code></pre><h3 id="云端构建"><a href="#云端构建" class="headerlink" title="云端构建"></a>云端构建</h3><p>在看过了上节的<code>.travis.yml</code>文件之后，云端构建的大致逻辑应该已经非常清楚了。我们在Travis CI的虚拟机中安装node依赖，build代码，压缩代码，然后运行一个js脚本。这个脚本的内容就是将代码上传到CDN。</p><p><code>deploy.js</code>中还可以向后端的平台发送部署的请求，以达到自动部署的目的。如果后端是分布式的架构，向管理的节点发送请求即可。</p><h3 id="一些展望"><a href="#一些展望" class="headerlink" title="一些展望"></a>一些展望</h3><p>Travis CI的能力取决于这个虚拟机里提供了怎样的环境。Travis CI支持docker，因此我们可以用Travis CI进行docker镜像的构建和上传。Travis CI支持Nodejs，因此我们可以在虚拟机中安装hexo，进行博客的云端构建和自动部署。云端的构建，由于保证环境的隔离，因此稳定性会比本地高。以上都是Travis CI可能的用途。Travis CI作为一个云服务，在运维方面，还有无限的可能性等我们去探索</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近在思考团队里前端代码部署的问题，之前采用的方案是在本地构建，推到Github上一个专门放build后前端代码的仓库，然后Github的Webhook去触发后端的部署逻辑。代码就从这个仓库里拉取。&lt;/p&gt;
&lt;p&gt;这种方案看起来没什么大问题，但总觉得比较awkward。首先这套方案不够自动化，需要大量的人工操作。然后Github的Webhook其实并不是特别好用，如果后期要和我们内部的私有云平台对接起来，还要经过一些桥接才可以。&lt;/p&gt;
&lt;p&gt;本来呢，因为最近学了docker的缘故，我想写一个简单的Node服务，用来自动构建代码，然后通知服务端部署。每个应用就是一个单独的容器，这样环境就可以隔离。这个方案想来也不错。直到我仔细研究了一下Travis CI，才发现这个CI真是不简单。云端构建的任务用Travis CI就可以完美的实现。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
</feed>
